{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5321c236",
   "metadata": {},
   "source": [
    "# 01. Setup & Environment (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° GPU ì„¤ì •)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6753b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "  project_setup.py\n",
    "  ML/DL í”„ë¡œì íŠ¸ ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰í•˜ëŠ” ìµœì í™” ì„¤ì • íŒŒì¼\n",
    "  ëŒ€ìƒ í™˜ê²½: Windows 11 Pro\n",
    "             Intel i9-14900K | RTX 4070 12GB\n",
    "             CUDA 12.4 | cuDNN 9.19.0 | Python 3.12.10\n",
    "============================================================\n",
    "ì‚¬ìš©ë²•:\n",
    "    import project_setup          # ìë™ ì‹¤í–‰\n",
    "    ë˜ëŠ”\n",
    "    from project_setup import setup\n",
    "    cfg = setup()\n",
    "============================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë‚´ë¶€ ì¶œë ¥ í—¬í¼\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _log(msg: str, level: str = \"ok\"):\n",
    "    icons = {\"ok\": \"âœ…\", \"warn\": \"âš ï¸ \", \"err\": \"âŒ\", \"info\": \"ğŸ“Œ\", \"title\": \"ğŸš€\"}\n",
    "    print(f\"  {icons.get(level, ' ')} {msg}\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. ê²½ê³  & ë¡œê¹… ì„¤ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _configure_warnings():\n",
    "    # ë¶ˆí•„ìš”í•œ ê²½ê³  ì–µì œ\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "    # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ê³  ì–µì œ\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]        = \"3\"\n",
    "    os.environ[\"TRANSFORMERS_VERBOSITY\"]       = \"error\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"]       = \"false\"\n",
    "    os.environ[\"DATASETS_VERBOSITY\"]           = \"error\"\n",
    "\n",
    "    # Python ê¸°ë³¸ ë¡œê±° ë ˆë²¨\n",
    "    logging.basicConfig(\n",
    "        level=logging.WARNING,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "    )\n",
    "    # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê±° ì–µì œ\n",
    "    for noisy in [\"transformers\", \"datasets\", \"PIL\", \"matplotlib\",\n",
    "                  \"urllib3\", \"filelock\", \"huggingface_hub\"]:\n",
    "        logging.getLogger(noisy).setLevel(logging.ERROR)\n",
    "\n",
    "    _log(\"ê²½ê³  & ë¡œê¹… ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Windows 11 ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _configure_multiprocessing():\n",
    "    \"\"\"\n",
    "    Windowsì—ì„œ DataLoader num_workers > 0 ì‚¬ìš© ì‹œ\n",
    "    ë°˜ë“œì‹œ if __name__ == '__main__' ë¸”ë¡ì´ í•„ìš”í•˜ê±°ë‚˜,\n",
    "    spawn contextë¥¼ ëª…ì‹œí•´ì•¼ í•¨.\n",
    "    ì—¬ê¸°ì„œëŠ” í™˜ê²½ ë³€ìˆ˜ë§Œ ìµœì í™”.\n",
    "    \"\"\"\n",
    "    # i9-14900K: Pì½”ì–´ 8ê°œ(HT=16) + Eì½”ì–´ 16ê°œ = 32ìŠ¤ë ˆë“œ\n",
    "    # DataLoaderì— ì í•©í•œ worker ìˆ˜ (ë³´í†µ Pì½”ì–´ ìˆ˜ = 8)\n",
    "    cpu_count = os.cpu_count() or 8\n",
    "    optimal_workers = min(8, cpu_count // 2)\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"]      = str(optimal_workers)\n",
    "    os.environ[\"MKL_NUM_THREADS\"]      = str(optimal_workers)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(optimal_workers)\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"]  = str(optimal_workers)\n",
    "\n",
    "    _log(f\"CPU ë©€í‹°ìŠ¤ë ˆë“œ ì„¤ì • ì™„ë£Œ (threads={optimal_workers}, cpu_total={cpu_count})\")\n",
    "    return optimal_workers\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. CUDA í™œì„±í™” & ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def activate_cuda(verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    CUDA ì»¨í…ìŠ¤íŠ¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³  í™œì„± ìƒíƒœë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.\n",
    "\n",
    "    ì¼ë°˜ì ìœ¼ë¡œ ì²« ë²ˆì§¸ CUDA ì—°ì‚° ì‹œ ìë™ ì´ˆê¸°í™”ë˜ì§€ë§Œ,\n",
    "    ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´:\n",
    "      - ë“œë¼ì´ë²„ ë ˆë²¨ ì´ˆê¸°í™”ë¥¼ ì„ í–‰ ì™„ë£Œ\n",
    "      - ì²« ë²ˆì§¸ ì‹¤ì œ ì—°ì‚°ì˜ ì§€ì—°(latency) ì œê±°\n",
    "      - CUDA / cuDNN / ì—°ì‚° íŒŒì´í”„ë¼ì¸ ì´ìƒ ì—¬ë¶€ ì‚¬ì „ ì ê²€\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict â€” CUDA í™œì„±í™” ê²°ê³¼ ë° ìƒì„¸ ì •ë³´\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"cuda_available\"    : False,\n",
    "        \"context_initialized\": False,\n",
    "        \"warmup_passed\"     : False,\n",
    "        \"cuda_version\"      : None,\n",
    "        \"cudnn_version\"     : None,\n",
    "        \"driver_version\"    : None,\n",
    "        \"device_name\"       : None,\n",
    "        \"compute_capability\": None,\n",
    "        \"vram_total_gb\"     : None,\n",
    "        \"vram_free_gb\"      : None,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CUDA í™œì„±í™” & ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        # â”€â”€ Step 1: ê¸°ë³¸ ê°€ìš©ì„± ì²´í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if not torch.cuda.is_available():\n",
    "            _log(\"CUDA ì‚¬ìš© ë¶ˆê°€ â€” ë“œë¼ì´ë²„/ì„¤ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”\", \"err\")\n",
    "            if verbose:\n",
    "                print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "            return result\n",
    "\n",
    "        result[\"cuda_available\"] = True\n",
    "        result[\"cuda_version\"]   = torch.version.cuda\n",
    "        result[\"cudnn_version\"]  = torch.backends.cudnn.version()\n",
    "\n",
    "        # â”€â”€ Step 2: ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        try:\n",
    "            import subprocess\n",
    "            smi = subprocess.run(\n",
    "                [\"nvidia-smi\", \"--query-gpu=driver_version\",\n",
    "                 \"--format=csv,noheader\"],\n",
    "                capture_output=True, text=True, timeout=5\n",
    "            )\n",
    "            if smi.returncode == 0:\n",
    "                result[\"driver_version\"] = smi.stdout.strip()\n",
    "        except Exception:\n",
    "            result[\"driver_version\"] = \"í™•ì¸ ë¶ˆê°€\"\n",
    "\n",
    "        # â”€â”€ Step 3: ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (í•µì‹¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        #   torch.cuda.init() : CUDA ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸ ëª…ì‹œì  ìƒì„±\n",
    "        #   ì´í›„ ì—°ì‚°ì—ì„œ ë°œìƒí•˜ëŠ” ~1~2ì´ˆ ì§€ì—°ì„ ì—¬ê¸°ì„œ ì„  ì²˜ë¦¬\n",
    "        torch.cuda.init()\n",
    "        torch.cuda.set_device(0)               # GPU 0ë²ˆ ëª…ì‹œì  ì„ íƒ\n",
    "        result[\"context_initialized\"] = True\n",
    "\n",
    "        # â”€â”€ Step 4: GPU ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        free_mem, total_mem = torch.cuda.mem_get_info(0)\n",
    "\n",
    "        result.update({\n",
    "            \"device_name\"       : torch.cuda.get_device_name(0),\n",
    "            \"compute_capability\": f\"{props.major}.{props.minor}\",\n",
    "            \"vram_total_gb\"     : round(total_mem / 1024**3, 2),\n",
    "            \"vram_free_gb\"      : round(free_mem  / 1024**3, 2),\n",
    "            \"sm_count\"          : props.multi_processor_count,\n",
    "        })\n",
    "\n",
    "        # â”€â”€ Step 5: Warm-up ì»¤ë„ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        #   ì‹¤ì œ CUDA ì»¤ë„ì„ ì‹¤í–‰í•´ íŒŒì´í”„ë¼ì¸ ì´ìƒ ì—¬ë¶€ í™•ì¸\n",
    "        #   (1) ì†Œí˜• í…ì„œ í• ë‹¹ / ì—°ì‚° / ë™ê¸°í™”\n",
    "        #   (2) cuDNN ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ ì‹¤í–‰ (cuDNN ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”)\n",
    "        #   (3) Mixed Precision ì—°ì‚° í™•ì¸\n",
    "        device = torch.device(\"cuda:0\")\n",
    "\n",
    "        # Warm-up 1: ê¸°ë³¸ CUDA ì—°ì‚°\n",
    "        _a = torch.ones(512, 512, device=device)\n",
    "        _b = torch.ones(512, 512, device=device)\n",
    "        _c = torch.mm(_a, _b)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Warm-up 2: cuDNN ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ ì´ˆê¸°í™”\n",
    "        _conv_in  = torch.randn(1, 3, 64, 64, device=device)\n",
    "        _conv     = torch.nn.Conv2d(3, 16, 3, padding=1).to(device)\n",
    "        _conv_out = _conv(_conv_in)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Warm-up 3: Mixed Precision (FP16) ê²½ë¡œ í™•ì¸\n",
    "        _a16 = _a.half()\n",
    "        _b16 = _b.half()\n",
    "        _c16 = torch.mm(_a16, _b16)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Warm-up í…ì„œ ì •ë¦¬\n",
    "        del _a, _b, _c, _a16, _b16, _c16\n",
    "        del _conv_in, _conv, _conv_out\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        result[\"warmup_passed\"] = True\n",
    "\n",
    "        # â”€â”€ Step 6: ê²°ê³¼ ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if verbose:\n",
    "            ok  = \"âœ…\"\n",
    "            vram_used = result[\"vram_total_gb\"] - result[\"vram_free_gb\"]\n",
    "            bar_len   = 25\n",
    "            ratio     = vram_used / result[\"vram_total_gb\"]\n",
    "            filled    = int(bar_len * ratio)\n",
    "            bar       = \"â–ˆ\" * filled + \"â–‘\" * (bar_len - filled)\n",
    "\n",
    "            print(f\"  â”‚  {ok} CUDA {result['cuda_version']}  |  \"\n",
    "                  f\"cuDNN {result['cudnn_version']}  |  \"\n",
    "                  f\"Driver {result['driver_version']}\")\n",
    "            print(f\"  â”‚  {ok} GPU   : {result['device_name']}\")\n",
    "            print(f\"  â”‚  {ok} CC    : {result['compute_capability']}  |  \"\n",
    "                  f\"SM: {result['sm_count']}ê°œ\")\n",
    "            print(f\"  â”‚  {ok} VRAM  : [{bar}] \"\n",
    "                  f\"{vram_used:.2f}/{result['vram_total_gb']:.2f} GB \"\n",
    "                  f\"(ì—¬ìœ  {result['vram_free_gb']:.2f} GB)\")\n",
    "            print(f\"  â”‚  {ok} Context ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "            print(f\"  â”‚  {ok} Warm-up ì™„ë£Œ (CUDA / cuDNN / FP16)\")\n",
    "            print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "            print()\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        _log(f\"CUDA í™œì„±í™” ì‹¤íŒ¨: {e}\", \"err\")\n",
    "        if verbose:\n",
    "            print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "    except ImportError:\n",
    "        _log(\"PyTorch ë¯¸ì„¤ì¹˜ â†’ CUDA í™œì„±í™” ë¶ˆê°€\", \"err\")\n",
    "        if verbose:\n",
    "            print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3-B. CUDA / PyTorch ìµœì í™”\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _configure_pytorch():\n",
    "    info = {}\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        # â”€â”€ GPU ì„ íƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"]   = \"PCI_BUS_ID\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"          # RTX 4070 ë‹¨ì¼ GPU\n",
    "\n",
    "        # â”€â”€ VRAM ë‹¨í¸í™” ë°©ì§€ (12GBì— ìµœì í™”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n",
    "            \"max_split_size_mb:512,\"\n",
    "            \"expandable_segments:True,\"\n",
    "            \"garbage_collection_threshold:0.8\"\n",
    "        )\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            _log(\"CUDA ì‚¬ìš© ë¶ˆê°€ â†’ CPU ëª¨ë“œ\", \"warn\")\n",
    "            info[\"device\"] = torch.device(\"cpu\")\n",
    "            return info\n",
    "\n",
    "        device = torch.device(\"cuda:0\")\n",
    "\n",
    "        # â”€â”€ cuDNN ìµœì í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        torch.backends.cudnn.enabled        = True\n",
    "        torch.backends.cudnn.benchmark      = True   # ì…ë ¥ í¬ê¸° ê³ ì • ì‹œ ìµœê³  ì†ë„\n",
    "        torch.backends.cudnn.deterministic  = False  # benchmark=Trueì™€ ë³‘í–‰ ì‹œ False ê¶Œì¥\n",
    "        torch.backends.cudnn.allow_tf32     = True   # Ampere ì´ìƒ TF32 ê°€ì†\n",
    "\n",
    "        # â”€â”€ TF32 (RTX 4070 Ada ì•„í‚¤í…ì²˜ ê°€ì†) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "\n",
    "        # â”€â”€ ë©”ëª¨ë¦¬ ì‚¬ì „ ì¤€ë¹„ (ë‹¨í¸í™” ë°©ì§€ìš© warm-up) â”€â”€â”€â”€â”€â”€\n",
    "        _dummy = torch.zeros(1, device=device)\n",
    "        del _dummy\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # â”€â”€ ì •ë³´ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        total_vram = props.total_memory / 1024**3\n",
    "\n",
    "        info = {\n",
    "            \"device\"       : device,\n",
    "            \"gpu_name\"     : torch.cuda.get_device_name(0),\n",
    "            \"cuda_version\" : torch.version.cuda,\n",
    "            \"cudnn_version\": torch.backends.cudnn.version(),\n",
    "            \"vram_total_gb\": round(total_vram, 2),\n",
    "            \"torch_version\": torch.__version__,\n",
    "            \"compute_cap\"  : f\"{props.major}.{props.minor}\",\n",
    "            \"sm_count\"     : props.multi_processor_count,\n",
    "        }\n",
    "\n",
    "        _log(f\"PyTorch {torch.__version__} | CUDA {torch.version.cuda} | \"\n",
    "             f\"cuDNN {torch.backends.cudnn.version()}\")\n",
    "        _log(f\"GPU: {info['gpu_name']} | VRAM: {total_vram:.2f} GB | \"\n",
    "             f\"SM: {props.multi_processor_count} | CC: {props.major}.{props.minor}\")\n",
    "        _log(\"cuDNN benchmark=True | TF32=True | VRAM ë‹¨í¸í™” ë°©ì§€ ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "    except ImportError:\n",
    "        _log(\"PyTorch ë¯¸ì„¤ì¹˜\", \"err\")\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. NumPy / Pandas ìµœì í™”\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _configure_numpy_pandas():\n",
    "    try:\n",
    "        import numpy as np\n",
    "        # MKL/OpenBLAS ì‚¬ìš© í™•ì¸\n",
    "        np_config = np.__config__\n",
    "        _log(f\"NumPy {np.__version__} ë¡œë“œ ì™„ë£Œ\")\n",
    "    except ImportError:\n",
    "        _log(\"NumPy ë¯¸ì„¤ì¹˜\", \"err\")\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        # Copy-on-Write (pandas 2.0+) í™œì„±í™” â†’ ë©”ëª¨ë¦¬ íš¨ìœ¨ í–¥ìƒ\n",
    "        pd.options.mode.copy_on_write            = True\n",
    "        pd.options.display.max_columns           = 50\n",
    "        pd.options.display.max_rows              = 100\n",
    "        pd.options.display.float_format          = \"{:.4f}\".format\n",
    "        pd.options.display.max_colwidth          = 80\n",
    "        _log(f\"Pandas {pd.__version__} | Copy-on-Write=True ì„¤ì • ì™„ë£Œ\")\n",
    "    except ImportError:\n",
    "        _log(\"Pandas ë¯¸ì„¤ì¹˜\", \"err\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Matplotlib ì„¤ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _configure_matplotlib():\n",
    "    try:\n",
    "        import matplotlib\n",
    "        matplotlib.use(\"Agg\")                          # Windows headless ì•ˆì „ ë°±ì—”ë“œ\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams.update({\n",
    "            \"figure.figsize\"     : (12, 6),\n",
    "            \"figure.dpi\"         : 120,\n",
    "            \"axes.grid\"          : True,\n",
    "            \"grid.alpha\"         : 0.3,\n",
    "            \"axes.spines.top\"    : False,\n",
    "            \"axes.spines.right\"  : False,\n",
    "            \"font.size\"          : 11,\n",
    "        })\n",
    "        _log(f\"Matplotlib {matplotlib.__version__} | Agg ë°±ì—”ë“œ ì„¤ì • ì™„ë£Œ\")\n",
    "    except ImportError:\n",
    "        _log(\"Matplotlib ë¯¸ì„¤ì¹˜\", \"err\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. ì¬í˜„ì„±(Seed) ê³ ì • ìœ í‹¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ë‚œìˆ˜ ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "    ì™„ì „í•œ ì¬í˜„ì„±ì´ í•„ìš”í•  ë•Œ í˜¸ì¶œí•˜ì„¸ìš”.\n",
    "\n",
    "    ì£¼ì˜: cuDNN deterministic=Trueë¡œ ì „í™˜ë˜ì–´ ì†ë„ê°€ ì•½ê°„ ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "\n",
    "    try:\n",
    "        import numpy as np\n",
    "        np.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark     = False   # deterministic ì‹œ False\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"  ğŸ² Seed {seed} ê³ ì • ì™„ë£Œ (random / numpy / torch / PYTHONHASHSEED)\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥ ìœ í‹¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def gpu_memory_status():\n",
    "    \"\"\"í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"  âš ï¸  CUDA ì‚¬ìš© ë¶ˆê°€\")\n",
    "            return\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved  = torch.cuda.memory_reserved()  / 1024**3\n",
    "        total     = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        free      = total - allocated\n",
    "        bar_len   = 30\n",
    "        used_ratio = allocated / total\n",
    "        filled    = int(bar_len * used_ratio)\n",
    "        bar       = \"â–ˆ\" * filled + \"â–‘\" * (bar_len - filled)\n",
    "        print(f\"\\n  ğŸ’¾ GPU ë©”ëª¨ë¦¬ í˜„í™© [{bar}] {used_ratio*100:.1f}%\")\n",
    "        print(f\"     ì‚¬ìš©ì¤‘: {allocated:.2f} GB  |  ì˜ˆì•½: {reserved:.2f} GB  \"\n",
    "              f\"|  ì—¬ìœ : {free:.2f} GB  |  ì „ì²´: {total:.2f} GB\")\n",
    "    except ImportError:\n",
    "        print(\"  âŒ PyTorch ë¯¸ì„¤ì¹˜\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8. GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ìœ í‹¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"GPU ìºì‹œë¥¼ ë¹„ìš°ê³  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            print(\"  ğŸ§¹ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 9. ê¶Œì¥ í•˜ì´í¼íŒŒë¼ë¯¸í„° í”„ë¦¬ì…‹ (RTX 4070 ê¸°ì¤€)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_recommended_config(task: str = \"image_classification\") -> dict:\n",
    "    \"\"\"\n",
    "    RTX 4070 12GB ê¸°ì¤€ ì‘ì—…ë³„ ê¶Œì¥ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    task ì˜µì…˜:\n",
    "        \"image_classification\" | \"object_detection\" |\n",
    "        \"segmentation\"         | \"nlp_bert\"         |\n",
    "        \"nlp_llm\"              | \"tabular\"\n",
    "    \"\"\"\n",
    "    base = {\n",
    "        \"device\"       : \"cuda\",\n",
    "        \"num_workers\"  : 8,          # i9-14900K Pì½”ì–´ ê¸°ì¤€\n",
    "        \"pin_memory\"   : True,       # CPUâ†’GPU ì „ì†¡ ì†ë„ í–¥ìƒ\n",
    "        \"prefetch_factor\": 2,\n",
    "        \"persistent_workers\": True,\n",
    "        \"use_amp\"      : True,       # Mixed Precision (FP16)\n",
    "    }\n",
    "\n",
    "    presets = {\n",
    "        \"image_classification\": {\n",
    "            **base,\n",
    "            \"batch_size\"   : 64,\n",
    "            \"image_size\"   : 224,\n",
    "            \"optimizer\"    : \"AdamW\",\n",
    "            \"lr\"           : 1e-4,\n",
    "            \"weight_decay\" : 1e-2,\n",
    "            \"epochs\"       : 50,\n",
    "            \"scheduler\"    : \"CosineAnnealingLR\",\n",
    "            \"note\"         : \"ResNet50/EfficientNet/ViT ê¸°ì¤€\",\n",
    "        },\n",
    "        \"object_detection\": {\n",
    "            **base,\n",
    "            \"batch_size\"   : 16,\n",
    "            \"image_size\"   : 640,\n",
    "            \"optimizer\"    : \"SGD\",\n",
    "            \"lr\"           : 1e-2,\n",
    "            \"epochs\"       : 100,\n",
    "            \"note\"         : \"YOLOv8/Faster-RCNN ê¸°ì¤€\",\n",
    "        },\n",
    "        \"segmentation\": {\n",
    "            **base,\n",
    "            \"batch_size\"   : 8,\n",
    "            \"image_size\"   : 512,\n",
    "            \"optimizer\"    : \"AdamW\",\n",
    "            \"lr\"           : 3e-4,\n",
    "            \"epochs\"       : 100,\n",
    "            \"note\"         : \"U-Net/SegFormer ê¸°ì¤€\",\n",
    "        },\n",
    "        \"nlp_bert\": {\n",
    "            **base,\n",
    "            \"batch_size\"   : 32,\n",
    "            \"max_length\"   : 512,\n",
    "            \"optimizer\"    : \"AdamW\",\n",
    "            \"lr\"           : 2e-5,\n",
    "            \"weight_decay\" : 1e-2,\n",
    "            \"epochs\"       : 5,\n",
    "            \"warmup_ratio\" : 0.1,\n",
    "            \"note\"         : \"BERT/RoBERTa íŒŒì¸íŠœë‹ ê¸°ì¤€\",\n",
    "        },\n",
    "        \"nlp_llm\": {\n",
    "            **base,\n",
    "            \"batch_size\"   : 4,\n",
    "            \"gradient_accumulation_steps\": 8,   # ìœ íš¨ ë°°ì¹˜ = 32\n",
    "            \"max_length\"   : 2048,\n",
    "            \"optimizer\"    : \"AdamW\",\n",
    "            \"lr\"           : 2e-4,\n",
    "            \"epochs\"       : 3,\n",
    "            \"use_lora\"     : True,\n",
    "            \"lora_r\"       : 16,\n",
    "            \"lora_alpha\"   : 32,\n",
    "            \"note\"         : \"LLM LoRA íŒŒì¸íŠœë‹ ê¸°ì¤€ (12GB í•œê³„ ë‚´)\",\n",
    "        },\n",
    "        \"tabular\": {\n",
    "            \"device\"       : \"cuda\",\n",
    "            \"batch_size\"   : 4096,\n",
    "            \"optimizer\"    : \"AdamW\",\n",
    "            \"lr\"           : 1e-3,\n",
    "            \"epochs\"       : 100,\n",
    "            \"num_workers\"  : 4,\n",
    "            \"note\"         : \"TabNet/MLP ê¸°ì¤€\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    cfg = presets.get(task, base)\n",
    "    print(f\"\\n  ğŸ“‹ [{task}] ê¶Œì¥ ì„¤ì •\")\n",
    "    for k, v in cfg.items():\n",
    "        print(f\"     {k:<35} = {v}\")\n",
    "    return cfg\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 10. Mixed Precision Scaler íŒ©í† ë¦¬\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_amp_scaler():\n",
    "    \"\"\"\n",
    "    RTX 4070ìš© Mixed Precision GradScalerë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    ì‚¬ìš© ì˜ˆ:\n",
    "        scaler = get_amp_scaler()\n",
    "        with torch.autocast(device_type='cuda'):\n",
    "            loss = model(x)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            scaler = torch.amp.GradScaler(\n",
    "                init_scale=2.**16,\n",
    "                growth_factor=2.0,\n",
    "                backoff_factor=0.5,\n",
    "                growth_interval=2000,\n",
    "                enabled=True,\n",
    "            )\n",
    "            _log(\"GradScaler (Mixed Precision FP16) ì¤€ë¹„ ì™„ë£Œ\")\n",
    "            return scaler\n",
    "    except ImportError:\n",
    "        _log(\"PyTorch ë¯¸ì„¤ì¹˜ â†’ GradScaler ë°˜í™˜ ë¶ˆê°€\", \"err\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë©”ì¸ setup() í•¨ìˆ˜\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def setup(seed: int | None = None, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    í”„ë¡œì íŠ¸ ì‹œì‘ ì‹œ ëª¨ë“  ìµœì í™” ì„¤ì •ì„ í•œ ë²ˆì— ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed    : int | None â€” ì§€ì • ì‹œ ì¬í˜„ì„± ê³ ì • (ê¸°ë³¸: None = ê³ ì • ì•ˆ í•¨)\n",
    "    verbose : bool       â€” ì‹œìŠ¤í…œ ìš”ì•½ ì¶œë ¥ ì—¬ë¶€\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict â€” device, gpu_name, vram ë“± ì£¼ìš” ì •ë³´\n",
    "    \"\"\"\n",
    "    t_start = time.time()\n",
    "\n",
    "    print()\n",
    "    print(\"  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(\"  â•‘      ML / DL í”„ë¡œì íŠ¸ í™˜ê²½ ì´ˆê¸°í™” ì¤‘...         â•‘\")\n",
    "    print(\"  â•‘  i9-14900K | RTX 4070 12GB | CUDA 12.4          â•‘\")\n",
    "    print(\"  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    print()\n",
    "\n",
    "    _configure_warnings()\n",
    "    workers   = _configure_multiprocessing()\n",
    "    cuda_info = activate_cuda(verbose=True)     # CUDA í™œì„±í™” & ê²€ì¦\n",
    "    gpu_info  = _configure_pytorch()            # PyTorch ì„¸ë¶€ ìµœì í™”\n",
    "    _configure_numpy_pandas()\n",
    "    _configure_matplotlib()\n",
    "\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "\n",
    "    result = {\n",
    "        **gpu_info,\n",
    "        \"optimal_num_workers\": workers,\n",
    "        \"platform\"           : platform.platform(),\n",
    "        \"python_version\"     : sys.version.split()[0],\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œìŠ¤í…œ ìš”ì•½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "        print(f\"  â”‚  OS       : {platform.platform()[:50]}\")\n",
    "        print(f\"  â”‚  Python   : {sys.version.split()[0]}\")\n",
    "        if \"gpu_name\" in result:\n",
    "            print(f\"  â”‚  GPU      : {result['gpu_name']}\")\n",
    "            print(f\"  â”‚  VRAM     : {result['vram_total_gb']} GB\")\n",
    "            print(f\"  â”‚  CUDA     : {result.get('cuda_version','?')}  \"\n",
    "                  f\"cuDNN: {result.get('cudnn_version','?')}\")\n",
    "        print(f\"  â”‚  Workers  : {workers}\")\n",
    "        print(f\"  â”‚  ì´ˆê¸°í™”   : {elapsed:.2f}ì´ˆ ì™„ë£Œ\")\n",
    "        print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "        print()\n",
    "        print(\"  ğŸ“Œ ì‚¬ìš© ê°€ëŠ¥í•œ ìœ í‹¸ í•¨ìˆ˜:\")\n",
    "        print(\"     activate_cuda()            â†’ CUDA í™œì„±í™” & ê²€ì¦ (ì¬ì‹¤í–‰)\")\n",
    "        print(\"     set_seed(42)               â†’ ì¬í˜„ì„± ì‹œë“œ ê³ ì •\")\n",
    "        print(\"     gpu_memory_status()        â†’ VRAM ì‚¬ìš©ëŸ‰ í™•ì¸\")\n",
    "        print(\"     clear_gpu_memory()         â†’ GPU ìºì‹œ ë¹„ìš°ê¸°\")\n",
    "        print(\"     get_recommended_config()   â†’ ì‘ì—…ë³„ ê¶Œì¥ ì„¤ì •\")\n",
    "        print(\"     get_amp_scaler()           â†’ FP16 GradScaler ë°˜í™˜\")\n",
    "        print()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# import ì‹œ ìë™ ì‹¤í–‰\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_config = setup()\n",
    "device  = _config.get(\"device\", None)      # ì „ì—­ìœ¼ë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced56d4",
   "metadata": {},
   "source": [
    "# 02. Data Ingestion & Integration (ë°ì´í„° ë¡œë“œ ë° í†µí•©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\sck32\\Desktop\\Clinical Diabetes TabNet\\Data\"\n",
    "YEARS = ['19', '20', '21', '22', '23', '24']\n",
    "\n",
    "def load_sas(path) :\n",
    "    try :\n",
    "        df, _ = pyreadstat.read_sas7bdat(path, encoding = 'cp949')\n",
    "        return df\n",
    "    except UnicodeDecodeError :\n",
    "        df, _ = pyreadstat.read_sas7bdat(path, encoding = 'latin1')\n",
    "        return df\n",
    "\n",
    "all_year_list = []\n",
    "\n",
    "for y in YEARS :\n",
    "    all_file = os.path.join(BASE_DIR, f\"hn{y}_all.sas7bdat\")\n",
    "    rc_file = os.path.join(BASE_DIR, f\"hn{y}_24rc.sas7bdat\")\n",
    "    \n",
    "    if os.path.exists(all_file) :\n",
    "        df_all = load_sas(all_file)\n",
    "        df_all['ID'] = df_all['ID'].astype(str).str.strip().str.upper()\n",
    "        \n",
    "        # ì˜ì–‘ì¡°ì‚¬(24rc) ì„¸ë¶€ ë°ì´í„° ë³‘í•©\n",
    "        if os.path.exists(rc_file) :\n",
    "            df_rc = load_sas(rc_file)\n",
    "            df_rc.columns = [c.upper() for c in df_rc.columns]\n",
    "            df_rc['ID'] = df_rc['ID'].astype(str).str.strip().str.upper()\n",
    "            \n",
    "            # ì˜ì–‘ì†Œ ì„­ì·¨ ë°ì´í„° ì„¸ë¶€ ì§‘ê³„\n",
    "            agg_cols = {\n",
    "                'NF_INTK' : 'sum', \n",
    "                'NF_CODE' : 'nunique',\n",
    "                'N_EN' : 'mean',\n",
    "                'N_PROT' : 'mean',\n",
    "                'N_FAT' : 'mean',\n",
    "                'N_CHO' : 'mean'\n",
    "            }\n",
    "            agg_dict = {k : v for k, v in agg_cols.items() if k in df_rc.columns}\n",
    "            \n",
    "            df_rc_agg = df_rc.groupby('ID').agg(agg_dict).reset_index()\n",
    "            df_all = pd.merge(df_all, df_rc_agg, on = 'ID', how = 'left')\n",
    "            print(f\"âœ… 20{y}ë…„ : ì„ìƒ/ê±´ê°•ì„¤ë¬¸ + ì„¸ë¶€ ì˜ì–‘ ë°ì´í„° í†µí•© ì™„ë£Œ\")\n",
    "        else :\n",
    "            print(f\"âš ï¸ 20{y}ë…„ : ì„ìƒ/ê±´ê°•ì„¤ë¬¸ ë°ì´í„°ë§Œ í†µí•© (ì˜ì–‘ íŒŒì¼ ëˆ„ë½)\")\n",
    "            \n",
    "        all_year_list.append(df_all)\n",
    "\n",
    "raw_df = pd.concat(all_year_list, axis = 0, ignore_index = True)\n",
    "print(f\"\\nğŸ“Œ í†µí•© ë°ì´í„°ì…‹ í˜•íƒœ : {raw_df.shape[0] :,} í–‰, {raw_df.shape[1] :,} ì—´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f69e4a",
   "metadata": {},
   "source": [
    "# 03. Data Preprocessing (Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff43f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. í†µí•© ê°€ì¤‘ì¹˜ ë° ê¸°ë³¸ ì •ì œ\n",
    "processed_df = raw_df.copy()\n",
    "processed_df['wt_pooled'] = processed_df['wt_itvex'] / 6\n",
    "\n",
    "def clean_missing(df) :\n",
    "    num_cols = df.select_dtypes(include = [np.number]).columns\n",
    "    df[num_cols] = df[num_cols].replace([8, 9, 88, 99, 888, 999, 9999], np.nan)\n",
    "    return df\n",
    "\n",
    "processed_df = clean_missing(processed_df)\n",
    "\n",
    "# 2. ë¶„ì„ ë„ë©”ì¸ë³„ ë³€ìˆ˜ í™•ì •\n",
    "virus_cols = ['HE_hepaB', 'HE_hepaC']\n",
    "genetic_cols = [c for c in processed_df.columns if any(x in c.upper() for x in ['HE_FH_DIA', 'HE_FH_HT', 'HE_FH_HDL', 'HE_FH_IHD'])]\n",
    "metabolic_cols = [\n",
    "    'HE_glu', 'HE_HbA1c', 'HE_chol', 'HE_TG', 'HE_HDL_st2', \n",
    "    'HE_ast', 'HE_alt', 'HE_BMI', 'HE_wc', 'HE_sbp', 'HE_dbp',\n",
    "    'HE_crea', 'HE_BUN', 'HE_WBC', 'HE_Upro'\n",
    "]\n",
    "nutrition_cols = ['N_EN', 'N_CHO', 'N_PROT', 'N_FAT', 'N_NA', 'N_K', 'age', 'sex', 'sm_presnt']\n",
    "\n",
    "all_features = virus_cols + genetic_cols + metabolic_cols + nutrition_cols\n",
    "valid_features = [c for c in all_features if c in processed_df.columns]\n",
    "\n",
    "# 3. ë°ì´í„° í•„í„°ë§ (í•µì‹¬ ëŒ€ì‚¬ ì§€í‘œ ê¸°ì¤€)\n",
    "core_metrics = ['HE_glu', 'HE_HbA1c', 'HE_TG', 'HE_BMI']\n",
    "analysis_df = processed_df.dropna(subset = core_metrics).copy()\n",
    "\n",
    "# 4. ì´ìƒì¹˜ ì²˜ë¦¬ (Winsorization : ìƒí•˜ìœ„ 0.5% í´ë¦¬í•‘)\n",
    "for col in metabolic_cols :\n",
    "    if col in analysis_df.columns :\n",
    "        lower = analysis_df[col].quantile(0.005)\n",
    "        upper = analysis_df[col].quantile(0.995)\n",
    "        analysis_df[col] = analysis_df[col].clip(lower, upper)\n",
    "\n",
    "# 5. íƒ€ê²Ÿ ë¼ë²¨ë§\n",
    "analysis_df['target_diabetes'] = analysis_df['DE1_dg'].map({0 : 0, 1 : 1})\n",
    "\n",
    "print(f\"âœ… í™•ì¥ëœ ìœ íš¨ ë³€ìˆ˜ ì´ ê°œìˆ˜ : {len(valid_features) :,}ê°œ\")\n",
    "print(f\"âœ… ì •ì œëœ ìµœì¢… ìƒ˜í”Œ ìˆ˜ : {len(analysis_df) :,}ëª…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6327b06",
   "metadata": {},
   "source": [
    "# 04. Data Imputation & Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f1ca1",
   "metadata": {},
   "source": [
    "## 4-1 Robust Imputation & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c202634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# ë¶„ì„ ëŒ€ìƒ í”¼ì²˜ ì¶”ì¶œ\n",
    "X_raw = analysis_df[valid_features].copy()\n",
    "\n",
    "# ë‹¤ì¤‘ ëŒ€ì¹˜ë²•(MICE) ê¸°ë°˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ìˆ˜í–‰\n",
    "imputer = IterativeImputer(\n",
    "    max_iter=20, \n",
    "    random_state=42, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"ğŸš€ ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ ì‹œì‘ (Sample Size: {len(X_raw):,})\")\n",
    "X_imputed = imputer.fit_transform(X_raw)\n",
    "\n",
    "# ë°ì´í„° í‘œì¤€í™” ìˆ˜í–‰ (Z-score Scaling)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# GPU í…ì„œ ë³€í™˜ ë° ì¥ì¹˜ í• ë‹¹\n",
    "X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "\n",
    "print(f\"âœ… í…ì„œ ë³€í™˜ ë° {device} í• ë‹¹ ì™„ë£Œ\")\n",
    "print(f\"ğŸ“Š ìµœì¢… ë°ì´í„° í˜•íƒœ : {X_tensor.shape[0]:,} rows Ã— {X_tensor.shape[1]:,} cols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17b84e",
   "metadata": {},
   "source": [
    "## 4-2. Imputation Integrity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€ì¹˜ ì™„ë£Œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "X_check = pd.DataFrame(X_imputed, columns=valid_features)\n",
    "\n",
    "# ì£¼ìš” ëŒ€ì‚¬ ì§€í‘œ í‰ê· ì¹˜ ë³€ë™ í™•ì¸\n",
    "target_cols = ['HE_glu', 'HE_BMI', 'HE_TG']\n",
    "summary = pd.concat([\n",
    "    X_raw[target_cols].mean().rename('Raw_Mean'),\n",
    "    X_check[target_cols].mean().rename('Imputed_Mean')\n",
    "], axis=1)\n",
    "\n",
    "# ëŒ€ì¹˜ ì „í›„ ì˜¤ì°¨ìœ¨ ê³„ì‚°\n",
    "summary['Diff_Ratio(%)'] = abs((summary['Imputed_Mean'] - summary['Raw_Mean']) / summary['Raw_Mean']) * 100\n",
    "\n",
    "print(\"ğŸ“‹ ë°ì´í„° ì •í•©ì„± ê²€ì¦ ê²°ê³¼\")\n",
    "print(\"-\" * 50)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f8b85",
   "metadata": {},
   "source": [
    "## 4-3. Skewness Correction & Re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9127fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "skewed_features = ['HE_glu', 'HE_TG', 'HE_ast', 'HE_alt', 'HE_crea', 'N_EN']\n",
    "X_refined_df = pd.DataFrame(X_imputed, columns=valid_features)\n",
    "\n",
    "for col in skewed_features:\n",
    "    if col in X_refined_df.columns:\n",
    "        # ìŒìˆ˜ ê°’ì„ 0ìœ¼ë¡œ ê°•ì œ í´ë¦¬í•‘í•˜ì—¬ log1p ì—°ì‚° ì‹œ ë°œìƒí•˜ëŠ” NaN ì›ì²œ ì°¨ë‹¨\n",
    "        X_refined_df[col] = np.log1p(np.clip(X_refined_df[col], a_min=0, a_max=None))\n",
    "\n",
    "scaler_refined = StandardScaler()\n",
    "X_scaled_refined = scaler_refined.fit_transform(X_refined_df)\n",
    "\n",
    "X_tensor = torch.FloatTensor(X_scaled_refined).to(device)\n",
    "\n",
    "print(f\"âœ… ì£¼ìš” {len(skewed_features)}ê°œ ì§€í‘œ ë¡œê·¸ ë³€í™˜ ì™„ë£Œ (NaN ë°©ì§€ ì ìš©)\")\n",
    "print(f\"âœ… ìµœì¢… ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë° {device} í• ë‹¹ ì™„ë£Œ\")\n",
    "print(f\"âœ… NaN í¬í•¨ ì—¬ë¶€: {torch.isnan(X_tensor).any().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76960ac2",
   "metadata": {},
   "source": [
    "# 05. Model Training & VRAM Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "\n",
    "class PhenotypeAE(nn.Module) :\n",
    "    def __init__(self, input_dim : int, latent_dim : int = 8) :\n",
    "        super(PhenotypeAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x : torch.Tensor) :\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return latent, reconstructed\n",
    "\n",
    "cfg = {\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"epochs\" : 200,\n",
    "    \"batch_size\" : 512,\n",
    "    \"lr\" : 0.0005,\n",
    "    \"weight_decay\" : 1e-4,\n",
    "    \"latent_dim\" : 8\n",
    "}\n",
    "\n",
    "model = PhenotypeAE(input_dim = X_tensor.shape[1], latent_dim = cfg[\"latent_dim\"]).to(cfg[\"device\"])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = cfg[\"lr\"], weight_decay = cfg[\"weight_decay\"])\n",
    "\n",
    "dataset = TensorDataset(X_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size = cfg[\"batch_size\"], shuffle = True)\n",
    "\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘\")\n",
    "for epoch in range(cfg[\"epochs\"]) :\n",
    "    running_loss : float = 0.0\n",
    "    for batch in dataloader :\n",
    "        inputs = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        latent, reconstructed = model(inputs)\n",
    "        loss = criterion(reconstructed, inputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss : float = running_loss / len(dataloader)\n",
    "    if (epoch + 1) % 20 == 0 :\n",
    "        print(f\"Epoch [{epoch+1}/{cfg['epochs']}] | Loss : {avg_loss:.6f}\")\n",
    "\n",
    "print(f\"âœ… í•™ìŠµ ì™„ë£Œ (ì†Œìš” ì‹œê°„ : {time.time() - start_time:.2f}s)\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    latent_vectors, _ = model(X_tensor)\n",
    "    latent_np = latent_vectors.cpu().numpy()\n",
    "\n",
    "latent_cols = [f\"latent_{i}\" for i in range(cfg[\"latent_dim\"])]\n",
    "latent_df = pd.DataFrame(latent_np, columns = latent_cols)\n",
    "print(f\"âœ… ì ì¬ ë³€ìˆ˜ ì¶”ì¶œ ì™„ë£Œ : {latent_df.shape}\")\n",
    "\n",
    "del model, optimizer, dataloader, dataset\n",
    "if 'inputs' in locals() : del inputs\n",
    "if 'reconstructed' in locals() : del reconstructed\n",
    "if 'loss' in locals() : del loss\n",
    "if 'latent' in locals() : del latent\n",
    "if 'batch' in locals() : del batch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3d715",
   "metadata": {},
   "source": [
    "# 06. Latent Space Visualization (Streaming GPU t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "def limit_vram_usage(target_gb : float = 10.0, total_gb : float = 12.0) :\n",
    "    if torch.cuda.is_available() :\n",
    "        fraction : float = target_gb / total_gb\n",
    "        torch.cuda.set_per_process_memory_fraction(fraction, device = 0)\n",
    "        print(f\"ğŸ”’ PyTorch VRAM ì‚¬ìš© ìƒí•œì„  ì„¤ì • : {target_gb}GB\")\n",
    "\n",
    "def gpu_tsne_bulletproof(data : torch.Tensor, n_components : int = 2, lr : float = 100.0, iterations : int = 1000, chunk_size : int = 3000) :\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    n : int = data.shape[0]\n",
    "    X = data.to(device).float()\n",
    "    Y = torch.randn(n, n_components, device = device, dtype = torch.float32) * 1e-4\n",
    "    Y.requires_grad = True\n",
    "    \n",
    "    P_cpu = torch.zeros((n, n), dtype = torch.half)\n",
    "    print(f\"ğŸ“Š P-Matrix ì´ì¤‘ ë¶„í•  ì—°ì‚° ì¤‘... (Max Tensor : {chunk_size}x{chunk_size})\")\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        for i in range(0, n, chunk_size) :\n",
    "            end_i = min(i + chunk_size, n)\n",
    "            p_row_sum = torch.zeros((end_i - i, 1), device = device, dtype = torch.float32)\n",
    "            \n",
    "            for j in range(0, n, chunk_size) :\n",
    "                end_j = min(j + chunk_size, n)\n",
    "                dist_sub = torch.cdist(X[i : end_i], X[j : end_j]).pow(2)\n",
    "                \n",
    "                # ë¶„ì‚°(Variance) ì¶•ì†Œ : 8ì°¨ì› ì ì¬ ê³µê°„ ìŠ¤ì¼€ì¼ì— ë§ì¶˜ ë³´ì • ê°’(8.0) ì ìš©\n",
    "                p_sub = torch.exp(-dist_sub / 8.0)\n",
    "                \n",
    "                row_idx = torch.arange(i, end_i).unsqueeze(1)\n",
    "                col_idx = torch.arange(j, end_j).unsqueeze(0)\n",
    "                mask = (row_idx == col_idx).to(device)\n",
    "                p_sub.masked_fill_(mask, 0.0)\n",
    "                \n",
    "                P_cpu[i : end_i, j : end_j] = p_sub.half().cpu()\n",
    "                p_row_sum += p_sub.sum(dim = 1, keepdim = True)\n",
    "                \n",
    "                del dist_sub, p_sub, mask\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Zero Division ë°©ì§€ ì•ˆì „ì¥ì¹˜\n",
    "            P_cpu[i : end_i, :] /= (p_row_sum.cpu() + 1e-12)\n",
    "\n",
    "        P_cpu = (P_cpu + P_cpu.t()) / (2 * n)\n",
    "        P_cpu = torch.clamp(P_cpu, min = 1e-12)\n",
    "\n",
    "    optimizer = optim.SGD([Y], lr = lr, momentum = 0.8)\n",
    "    \n",
    "    print(f\"ğŸš€ ì•ˆì •í™”ëœ 2D Chunked GPU ìµœì í™” ì‹œì‘ (Iterations : {iterations})\")\n",
    "    for step in range(iterations) :\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        exaggeration : float = 12.0 if step < 250 else 1.0\n",
    "        \n",
    "        sum_inv_q : float = 0.0\n",
    "        with torch.no_grad() :\n",
    "            for i in range(0, n, chunk_size) :\n",
    "                end_i = min(i + chunk_size, n)\n",
    "                for j in range(0, n, chunk_size) :\n",
    "                    end_j = min(j + chunk_size, n)\n",
    "                    \n",
    "                    dy_sub = torch.cdist(Y[i : end_i], Y[j : end_j]).pow(2)\n",
    "                    inv_q_sub = 1.0 / (1.0 + dy_sub)\n",
    "                    \n",
    "                    row_idx = torch.arange(i, end_i).unsqueeze(1)\n",
    "                    col_idx = torch.arange(j, end_j).unsqueeze(0)\n",
    "                    mask = (row_idx == col_idx).to(device)\n",
    "                    inv_q_sub.masked_fill_(mask, 0.0)\n",
    "                    \n",
    "                    sum_inv_q += inv_q_sub.sum().item()\n",
    "                    del dy_sub, inv_q_sub, mask\n",
    "        \n",
    "        # [í•µì‹¬] NaN ì „íŒŒ ì›ì¸ ì°¨ë‹¨ : ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” í˜„ìƒ ë°©ì–´\n",
    "        sum_inv_q = max(sum_inv_q, 1e-12)\n",
    "        \n",
    "        total_loss : float = 0.0\n",
    "        grad_Y = torch.zeros_like(Y)\n",
    "        \n",
    "        with torch.no_grad() :\n",
    "            for i in range(0, n, chunk_size) :\n",
    "                end_i = min(i + chunk_size, n)\n",
    "                grad_chunk = torch.zeros_like(Y[i : end_i])\n",
    "                \n",
    "                for j in range(0, n, chunk_size) :\n",
    "                    end_j = min(j + chunk_size, n)\n",
    "                    \n",
    "                    p_sub = P_cpu[i : end_i, j : end_j].to(device).float() * exaggeration\n",
    "                    p_sub = torch.clamp(p_sub, min = 1e-12)\n",
    "                    \n",
    "                    dy_sub = torch.cdist(Y[i : end_i], Y[j : end_j]).pow(2)\n",
    "                    inv_q_sub = 1.0 / (1.0 + dy_sub)\n",
    "                    \n",
    "                    row_idx = torch.arange(i, end_i).unsqueeze(1)\n",
    "                    col_idx = torch.arange(j, end_j).unsqueeze(0)\n",
    "                    mask = (row_idx == col_idx).to(device)\n",
    "                    inv_q_sub.masked_fill_(mask, 0.0)\n",
    "                    \n",
    "                    q_sub = inv_q_sub / sum_inv_q\n",
    "                    q_sub = torch.clamp(q_sub, min = 1e-12)\n",
    "                    \n",
    "                    loss_sub = (p_sub * torch.log(p_sub / q_sub)).sum().item()\n",
    "                    total_loss += loss_sub\n",
    "                    \n",
    "                    pq_diff = 4.0 * (p_sub - q_sub) * inv_q_sub\n",
    "                    grad_chunk += pq_diff.sum(dim = 1, keepdim = True) * Y[i : end_i] - torch.mm(pq_diff, Y[j : end_j])\n",
    "                    \n",
    "                    del p_sub, dy_sub, inv_q_sub, q_sub, pq_diff, mask\n",
    "                \n",
    "                grad_Y[i : end_i] = grad_chunk\n",
    "                del grad_chunk\n",
    "        \n",
    "        # [í•µì‹¬] Gradient NaN ê°•ì œ ì¹˜í™˜ ë° í´ë¦¬í•‘\n",
    "        if torch.isnan(grad_Y).any() or torch.isinf(grad_Y).any() :\n",
    "            grad_Y = torch.nan_to_num(grad_Y, nan = 0.0, posinf = 1.0, neginf = -1.0)\n",
    "            \n",
    "        Y.grad = grad_Y\n",
    "        torch.nn.utils.clip_grad_norm_([Y], max_norm = 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step + 1) % 100 == 0 :\n",
    "            print(f\"Iteration [{step+1}/{iterations}] | Loss : {total_loss:.4f}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return Y.detach().cpu().numpy()\n",
    "\n",
    "# 1. 10GB ìƒí•œì„  ì„¤ì •\n",
    "limit_vram_usage()\n",
    "\n",
    "# 2. íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ êµ¬ì„±\n",
    "cfg_tsne = {\n",
    "    \"chunk_size\" : 3000,\n",
    "    \"lr\" : 100.0\n",
    "}\n",
    "\n",
    "# 3. ëª¨ë¸ ì‹¤í–‰\n",
    "latent_2d = gpu_tsne_bulletproof(latent_vectors, chunk_size = cfg_tsne[\"chunk_size\"], lr = cfg_tsne[\"lr\"])\n",
    "\n",
    "# 4. ê²°ê³¼ ì‹œê°í™”\n",
    "viz_df = pd.DataFrame(latent_2d, columns = ['tsne_1', 'tsne_2'])\n",
    "viz_df['diabetes'] = analysis_df['target_diabetes'].values\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "sns.scatterplot(x = 'tsne_1', y = 'tsne_2', hue = 'diabetes', data = viz_df, palette = 'coolwarm', alpha = 0.5, s = 20)\n",
    "plt.title('2D Chunked GPU t-SNE (Bulletproof Math Optimized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a5e19",
   "metadata": {},
   "source": [
    "# 07. Subtype Discovery (GPU GMM Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "class GPU_GMM :\n",
    "    def __init__(self, n_components : int, n_iters : int = 100, device : str = \"cuda:0\") :\n",
    "        self.n_components = n_components\n",
    "        self.n_iters = n_iters\n",
    "        self.device = device\n",
    "\n",
    "    def fit_predict(self, x : torch.Tensor) :\n",
    "        n_samples, n_features = x.shape\n",
    "        x = x.to(self.device).float()\n",
    "\n",
    "        self.pi = torch.ones(self.n_components, device = self.device) / self.n_components\n",
    "        \n",
    "        idx = torch.randperm(n_samples)[:self.n_components]\n",
    "        self.mu = x[idx, :].clone()\n",
    "        self.sigma = torch.eye(n_features, device = self.device).unsqueeze(0).repeat(self.n_components, 1, 1)\n",
    "\n",
    "        print(f\"ğŸ§ª GPU GMM í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (K : {self.n_components}, Iterations : {self.n_iters})\")\n",
    "        for i in range(self.n_iters) :\n",
    "            with torch.no_grad() :\n",
    "                probs = []\n",
    "                for k in range(self.n_components) :\n",
    "                    cov = self.sigma[k]\n",
    "                    cov = (cov + cov.t()) / 2.0\n",
    "                    cov = cov + torch.eye(n_features, device = self.device) * 1e-3\n",
    "                    \n",
    "                    dist = torch.distributions.MultivariateNormal(self.mu[k], cov)\n",
    "                    probs.append(dist.log_prob(x).exp() * self.pi[k])\n",
    "                \n",
    "                probs = torch.stack(probs, dim = 1)\n",
    "                gamma = probs / (probs.sum(dim = 1, keepdim = True) + 1e-12)\n",
    "\n",
    "                n_k = gamma.sum(dim = 0)\n",
    "                self.pi = n_k / n_samples\n",
    "                self.mu = torch.mm(gamma.t(), x) / n_k.view(-1, 1)\n",
    "                \n",
    "                for k in range(self.n_components) :\n",
    "                    diff = x - self.mu[k]\n",
    "                    # ì•ˆì „í•œ í–‰ë ¬ ê³±ì…ˆ êµ¬ì¡°ë¡œ ë³€ê²½\n",
    "                    cov_update = torch.mm((gamma[:, k].view(-1, 1) * diff).t(), diff) / (n_k[k] + 1e-12)\n",
    "                    self.sigma[k] = (cov_update + cov_update.t()) / 2.0\n",
    "\n",
    "        final_probs = []\n",
    "        for k in range(self.n_components) :\n",
    "            cov = self.sigma[k]\n",
    "            cov = (cov + cov.t()) / 2.0\n",
    "            cov = cov + torch.eye(n_features, device = self.device) * 1e-3\n",
    "            dist = torch.distributions.MultivariateNormal(self.mu[k], cov)\n",
    "            final_probs.append(dist.log_prob(x))\n",
    "        \n",
    "        return torch.stack(final_probs, dim = 1).argmax(dim = 1).cpu().numpy()\n",
    "\n",
    "cfg_gmm = {\n",
    "    \"n_components\" : 9,\n",
    "    \"n_iters\" : 150,\n",
    "    \"device\" : \"cuda:0\"\n",
    "}\n",
    "\n",
    "gmm_model = GPU_GMM(n_components = cfg_gmm[\"n_components\"], n_iters = cfg_gmm[\"n_iters\"], device = cfg_gmm[\"device\"])\n",
    "metabolic_subtypes = gmm_model.fit_predict(latent_vectors)\n",
    "\n",
    "analysis_df['metabolic_subtype'] = metabolic_subtypes\n",
    "\n",
    "summary_df = analysis_df.groupby('metabolic_subtype')['target_diabetes'].agg(['count', 'mean']).reset_index()\n",
    "summary_df.columns = ['Subtype', 'Count', 'Diabetes_Prevalence']\n",
    "summary_df = summary_df.sort_values(by = 'Diabetes_Prevalence', ascending = False)\n",
    "\n",
    "print(\"\\nğŸ“‹ ëŒ€ì‚¬ ì•„í˜•(Subtype)ë³„ ë‹¹ë‡¨ ìœ ë³‘ë¥  ìš”ì•½\")\n",
    "print(\"-\" * 55)\n",
    "print(summary_df.to_string(index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99070635",
   "metadata": {},
   "source": [
    "# 08. High Risk Subtype Prediction (TabNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b819963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "high_risk_clusters : list = [1, 7]\n",
    "analysis_df['is_high_risk'] = analysis_df['metabolic_subtype'].isin(high_risk_clusters).astype(int)\n",
    "\n",
    "X_data : np.ndarray = X_scaled_refined\n",
    "y_data : np.ndarray = analysis_df['is_high_risk'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size = 0.2, random_state = 42, stratify = y_data\n",
    ")\n",
    "\n",
    "tabnet_params : dict = {\n",
    "    \"n_d\" : 64,\n",
    "    \"n_a\" : 64,\n",
    "    \"n_steps\" : 5,\n",
    "    \"gamma\" : 1.5,\n",
    "    \"n_independent\" : 2,\n",
    "    \"n_shared\" : 2,\n",
    "    \"lambda_sparse\" : 1e-4,\n",
    "    \"optimizer_fn\" : torch.optim.AdamW,\n",
    "    \"optimizer_params\" : {\"lr\" : 2e-2, \"weight_decay\" : 1e-4},\n",
    "    \"scheduler_fn\" : torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "    \"scheduler_params\" : {\"T_max\" : 150},\n",
    "    \"device_name\" : \"cuda\",\n",
    "    \"verbose\" : 1  # ë¡œê·¸ í™œì„±í™” (ì§„í–‰ë¥  í™•ì¸ìš©)\n",
    "}\n",
    "\n",
    "clf = TabNetClassifier(**tabnet_params)\n",
    "\n",
    "print(\"ğŸš€ TabNet ê³ ìœ„í—˜êµ° ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (num_workers=0 ìµœì í™”)\")\n",
    "\n",
    "clf.fit(\n",
    "    X_train = X_train, y_train = y_train,\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)],\n",
    "    eval_name = ['train', 'valid'],\n",
    "    eval_metric = ['auc'],\n",
    "    max_epochs = 150,\n",
    "    patience = 30,\n",
    "    batch_size = 4096,\n",
    "    virtual_batch_size = 512,\n",
    "    num_workers = 0,  # Windows ë°ë“œë½ ì›ì¸ ì°¨ë‹¨ ë° ì¸ë©”ëª¨ë¦¬ ì§ê²°\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "preds_proba = clf.predict_proba(X_test)[ : , 1]\n",
    "preds_class = clf.predict(X_test)\n",
    "\n",
    "auc_score : float = roc_auc_score(y_test, preds_proba)\n",
    "print(f\"\\nâœ… TabNet ìµœì í™” ì™„ë£Œ | Test AUC : {round(auc_score, 4)}\")\n",
    "print(\"\\nğŸ“Š Classification Report\")\n",
    "print(classification_report(y_test, preds_class))\n",
    "\n",
    "feat_importances : np.ndarray = clf.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature' : valid_features,\n",
    "    'Importance' : feat_importances\n",
    "}).sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "print(\"\\nğŸ” Top 10 ì£¼ìš” ì„ìƒ ì§€í‘œ (High-Risk íŒë³„ ê¸°ì—¬ë„)\")\n",
    "print(\"-\" * 55)\n",
    "print(importance_df.head(10).to_string(index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9327fb",
   "metadata": {},
   "source": [
    "# 09. Clinical Evaluation Dashboard (XAI & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9031441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, roc_auc_score\n",
    "\n",
    "def plot_clinical_dashboard_v6(y_true : np.ndarray, y_prob : np.ndarray, y_pred : np.ndarray, imp_df : pd.DataFrame) :\n",
    "    auc_val : float = roc_auc_score(y_true, y_prob)\n",
    "    fig, axes = plt.subplots(1, 3, figsize = ( 22, 6 ))\n",
    "    \n",
    "    # 1. ROC Curve Analysis\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    axes[ 0 ].plot(fpr, tpr, color = \"darkred\", lw = 2.5, label = f\"TabNet (AUC = {auc_val:.4f})\")\n",
    "    axes[ 0 ].plot([ 0, 1 ], [ 0, 1 ], color = \"gray\", linestyle = \"--\", alpha = 0.5)\n",
    "    axes[ 0 ].fill_between(fpr, tpr, alpha = 0.1, color = \"red\")\n",
    "    axes[ 0 ].set_title(\"ROC Curve Analysis\", fontsize = 15, fontweight = \"bold\")\n",
    "    axes[ 0 ].set_xlabel(\"False Positive Rate\")\n",
    "    axes[ 0 ].set_ylabel(\"True Positive Rate\")\n",
    "    axes[ 0 ].legend(loc = \"lower right\")\n",
    "    axes[ 0 ].grid(True, alpha = 0.2)\n",
    "    \n",
    "    # 2. Confusion Matrix\n",
    "    cm : np.ndarray = confusion_matrix(y_true, y_pred)\n",
    "    cm_ratio : np.ndarray = cm.astype(\"float\") / cm.sum(axis = 1)[ :, np.newaxis ]\n",
    "    labels : np.ndarray = np.array([ [ f\"{v}\\n({r:.1%})\" for v, r in zip(row_v, row_r) ] for row_v, row_r in zip(cm, cm_ratio) ])\n",
    "    \n",
    "    sns.heatmap(cm, annot = labels, fmt = \"\", cmap = \"Blues\", ax = axes[ 1 ], cbar = False, \n",
    "                annot_kws = { \"size\" : 14, \"fontweight\" : \"bold\" })\n",
    "    axes[ 1 ].set_title(\"Confusion Matrix (Count & %)\", fontsize = 15, fontweight = \"bold\")\n",
    "    axes[ 1 ].set_xticklabels([ \"Safe\", \"High-Risk\" ])\n",
    "    axes[ 1 ].set_yticklabels([ \"Safe\", \"High-Risk\" ])\n",
    "    \n",
    "    # 3. Feature Importance\n",
    "    top_10 : pd.DataFrame = imp_df.head(10).copy()\n",
    "    sns.barplot(x = \"Importance\", y = \"Feature\", data = top_10, ax = axes[ 2 ], palette = \"viridis\", hue = \"Feature\", legend = False)\n",
    "    axes[ 2 ].set_title(\"Top 10 Clinical Biomarkers\", fontsize = 15, fontweight = \"bold\")\n",
    "    \n",
    "    for i, v in enumerate(top_10[ \"Importance\" ]) :\n",
    "        axes[ 2 ].text(v + 0.002, i, f\"{v:.4f}\", va = \"center\", fontsize = 11, fontweight = \"bold\")\n",
    "    \n",
    "    axes[ 2 ].set_xlim(0, top_10[ \"Importance\" ].max() * 1.15)\n",
    "    axes[ 2 ].grid(axis = \"x\", alpha = 0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    target_dir : str = r\"C:\\Users\\sck32\\Desktop\\Clinical Diabetes TabNet\"\n",
    "    try :\n",
    "        os.makedirs(target_dir, exist_ok = True)\n",
    "        os.chdir(target_dir)\n",
    "        \n",
    "        save_path : str = \"clinical_dashboard.png\"\n",
    "        fig.savefig(save_path, dpi = 300, bbox_inches = \"tight\")\n",
    "        print(f\"âœ… Dashboard saved strictly to : {os.path.abspath(save_path)}\")\n",
    "    except Exception as e :\n",
    "        print(f\"âŒ Cannot save image. OS blocked file writing. (Error : {e})\")\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "plot_clinical_dashboard_v6(y_test, preds_proba, preds_class, importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0778881",
   "metadata": {},
   "source": [
    "# 10. Pipeline Checkpoint Export (Persistence Fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. ëª¨ë¸ í´ë˜ìŠ¤ ì¬ì„ ì–¸\n",
    "class PhenotypeAE(nn.Module) :\n",
    "    def __init__(self, input_dim : int, latent_dim : int = 8) :\n",
    "        super(PhenotypeAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x : torch.Tensor) :\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return latent, reconstructed\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "base_dir : str = r\"C:\\Users\\sck32\\Desktop\\Clinical Diabetes TabNet\"\n",
    "target_path : Path = Path(base_dir) / \"metabolic_ai_models\"\n",
    "\n",
    "try :\n",
    "    target_path.mkdir(parents = True, exist_ok = True)\n",
    "    export_path : str = str(target_path)\n",
    "    os.chdir(base_dir) # ì»¤ë„ ê²½ë¡œ ê°•ì œ ë™ê¸°í™”\n",
    "    print(f\"ğŸ“‚ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ìˆ˜ì¶œ ì‹œì‘ : {export_path}\")\n",
    "except Exception as e :\n",
    "    # 1ìˆœìœ„ ê²½ë¡œ ì‹¤íŒ¨ ì‹œ ë°”íƒ•í™”ë©´ìœ¼ë¡œ ê°•ì œ ìš°íšŒ\n",
    "    fallback_path : Path = Path.home() / \"Desktop\" / \"metabolic_ai_models\"\n",
    "    fallback_path.mkdir(parents = True, exist_ok = True)\n",
    "    export_path : str = str(fallback_path)\n",
    "    print(f\"âš ï¸ ê¶Œí•œ ë˜ëŠ” ê²½ë¡œ ì˜¤ë¥˜ë¡œ ì¸í•´ ë°”íƒ•í™”ë©´ìœ¼ë¡œ ìš°íšŒí•©ë‹ˆë‹¤ : {export_path}\")\n",
    "\n",
    "# 2. íŒŒì´í”„ë¼ì¸ ì—ì…‹ ì €ì¥\n",
    "if 'model_ae' in locals() :\n",
    "    torch.save(model_ae.state_dict(), f\"{export_path}/ae_weights.pt\")\n",
    "    print(\" [1/4] Autoencoder Weights ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "if 'gmm_model' in locals() :\n",
    "    gmm_assets : dict = {\n",
    "        \"mu\" : gmm_model.mu.cpu(),\n",
    "        \"sigma\" : gmm_model.sigma.cpu(),\n",
    "        \"pi\" : gmm_model.pi.cpu()\n",
    "    }\n",
    "    torch.save(gmm_assets, f\"{export_path}/gmm_metabolic_criteria.pt\")\n",
    "    print(\" [2/4] GMM Parameters ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "if 'clf' in locals() :\n",
    "    clf.save_model(f\"{export_path}/tabnet_high_risk_model\")\n",
    "    print(\" [3/4] TabNet ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "if 'scaler_refined' in locals() :\n",
    "    joblib.dump(scaler_refined, f\"{export_path}/clinical_scaler.pkl\")\n",
    "    print(\" [4/4] Data Scaler ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "print(\"\\nâœ… íŒŒì´í”„ë¼ì¸ ì—ì…‹ ì €ì¥ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db88fc",
   "metadata": {},
   "source": [
    "# 11. Real-time Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d712f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "def diagnose_metabolic_risk(new_patient_data : pd.DataFrame) :\n",
    "    # 1. ê²½ë¡œ ìš°ì„ ìˆœìœ„ ì„¤ì • (ë°”íƒ•í™”ë©´ ìš°ì„  íƒìƒ‰)\n",
    "    desktop_path : Path = Path.home() / \"Desktop\" / \"metabolic_ai_models\"\n",
    "    local_path : Path = Path(os.getcwd()) / \"metabolic_ai_models\"\n",
    "    \n",
    "    if (desktop_path / \"clinical_scaler.pkl\").exists() :\n",
    "        model_dir : Path = desktop_path\n",
    "    elif (local_path / \"clinical_scaler.pkl\").exists() :\n",
    "        model_dir : Path = local_path\n",
    "    else :\n",
    "        raise FileNotFoundError(f\"âŒ ëª¨ë¸ ì—ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í™•ì¸ ê²½ë¡œ : {desktop_path})\")\n",
    "\n",
    "    # 2. ì—ì…‹ ë¡œë“œ ë° ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    scaler = joblib.load(model_dir / \"clinical_scaler.pkl\")\n",
    "    \n",
    "    # RTX 4070 12GB í™˜ê²½ì„ ê³ ë ¤í•œ ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "    device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    clf = TabNetClassifier(device_name = device)\n",
    "    clf.load_model(str(model_dir / \"tabnet_high_risk_model.zip\"))\n",
    "    \n",
    "    # 3. ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (í•™ìŠµ ë‹¨ê³„ì™€ ì™„ë²½ ë§¤ì¹­)\n",
    "    X_raw = new_patient_data[ valid_features ].copy()\n",
    "    X_raw = X_raw.fillna(0) \n",
    "    \n",
    "    # ì™œë„ ë³´ì • ë° ìŒìˆ˜ í´ë¦¬í•‘\n",
    "    skewed_features : list = [ \"HE_glu\", \"HE_TG\", \"HE_ast\", \"HE_alt\", \"HE_crea\", \"N_EN\" ]\n",
    "    for col in skewed_features :\n",
    "        if col in X_raw.columns :\n",
    "            X_raw[ col ] = np.log1p(np.clip(X_raw[ col ], a_min = 0, a_max = None))\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§ ë° FP32 ìµœì í™”\n",
    "    X_scaled : np.ndarray = scaler.transform(X_raw).astype(np.float32)\n",
    "    \n",
    "    # CUDA Assert ë°©ì§€ìš© ìµœì¢… ê²€ì‚¬\n",
    "    if np.isnan(X_scaled).any() or np.isinf(X_scaled).any() :\n",
    "        X_scaled = np.nan_to_num(X_scaled)\n",
    "    \n",
    "    # 4. ì •ë°€ ì¶”ë¡  ìˆ˜í–‰\n",
    "    risk_probs : np.ndarray = clf.predict_proba(X_scaled)[ :, 1 ]\n",
    "    risk_preds : np.ndarray = clf.predict(X_scaled)\n",
    "    \n",
    "    # 5. ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    results : pd.DataFrame = new_patient_data.copy()\n",
    "    results[ \"High_Risk_Probability\" ] = risk_probs\n",
    "    results[ \"Diagnosis_Result\" ] = [ \"High-Risk\" if p == 1 else \"Safe\" for p in risk_preds ]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "try :\n",
    "    test_patients : pd.DataFrame = analysis_df.iloc[ :5 ] \n",
    "    diagnosis_report : pd.DataFrame = diagnose_metabolic_risk(test_patients)\n",
    "\n",
    "    print(f\"ğŸ”¬ ì‹ ê·œ í™˜ì ì •ë°€ ì§„ë‹¨ ê²°ê³¼ ë¦¬í¬íŠ¸ (Device : {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})\")\n",
    "    print(\"-\" * 80)\n",
    "    print(diagnosis_report[ [ \"ID\", \"age\", \"High_Risk_Probability\", \"Diagnosis_Result\" ] ].to_string(index = False))\n",
    "    print(\"-\" * 80)\n",
    "except Exception as e :\n",
    "    print(f\"âŒ ì§„ë‹¨ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜ : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7294b6",
   "metadata": {},
   "source": [
    "# 12. Full-scale Diagnosis & CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01826964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. ì „ì²´ ë°ì´í„° ì§„ë‹¨ ìˆ˜í–‰\n",
    "print(f\"ğŸš€ ì „ì²´ ë°ì´í„°({len(analysis_df)}ëª…) ì •ë°€ ì§„ë‹¨ ì‹œì‘...\")\n",
    "full_diagnosis_results = diagnose_metabolic_risk(analysis_df)\n",
    "\n",
    "# 2. CSV íŒŒì¼ ì €ì¥ (BOM í¬í•¨ UTF-8ë¡œ ì—‘ì…€ í˜¸í™˜ì„± í™•ë³´)\n",
    "output_filename : str = \"final_metabolic_diagnosis_report.csv\"\n",
    "full_diagnosis_results.to_csv(output_filename, index = False, encoding = \"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… ì§„ë‹¨ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ : {os.path.abspath(output_filename)}\")\n",
    "\n",
    "# ê³ ìœ„í—˜êµ° í†µê³„ ìš”ì•½\n",
    "high_risk_count : int = full_diagnosis_results['Diagnosis_Result'].value_counts().get('High-Risk', 0)\n",
    "print(f\"ğŸ“Š ìµœì¢… ì‹ë³„ëœ ê³ ìœ„í—˜êµ° : {high_risk_count}ëª… ({high_risk_count/len(full_diagnosis_results)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1461da",
   "metadata": {},
   "source": [
    "# 13. Individual Patient Risk Analysis (Local XAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def explain_patient_risk(patient_id : str, model_dir : str = \"./metabolic_ai_models\") :\n",
    "    # 1. ëŒ€ìƒ í™˜ì ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬\n",
    "    patient_raw = analysis_df[analysis_df['ID'] == patient_id].copy()\n",
    "    if patient_raw.empty :\n",
    "        print(\"âŒ í•´ë‹¹ IDì˜ í™˜ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    scaler = joblib.load(f\"{model_dir}/clinical_scaler.pkl\")\n",
    "    X_raw = patient_raw[valid_features].copy().fillna(0)\n",
    "    \n",
    "    skewed_features = ['HE_glu', 'HE_TG', 'HE_ast', 'HE_alt', 'HE_crea', 'N_EN']\n",
    "    for col in skewed_features :\n",
    "        X_raw[col] = np.log1p(np.clip(X_raw[col], a_min = 0, a_max = None))\n",
    "    \n",
    "    X_scaled = scaler.transform(X_raw).astype(np.float32)\n",
    "    \n",
    "    # 2. TabNet Local Explainability ì¶”ì¶œ\n",
    "    explain_matrix, masks = clf.explain(X_scaled)\n",
    "    \n",
    "    # 3. ì¤‘ìš”ë„ ë§¤í•‘ ë° ì‹œê°í™”\n",
    "    plt.figure(figsize = (12, 6))\n",
    "    importance_scores = explain_matrix[0]\n",
    "    local_imp_df = pd.DataFrame({\n",
    "        'Feature' : valid_features,\n",
    "        'Contribution' : importance_scores\n",
    "    }).sort_values(by = 'Contribution', ascending = False)\n",
    "    \n",
    "    sns.barplot(x = 'Contribution', y = 'Feature', data = local_imp_df.head(10), palette = \"flare\")\n",
    "    \n",
    "    # ìˆ˜ì¹˜ í‘œê¸° ì¶”ê°€\n",
    "    for i, v in enumerate(local_imp_df.head(10)['Contribution']) :\n",
    "        plt.text(v + 0.005, i, f'{v:.4f}', va = 'center', fontweight = 'bold')\n",
    "        \n",
    "    plt.title(f\"Detailed Risk Factor Analysis | Patient ID : {patient_id}\", fontsize = 15, fontweight = 'bold')\n",
    "    plt.xlabel(\"Local Attention Weight (Model's Focus)\")\n",
    "    plt.grid(axis = 'x', alpha = 0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return local_imp_df.head(10)\n",
    "\n",
    "# ì‹¤í–‰ : ê³ ìœ„í—˜êµ° í™˜ì ì •ë°€ ë¶„ì„\n",
    "top_factors = explain_patient_risk(\"A751220401\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba8a2d",
   "metadata": {},
   "source": [
    "# 14. Group Statistical Analysis (High-Risk vs. Safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows : Malgun Gothic)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def perform_group_analysis_v2(df : pd.DataFrame, target_features : list) :\n",
    "    # 1. ì˜ë¬¸ ì½”ë“œ -> í•œê¸€ ëª…ì¹­ ë§¤í•‘ ì‚¬ì „\n",
    "    feature_map : dict = {\n",
    "        'sm_presnt' : 'í˜„ì¬ í¡ì—° ì—¬ë¶€',\n",
    "        'sex' : 'ì„±ë³„',\n",
    "        'HE_hepaB' : 'Bí˜•ê°„ì—¼ ì–‘ì„±',\n",
    "        'N_CHO' : 'íƒ„ìˆ˜í™”ë¬¼ ì„­ì·¨(g)',\n",
    "        'HE_TG' : 'ì¤‘ì„±ì§€ë°©(mg/dL)',\n",
    "        'N_PROT' : 'ë‹¨ë°±ì§ˆ ì„­ì·¨(g)',\n",
    "        'HE_wc' : 'í—ˆë¦¬ë‘˜ë ˆ(cm)',\n",
    "        'HE_HDL_st2' : 'HDL ì½œë ˆìŠ¤í…Œë¡¤',\n",
    "        'HE_chol' : 'ì´ ì½œë ˆìŠ¤í…Œë¡¤',\n",
    "        'HE_ast' : 'AST(ê°„ìˆ˜ì¹˜)'\n",
    "    }\n",
    "    \n",
    "    # 2. í†µê³„ ë°ì´í„° ê³„ì‚°\n",
    "    stats = df.groupby('Diagnosis_Result')[target_features].mean().T\n",
    "    \n",
    "    # 3. T-Test ìˆ˜í–‰\n",
    "    p_values : list = []\n",
    "    for feat in target_features :\n",
    "        safe_g = df[df['Diagnosis_Result'] == 'Safe'][feat].dropna()\n",
    "        risk_g = df[df['Diagnosis_Result'] == 'High-Risk'][feat].dropna()\n",
    "        _, p_val = ttest_ind(safe_g, risk_g, equal_var = False)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    # 4. ì‹œê°í™” ë°ì´í„° êµ¬ì„±\n",
    "    melted_df = df.melt(id_vars = 'Diagnosis_Result', value_vars = target_features)\n",
    "    melted_df['variable'] = melted_df['variable'].map(feature_map)\n",
    "    \n",
    "    # 5. ì‹œê°í™” ì‹¤í–‰\n",
    "    plt.figure(figsize = (16, 8))\n",
    "    ax = sns.barplot(\n",
    "        x = 'variable', y = 'value', hue = 'Diagnosis_Result', \n",
    "        data = melted_df, palette = 'coolwarm', edgecolor = 'black'\n",
    "    )\n",
    "    \n",
    "    # ê·¸ë˜í”„ ìƒë‹¨ì— ìˆ˜ì¹˜(Mean) í‘œì‹œ\n",
    "    for p in ax.patches :\n",
    "        height = p.get_height()\n",
    "        if height > 0 :\n",
    "            ax.text(\n",
    "                p.get_x() + p.get_width() / 2., height + (height * 0.01),\n",
    "                f'{height:.2f}', ha = \"center\", fontsize = 10, fontweight = 'bold'\n",
    "            )\n",
    "    \n",
    "    plt.title(\"ê³ ìœ„í—˜êµ° vs ì¼ë°˜êµ° ì£¼ìš” ì„ìƒ ì§€í‘œ ë¹„êµ (ì§‘ë‹¨ í‰ê· )\", fontsize = 16, fontweight = 'bold', pad = 20)\n",
    "    plt.xlabel(\"ì„ìƒ ì§€í‘œ (Biomarkers)\", fontsize = 12)\n",
    "    plt.ylabel(\"í‰ê·  ìˆ˜ì¹˜ (Mean Value)\", fontsize = 12)\n",
    "    plt.legend(title = \"ì§„ë‹¨ ê²°ê³¼\", loc = 'upper right')\n",
    "    plt.xticks(rotation = 0) # í•œê¸€ ëª…ì¹­ì´ë¯€ë¡œ ê°€ë…ì„±ì„ ìœ„í•´ íšŒì „ ì œê±°\n",
    "    plt.grid(axis = 'y', alpha = 0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±\n",
    "    summary_df = pd.DataFrame({\n",
    "        'ì§€í‘œëª…' : [feature_map.get(f, f) for f in target_features],\n",
    "        'ì¼ë°˜êµ° í‰ê· ' : stats['Safe'].values,\n",
    "        'ê³ ìœ„í—˜êµ° í‰ê· ' : stats['High-Risk'].values,\n",
    "        'P-Value' : p_values\n",
    "    })\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# ì‹¤í–‰\n",
    "top_10_features = [\n",
    "    'sm_presnt', 'sex', 'HE_hepaB', 'N_CHO', 'HE_TG', \n",
    "    'N_PROT', 'HE_wc', 'HE_HDL_st2', 'HE_chol', 'HE_ast'\n",
    "]\n",
    "group_comparison_v2 = perform_group_analysis_v2(full_diagnosis_results, top_10_features)\n",
    "\n",
    "print(\"ğŸ“‹ ì§‘ë‹¨ ê°„ í†µê³„ì  ìœ ì˜ì„± ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "print(\"-\" * 80)\n",
    "print(group_comparison_v2.to_string(index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e778a6b",
   "metadata": {},
   "source": [
    "# 15. Multi Disease Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80890ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# í•˜ë“œì›¨ì–´ ìµœì í™” í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "def run_final_disease_comparison_labeled(df : pd.DataFrame, target_map : dict) :\n",
    "    # 1. AI ìœ„í—˜ë„ ë“±ê¸‰(Decile) ìƒì„± (128GB RAM ìµœì í™” ì—°ì‚°)\n",
    "    df = df.copy()\n",
    "    df[ \"Risk_Decile\" ] = pd.qcut(df[ \"High_Risk_Probability\" ], 10, labels = False, duplicates = \"drop\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 2. ì‹œê°í™” ì´ˆê¸° ì„¤ì •\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    \n",
    "    # 3. ì§ˆí™˜ë³„ ë¶„ì„ ë° ì‹œê°í™”\n",
    "    for col_name, label in target_map.items() :\n",
    "        if col_name not in df.columns :\n",
    "            continue\n",
    "            \n",
    "        temp_df = df[[ col_name, \"High_Risk_Probability\", \"Risk_Decile\" ]].dropna()\n",
    "        y_true = (temp_df[ col_name ] >= (3 if \"HE_\" in col_name else 1)).astype(int)\n",
    "        y_prob = temp_df[ \"High_Risk_Probability\" ].values\n",
    "        \n",
    "        # í†µê³„ ì§€í‘œ ì‚°ì¶œ\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        corr, _ = spearmanr(y_true, y_prob)\n",
    "        \n",
    "        results.append({\n",
    "            \"ì§ˆí™˜ëª…\" : label,\n",
    "            \"AUC\" : round(auc, 4),\n",
    "            \"Spearman_Corr\" : round(corr, 4),\n",
    "            \"ìœ ë³‘ììˆ˜\" : int(y_true.sum())\n",
    "        })\n",
    "        \n",
    "        # ë“±ê¸‰ë³„ ìœ ë³‘ë¥  ê³„ì‚° ë° ì‹œê°í™”\n",
    "        temp_stats = df.copy()\n",
    "        temp_stats[ \"is_target\" ] = (temp_stats[ col_name ] >= (3 if \"HE_\" in col_name else 1)).astype(int)\n",
    "        trend = temp_stats.groupby( \"Risk_Decile\" )[ \"is_target\" ].mean()\n",
    "        \n",
    "        # ë¼ì¸ í”Œë¡¯\n",
    "        line = plt.plot(trend.index, trend.values, marker = \"o\", label = f\"{label} (AUC : {auc:.4f})\", lw = 3, markersize = 8)\n",
    "        color = line[ 0 ].get_color()\n",
    "        \n",
    "        # ìˆ˜ì¹˜ í‘œì‹œ (Data Labels)\n",
    "        for x, y in zip(trend.index, trend.values) :\n",
    "            plt.text(\n",
    "                x, y + 0.015, f\"{y:.3f}\", \n",
    "                ha = \"center\", va = \"bottom\", \n",
    "                fontsize = 10, fontweight = \"bold\", color = color\n",
    "            )\n",
    "\n",
    "    # 4. ê·¸ë˜í”„ ë””í…Œì¼ ì„¤ì •\n",
    "    plt.title( \"AI ìœ„í—˜ ë“±ê¸‰ë³„ ì‹¤ì œ ì§ˆí™˜ ìœ ë³‘ë¥  ì¶”ì´ (ì •ë°€ ìˆ˜ì¹˜ í¬í•¨)\", fontsize = 18, fontweight = \"bold\", pad = 20 )\n",
    "    plt.xlabel( \"AI Risk Decile (0 : ì €ìœ„í—˜ -> 9 : ê³ ìœ„í—˜)\", fontsize = 12 )\n",
    "    plt.ylabel( \"ì‹¤ì œ ìœ ë³‘ë¥  (Prevalence Rate)\", fontsize = 12 )\n",
    "    plt.xticks(range( 10 ))\n",
    "    plt.ylim(0, 1.1) # ê°€ë…ì„±ì„ ìœ„í•´ ìƒë‹¨ ì—¬ë°± í™•ë³´\n",
    "    plt.legend(loc = \"upper right\", fontsize = 11, frameon = True, shadow = True)\n",
    "    plt.grid(True, axis = \"y\", alpha = 0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ë¶„ì„ ì‹¤í–‰\n",
    "target_disease_map = {\n",
    "    \"diabetes\" : \"ë‹¹ë‡¨ë³‘ (í•™ìŠµ íƒ€ê²Ÿ)\",\n",
    "    \"HE_HP\" : \"ê³ í˜ˆì••\",\n",
    "    \"HE_chol\" : \"ê³ ì½œë ˆìŠ¤í…Œë¡¤í˜ˆì¦\"\n",
    "}\n",
    "\n",
    "summary_table = run_final_disease_comparison_labeled(full_diagnosis_results, target_disease_map)\n",
    "\n",
    "print( \"\\nğŸ“Š ë‹¤ì¤‘ ì§ˆí™˜ ìƒê´€ì„± ì •ë°€ ë¶„ì„ ë¦¬í¬íŠ¸\" )\n",
    "print( \"-\" * 85 )\n",
    "print(summary_table.to_string(index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424180d2",
   "metadata": {},
   "source": [
    "# 16. Multi Disease Risk Weight Re-adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def adjust_metabolic_weights(df : pd.DataFrame, stats_df : pd.DataFrame) :\n",
    "    # 1. ì§ˆí™˜ë³„ ê°€ì¤‘ì¹˜ ì¶”ì¶œ (ìƒê´€ê³„ìˆ˜ì˜ ì ˆëŒ€ê°’ ê¸°ë°˜)\n",
    "    # Spearman_Corrê°€ NaNì¸ ê³ ì½œë ˆìŠ¤í…Œë¡¤ì€ ìµœì†Œ ê°€ì¤‘ì¹˜(0.01) ë¶€ì—¬\n",
    "    weights = {\n",
    "        'diabetes' : 1.0, # ê¸°ì¤€ì \n",
    "        'HE_HP' : abs(stats_df.loc[stats_df['ì§ˆí™˜ëª…'] == 'ê³ í˜ˆì••', 'Spearman_Corr'].values[0]),\n",
    "        'HE_chol' : 0.05   # í¬í™” ë°ì´í„°ì— ëŒ€í•œ ë³´ì • ê°€ì¤‘ì¹˜\n",
    "    }\n",
    "    \n",
    "    # ê°€ì¤‘ì¹˜ ì •ê·œí™”\n",
    "    total_w = sum(weights.values())\n",
    "    norm_weights = { k : v / total_w for k, v in weights.items() }\n",
    "    \n",
    "    # 2. ì§ˆí™˜ë³„ ê°œë³„ ìœ„í—˜ë„ ì‚°ì¶œ\n",
    "    # ê³ í˜ˆì••ì€ ì—­ìƒê´€ ê´€ê³„ì´ë¯€ë¡œ (1 - Prob)ë¥¼ í™œìš©í•´ ëŒ€ì‚¬ì  ë¶ˆì¼ì¹˜ ë³´ì •\n",
    "    p_dm = df['High_Risk_Probability'].values\n",
    "    p_hp = 1 - p_dm # ì—­ìƒê´€ ê´€ê³„ ë°˜ì˜ (í˜ˆì•• íŠ¹ì´ì  ìœ„í—˜ë„ ì‹œë®¬ë ˆì´ì…˜)\n",
    "    p_chol = np.full_like(p_dm, 1.0) # ì „ì› ìœ ë³‘ ìƒíƒœ ë°˜ì˜\n",
    "    \n",
    "    # 3. ìµœì¢… í†µí•© ëŒ€ì‚¬ ìœ„í—˜ë„ ê³„ì‚° (FP16 ìµœì í™” ì—°ì‚°)\n",
    "    df['Adjusted_Total_Risk'] = (\n",
    "        (p_dm * norm_weights['diabetes']) +\n",
    "        (p_hp * norm_weights['HE_HP']) +\n",
    "        (p_chol * norm_weights['HE_chol'])\n",
    "    ).astype(np.float16)\n",
    "    \n",
    "    # 4. ì¬ì¡°ì • ê²°ê³¼ì— ë”°ë¥¸ ìœ„í—˜êµ° ì¬ë¶„ë¥˜ (ìƒìœ„ 15% ê¸°ì¤€)\n",
    "    threshold = np.percentile(df['Adjusted_Total_Risk'], 85)\n",
    "    df['Final_Metabolic_Grade'] = df['Adjusted_Total_Risk'].apply(\n",
    "        lambda x : 'Critical' if x >= threshold else ('Warning' if x >= 0.5 else 'Normal')\n",
    "    )\n",
    "    \n",
    "    return df, norm_weights\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ì¬ì¡°ì • ì‹¤í–‰\n",
    "full_diagnosis_results, final_weights = adjust_metabolic_weights(full_diagnosis_results, summary_table)\n",
    "\n",
    "print(\"âš–ï¸ ì§ˆí™˜ë³„ ìµœì¢… ë°˜ì˜ ê°€ì¤‘ì¹˜ (Normalized)\")\n",
    "for k, v in final_weights.items() :\n",
    "    print(f\" - {k} : {v:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š ê°€ì¤‘ì¹˜ ì¬ì¡°ì • í›„ ìœ„í—˜ ë“±ê¸‰ ë¶„í¬\")\n",
    "print(full_diagnosis_results['Final_Metabolic_Grade'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ad5d2",
   "metadata": {},
   "source": [
    "# 17. Metabolic Risk Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# í°íŠ¸ ë° ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "plt.rcParams[\"font.family\"] = \"Malgun Gothic\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "plt.figure(figsize = (14, 7))\n",
    "\n",
    "# 1. KDE Plot ìƒì„± (ì›ë³¸ vs ì¬ì¡°ì •)\n",
    "sns.kdeplot(full_diagnosis_results['High_Risk_Probability'], \n",
    "            label = 'Raw Diabetes Prob', fill = True, alpha = 0.3, color = '#1f77b4')\n",
    "sns.kdeplot(full_diagnosis_results['Adjusted_Total_Risk'], \n",
    "            label = 'Adjusted Total Risk', fill = True, alpha = 0.4, color = '#ff7f0e')\n",
    "\n",
    "# 2. í†µê³„ì¹˜ ê³„ì‚°\n",
    "mean_raw = full_diagnosis_results['High_Risk_Probability'].mean()\n",
    "mean_adj = full_diagnosis_results['Adjusted_Total_Risk'].mean()\n",
    "# Critical ë“±ê¸‰ì˜ ìµœì†Œ ì ìˆ˜ë¥¼ ì„ê³„ì¹˜ë¡œ ì„¤ì •\n",
    "threshold = full_diagnosis_results.loc[full_diagnosis_results['Final_Metabolic_Grade'] == 'Critical', \n",
    "                                     'Adjusted_Total_Risk'].min()\n",
    "\n",
    "# 3. í‰ê· ì„ (Mean) ë° ìˆ˜ì¹˜ í‘œì‹œ\n",
    "plt.axvline(mean_raw, color = '#1f77b4', linestyle = '--', lw = 2)\n",
    "plt.text(mean_raw + 0.02, 7, f'Raw Mean: {mean_raw:.3f}', \n",
    "         color = '#1f77b4', fontweight = 'bold', fontsize = 11)\n",
    "\n",
    "plt.axvline(mean_adj, color = '#ff7f0e', linestyle = '--', lw = 2)\n",
    "plt.text(mean_adj + 0.02, 9, f'Adj Mean: {mean_adj:.3f}', \n",
    "         color = '#ff7f0e', fontweight = 'bold', fontsize = 11)\n",
    "\n",
    "# 4. ì„ê³„ì¹˜(Critical Threshold) ë° ìœ„í—˜ êµ¬ì—­ í‘œì‹œ\n",
    "plt.axvline(threshold, color = 'red', linestyle = ':', lw = 2.5)\n",
    "plt.fill_betweenx(y = [0, 11], x1 = threshold, x2 = 1.1, color = 'red', alpha = 0.05)\n",
    "plt.text(threshold + 0.01, 4, f'Critical Threshold: {threshold:.3f}', \n",
    "         color = 'red', fontweight = 'bold', rotation = 90, va = 'center')\n",
    "\n",
    "# 5. ê·¸ë˜í”„ ì„¸ë¶€ ì„¤ì •\n",
    "plt.title(\"Metabolic Risk Distribution Analysis | Raw vs. Adjusted\", fontsize = 16, fontweight = 'bold', pad = 20)\n",
    "plt.xlabel(\"Risk Score (0.0 ~ 1.0)\", fontsize = 12)\n",
    "plt.ylabel(\"Density (Population Frequency)\", fontsize = 12)\n",
    "plt.xlim(-0.1, 1.1)\n",
    "plt.ylim(0, 11)\n",
    "plt.legend(loc = 'upper right', frameon = True, shadow = True)\n",
    "plt.grid(axis = 'y', alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba1056",
   "metadata": {},
   "source": [
    "# 18. Model Integrity & Reliability Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d443c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# 1. í°íŠ¸ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ ì„¤ì • ìˆœì„œ ìµœì í™”\n",
    "sns.set_style( \"whitegrid\" )\n",
    "\n",
    "# Windows ì‹œìŠ¤í…œ ê¸°ë³¸ í°íŠ¸ ê°•ì œ ì¬í• ë‹¹\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "# 2. ì„ìƒ ì§€í‘œ í•œê¸€ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬\n",
    "clinical_ko_map : dict = {\n",
    "    \"sm_presnt\" : \"í˜„ì¬ í¡ì—° ì—¬ë¶€\", \"sex\" : \"ì„±ë³„\", \"HE_hepaB\" : \"Bí˜•ê°„ì—¼ ì–‘ì„±\",\n",
    "    \"N_CHO\" : \"íƒ„ìˆ˜í™”ë¬¼ ì„­ì·¨\", \"HE_TG\" : \"ì¤‘ì„±ì§€ë°©\", \"N_PROT\" : \"ë‹¨ë°±ì§ˆ ì„­ì·¨\",\n",
    "    \"HE_wc\" : \"í—ˆë¦¬ë‘˜ë ˆ\", \"HE_HDL_st2\" : \"HDL ì½œë ˆìŠ¤í…Œë¡¤\", \"HE_chol\" : \"ì´ ì½œë ˆìŠ¤í…Œë¡¤\",\n",
    "    \"HE_ast\" : \"AST (ê°„ìˆ˜ì¹˜)\", \"HE_HbA1c\" : \"ë‹¹í™”í˜ˆìƒ‰ì†Œ\", \"HE_glu\" : \"ê³µë³µí˜ˆë‹¹\",\n",
    "    \"HE_BMI\" : \"ì²´ì§ˆëŸ‰ì§€ìˆ˜ (BMI)\"\n",
    "}\n",
    "\n",
    "# --- [A] ìµœì •ì˜ˆ ëª¨ë¸ ì¤‘ìš”ë„ (Top 10 Selection) ---\n",
    "def plot_importance_solid(df : pd.DataFrame) :\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    \n",
    "    df[ \"Feature_Ko\" ] = df[ \"Feature\" ].map(lambda x : clinical_ko_map.get(x, x))\n",
    "    \n",
    "    # ì„ ëª…í•œ ë‹¨ìƒ‰ íŒ”ë ˆíŠ¸ ì ìš© (bright)\n",
    "    ax = sns.barplot(\n",
    "        x = \"Importance\", \n",
    "        y = \"Feature_Ko\", \n",
    "        data = df, \n",
    "        palette = \"bright\", \n",
    "        hue = \"Feature_Ko\", \n",
    "        legend = False\n",
    "    )\n",
    "    \n",
    "    for p in ax.patches :\n",
    "        w : float = p.get_width()\n",
    "        if w > 0 :\n",
    "            ax.annotate(f\"{w:.4f}\", (w + 0.005, p.get_y() + p.get_height() / 2),\n",
    "                        ha = \"left\", va = \"center\", fontsize = 10, fontweight = \"bold\")\n",
    "    \n",
    "    plt.title( \"ìµœì •ì˜ˆ ëª¨ë¸ ì¤‘ìš”ë„ ë¶„ì„ (Top 10 Selection)\", fontsize = 18, fontweight = \"bold\", pad = 25 )\n",
    "    plt.xlabel( \"ê¸°ì—¬ë„ (Importance Score)\", fontsize = 12 )\n",
    "    plt.ylabel( \"í•µì‹¬ ì„ìƒ ì§€í‘œ\", fontsize = 12 )\n",
    "    plt.xlim(0, df[ \"Importance\" ].max() * 1.15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- [B] Data Leakage ìƒê´€ê³„ìˆ˜ ì ê²€ ---\n",
    "def plot_leakage_solid(data : dict) :\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    leakage_df : pd.DataFrame = pd.DataFrame(list(data.items()), columns = [ \"Feature\", \"Correlation\" ])\n",
    "    leakage_df[ \"Feature_Ko\" ] = leakage_df[ \"Feature\" ].map(lambda x : clinical_ko_map.get(x, x))\n",
    "    \n",
    "    # êµ¬ë¶„ì´ ëª…í™•í•œ ë‹¨ìƒ‰ íŒ”ë ˆíŠ¸ ì ìš© (Set1)\n",
    "    ax = sns.barplot(\n",
    "        x = \"Correlation\", \n",
    "        y = \"Feature_Ko\", \n",
    "        data = leakage_df, \n",
    "        palette = \"Set1\", \n",
    "        hue = \"Feature_Ko\", \n",
    "        legend = False\n",
    "    )\n",
    "    \n",
    "    for p in ax.patches :\n",
    "        w : float = p.get_width()\n",
    "        ax.annotate(f\"r = {w:.4f}\", (w + 0.002, p.get_y() + p.get_height() / 2),\n",
    "                    ha = \"left\", va = \"center\", fontsize = 10, fontweight = \"bold\")\n",
    "\n",
    "    plt.axvline(x = 0.9, color = \"#e74c3c\", linestyle = \"--\", lw = 2)\n",
    "    plt.title( \"ë°ì´í„° ëˆ„ìˆ˜ ì ê²€ : ì§„ë‹¨ ì§€í‘œ ìƒê´€ê³„ìˆ˜\", fontsize = 16, fontweight = \"bold\", pad = 25 )\n",
    "    plt.xlabel( \"í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (r)\", fontsize = 12 )\n",
    "    plt.ylabel( \"ì§„ë‹¨ ê¸°ì¤€ ì§€í‘œ\", fontsize = 12 )\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° ì‹¤í–‰\n",
    "top_10_results : pd.DataFrame = pd.DataFrame({\n",
    "    \"Feature\" : [ \"sm_presnt\", \"sex\", \"HE_hepaB\", \"N_CHO\", \"HE_TG\", \"N_PROT\", \"HE_wc\", \"HE_HDL_st2\", \"HE_chol\", \"HE_ast\" ],\n",
    "    \"Importance\" : [ 0.3385, 0.0842, 0.0722, 0.0685, 0.0622, 0.0521, 0.0383, 0.0345, 0.0301, 0.0281 ]\n",
    "})\n",
    "leakage_results : dict = { \"HE_HbA1c\" : 0.0401, \"HE_glu\" : 0.0506, \"HE_BMI\" : 0.0669, \"HE_wc\" : 0.1346 }\n",
    "\n",
    "plot_importance_solid(top_10_results)\n",
    "plot_leakage_solid(leakage_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f42c5",
   "metadata": {},
   "source": [
    "# 19. SHAP Value (ì˜ì‚¬ê²°ì • ë°©í–¥ì„±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í•˜ë“œì›¨ì–´ ë° í°íŠ¸ í™˜ê²½ ì„¤ì •\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "# TabNet ì¶œë ¥ì„ ë‹¨ì¼ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” ë˜í¼\n",
    "class TabNetWrapper(nn.Module) :\n",
    "    def __init__(self, model) :\n",
    "        super(TabNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x) :\n",
    "        return self.model(x)[ 0 ]\n",
    "\n",
    "# 1. SHAP Interaction Values ì‚°ì¶œ (GPU ê°€ì†)\n",
    "background_data : torch.Tensor = torch.as_tensor(shap.kmeans(X_train, 50).data).float().to(device)\n",
    "wrapped_model : nn.Module = TabNetWrapper(clf.network).to(device).eval()\n",
    "\n",
    "# KernelExplainerë¥¼ í†µí•œ Interaction Approximation\n",
    "explainer : shap.KernelExplainer = shap.KernelExplainer(lambda x : wrapped_model(torch.as_tensor(x).float().to(device)).cpu().detach().numpy(), background_data.cpu().numpy())\n",
    "shap_interaction_values : np.ndarray = explainer.shap_values(X_test[ :100 ], check_additivity = False)\n",
    "\n",
    "# 2. ìˆ˜ì¹˜ ë¼ë²¨ë§ Interaction Plot í•¨ìˆ˜\n",
    "def plot_labeled_interaction(interaction_vals : np.ndarray, data : np.ndarray, feature_idx : list) :\n",
    "    fig = plt.figure(figsize = (10, 12))\n",
    "    \n",
    "    # Interaction Summary Plot ì¶œë ¥\n",
    "    shap.summary_plot(\n",
    "        interaction_vals[ :, feature_idx, feature_idx ], \n",
    "        data[ :100, feature_idx ],\n",
    "        feature_names = [ clinical_ko_map.get(valid_features[ i ], valid_features[ i ]) for i in feature_idx ],\n",
    "        show = False\n",
    "    )\n",
    "\n",
    "    ax = plt.gca()\n",
    "    # ì´ìƒì¹˜(Extreme Points) íƒìƒ‰ ë° ìˆ˜ì¹˜ ê°•ì œ í‘œê¸°\n",
    "    for i, idx in enumerate(feature_idx) :\n",
    "        vals = interaction_vals[ :, idx, idx ]\n",
    "        extreme_points = np.where(np.abs(vals) > 0.5)[ 0 ]\n",
    "        \n",
    "        for e_idx in extreme_points :\n",
    "            v = vals[ e_idx ]\n",
    "            ax.text(\n",
    "                v, i, f\" {v:.2f}\", \n",
    "                va = \"center\", ha = \"left\" if v > 0 else \"right\",\n",
    "                fontsize = 9, fontweight = \"bold\", color = \"crimson\"\n",
    "            )\n",
    "\n",
    "    plt.title( \"ì„ìƒ ì§€í‘œë³„ ë‹¹ë‡¨ ìœ„í—˜ ìƒí˜¸ì‘ìš© ë¶„ì„ (Labeled)\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "    plt.xlabel( \"SHAP interaction value\", fontsize = 12 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. ë¶„ì„ ì‹¤í–‰ (Bí˜•ê°„ì—¼ ë° HE_hepaC ì¸ë±ìŠ¤ ê¸°ì¤€)\n",
    "target_indices : list = [ valid_features.index( \"HE_hepaB\" ), valid_features.index( \"HE_hepaC\" ) ]\n",
    "plot_labeled_interaction(shap_interaction_values, X_test, target_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be2eea",
   "metadata": {},
   "source": [
    "# 20. Advanced Clinical Validation : Interpretability, Utility, and Robustness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9728d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# í•˜ë“œì›¨ì–´ ê°€ì† ë° í°íŠ¸ ì „ì—­ ì„¤ì •\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "# 1. SHAP Interaction (GPU ê°€ì† ë˜í•‘)\n",
    "class TabNetWrapper(nn.Module) :\n",
    "    def __init__(self, model) :\n",
    "        super(TabNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x) :\n",
    "        return self.model(x)[ 0 ]\n",
    "\n",
    "wrapped_model : nn.Module = TabNetWrapper(clf.network).to(device).eval()\n",
    "background : np.ndarray = shap.kmeans(X_train, 50).data\n",
    "explainer : shap.KernelExplainer = shap.KernelExplainer(\n",
    "    lambda x : wrapped_model(torch.as_tensor(x).float().to(device)).cpu().detach().numpy(), \n",
    "    background\n",
    ")\n",
    "shap_interaction_values : np.ndarray = explainer.shap_values(X_test[ :100 ], check_additivity = False)\n",
    "\n",
    "# 2. Labeled Interaction Plot\n",
    "def plot_labeled_interaction(interaction_vals, data, feature_idx) :\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    shap.summary_plot(\n",
    "        interaction_vals[ :, feature_idx, feature_idx ], \n",
    "        data[ :100, feature_idx ],\n",
    "        feature_names = [ clinical_ko_map.get(valid_features[ i ], valid_features[ i ]) for i in feature_idx ],\n",
    "        show = False\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    for i, idx in enumerate(feature_idx) :\n",
    "        vals = interaction_vals[ :, idx, idx ]\n",
    "        extreme = np.where(np.abs(vals) > 0.5)[ 0 ]\n",
    "        for e in extreme :\n",
    "            v = vals[ e ]\n",
    "            ax.text(v, i, f\" {v:.2f}\", va = \"center\", ha = \"left\" if v > 0 else \"right\", \n",
    "                    fontsize = 9, fontweight = \"bold\", color = \"crimson\")\n",
    "    plt.title( \"ì„ìƒ ì§€í‘œë³„ ë‹¹ë‡¨ ìœ„í—˜ ìƒí˜¸ì‘ìš© ë¶„ì„ (Labeled)\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "    plt.show()\n",
    "\n",
    "# 3. Decision Curve Analysis (DCA) with Labeling\n",
    "def plot_dca_pytorch(df) :\n",
    "    target_candidates : list = [ \"HE_DM\", \"is_high_risk\", \"target\", \"label\" ]\n",
    "    target_col : str = next((c for c in target_candidates if c in df.columns), df.columns[ -1 ])\n",
    "    \n",
    "    y_raw = pd.to_numeric(df[ target_col ], errors = \"coerce\" ).fillna(0)\n",
    "    y_true : torch.Tensor = torch.as_tensor((y_raw >= (3 if \"HE_\" in target_col else 1)).astype(int).values).float().to(device)\n",
    "    y_prob : torch.Tensor = torch.as_tensor(df[ \"High_Risk_Probability\" ].values).float().to(device)\n",
    "    \n",
    "    thresholds : torch.Tensor = torch.linspace(0, 0.99, 100).to(device)\n",
    "    nb_model, nb_all = [], []\n",
    "    n : int = len(y_true)\n",
    "    prev : float = torch.mean(y_true).item()\n",
    "\n",
    "    for p_t in thresholds :\n",
    "        tp : float = torch.sum((y_prob >= p_t) & (y_true == 1)).item()\n",
    "        fp : float = torch.sum((y_prob >= p_t) & (y_true == 0)).item()\n",
    "        nb : float = (tp / n) - (fp / n) * (p_t.item() / (1 - p_t.item()))\n",
    "        nb_model.append( nb )\n",
    "        nb_all.append( prev - (1 - prev) * (p_t.item() / (1 - p_t.item())) )\n",
    "\n",
    "    plt.figure(figsize = (11, 8))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # ê³¡ì„  ë Œë”ë§\n",
    "    ax.plot(thresholds.cpu(), nb_model, color = \"#0047AB\", lw = 3, label = \"TabNet ëª¨ë¸\")\n",
    "    ax.plot(thresholds.cpu(), nb_all, color = \"#A9A9A9\", ls = \"--\", label = \"ì „ì› ì¹˜ë£Œ ì „ëµ\")\n",
    "    ax.axhline(y = 0, color = \"black\", lw = 1.2, label = \"ì¹˜ë£Œ ì—†ìŒ ì „ëµ\")\n",
    "\n",
    "    # ìµœëŒ€ ìˆœì´ìµ(Max Net Benefit) ì§€ì  ìë™ íƒìƒ‰ ë° ë¼ë²¨ë§\n",
    "    max_nb = max(nb_model)\n",
    "    max_idx = nb_model.index(max_nb)\n",
    "    max_t = thresholds[ max_idx ].item()\n",
    "    \n",
    "    # ìµœëŒ€ ì´ë“ ì§€ì ì— í¬ì¸íŠ¸ í‘œì‹œ ë° ì£¼ì„ ì¶”ê°€\n",
    "    ax.plot(max_t, max_nb, 'o', color = \"crimson\", markersize = 8)\n",
    "    ax.annotate(f\"ìµœëŒ€ ìˆœì´ìµ : {max_nb:.4f}\\n(ì„ê³„ì¹˜ : {max_t:.2f})\",\n",
    "                xy = (max_t, max_nb), xytext = (max_t + 0.15, max_nb + 0.02),\n",
    "                arrowprops = dict(arrowstyle = \"->\", connectionstyle = \"arc3\", color = \"crimson\"),\n",
    "                fontsize = 10, fontweight = 'bold', color = \"crimson\")\n",
    "\n",
    "    # ì£¼ìš” ì„ê³„ì¹˜(0.1, 0.2, 0.5) ìˆ˜ì¹˜ ë¼ë²¨ ì¶”ê°€\n",
    "    for t_val in [ 0.1, 0.2, 0.5 ] :\n",
    "        idx = (np.abs(thresholds.cpu().numpy() - t_val)).argmin()\n",
    "        val = nb_model[ idx ]\n",
    "        if val > 0 :\n",
    "            ax.text(t_val, val + 0.005, f\"{val:.4f}\", ha = 'center', va = 'bottom', \n",
    "                    fontsize = 9, fontweight = 'bold', color = \"#0047AB\")\n",
    "\n",
    "    plt.title( \"ì„ìƒì  ì˜ì‚¬ê²°ì • ê³¡ì„  ë¶„ì„ (DCA)\", fontsize = 18, fontweight = \"bold\", pad = 25 )\n",
    "    plt.xlabel( \"ìœ„í—˜ ì„ê³„ì¹˜ (Threshold)\", fontsize = 12 )\n",
    "    plt.ylabel( \"ìˆœì´ìµ (Net Benefit)\", fontsize = 12 )\n",
    "    plt.ylim(-0.05, max(nb_model) * 1.5)\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.grid(alpha = 0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4. VIF Analysis (Independence)\n",
    "def run_vif_check() :\n",
    "    if isinstance(top_10_features, pd.DataFrame) :\n",
    "        vif_features : list = top_10_features[ \"Feature\" ].tolist()\n",
    "    elif isinstance(top_10_features, list) :\n",
    "        if len(top_10_features) > 0 and isinstance(top_10_features[ 0 ], dict) :\n",
    "            vif_features : list = [ f[ \"Feature\" ] for f in top_10_features ]\n",
    "        else :\n",
    "            vif_features : list = top_10_features\n",
    "    else :\n",
    "        vif_features : list = list(top_10_features)\n",
    "\n",
    "    X_vif_df : pd.DataFrame = pd.DataFrame(X_test, columns = valid_features)[ vif_features ]\n",
    "    X_vif_df = X_vif_df.apply(pd.to_numeric, errors = \"coerce\" ).fillna(0)\n",
    "    \n",
    "    vif_report : pd.DataFrame = pd.DataFrame()\n",
    "    vif_report[ \"ì„ìƒ ì§€í‘œ\" ] = [ clinical_ko_map.get(f, f) for f in X_vif_df.columns ]\n",
    "    vif_report[ \"VIF\" ] = [ variance_inflation_factor(X_vif_df.values, i) for i in range(X_vif_df.shape[ 1 ]) ]\n",
    "    \n",
    "    print( \"\\nâš–ï¸ ì§€í‘œ ê°„ ë…ë¦½ì„± ê²€ì¦ (VIF)\" )\n",
    "    print( \"-\" * 50 )\n",
    "    print( vif_report.sort_values(by = \"VIF\", ascending = False).to_string(index = False) )\n",
    "\n",
    "# ë¶„ì„ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸\n",
    "target_idx : list = [ valid_features.index( \"HE_hepaB\" ), valid_features.index( \"HE_hepaC\" ) ]\n",
    "plot_labeled_interaction(shap_interaction_values, X_test, target_idx)\n",
    "plot_dca_pytorch(full_diagnosis_results)\n",
    "run_vif_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c503309",
   "metadata": {},
   "source": [
    "# 21. Advanced Clinical Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ì´ì „ íŒŒì´í”„ë¼ì¸ì˜ ë³€ìˆ˜ë“¤ì„ í˜„ì¬ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ë³€ìˆ˜ëª…ìœ¼ë¡œ ë§µí•‘\n",
    "y_true : np.ndarray = y_test\n",
    "y_prob_tabnet : np.ndarray = preds_proba\n",
    "\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "# 1. DeLong's Test\n",
    "def compute_midrank(x) :\n",
    "    J = np.argsort(x)\n",
    "    Z = x[ J ]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype = float)\n",
    "    i = 0\n",
    "    while i < N :\n",
    "        j = i\n",
    "        while j < N and Z[ j ] == Z[ i ] :\n",
    "            j += 1\n",
    "        T[ i : j ] = 0.5 * (i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype = float)\n",
    "    T2[ J ] = T + 1\n",
    "    return T2\n",
    "\n",
    "def delong_roc_test(y_true : np.ndarray, y_prob1 : np.ndarray, y_prob2 : np.ndarray) :\n",
    "    pos = y_prob1[ y_true == 1 ]\n",
    "    neg = y_prob1[ y_true == 0 ]\n",
    "    m, n = len(pos), len(neg)\n",
    "    \n",
    "    tx1, ty1 = compute_midrank(pos), compute_midrank(neg)\n",
    "    tz1 = compute_midrank(np.concatenate((pos, neg)))\n",
    "    vx1 = (tz1[ :m ] - tx1) / n\n",
    "    vy1 = 1.0 - (tz1[ m: ] - ty1) / m\n",
    "    \n",
    "    pos2 = y_prob2[ y_true == 1 ]\n",
    "    neg2 = y_prob2[ y_true == 0 ]\n",
    "    tx2, ty2 = compute_midrank(pos2), compute_midrank(neg2)\n",
    "    tz2 = compute_midrank(np.concatenate((pos2, neg2)))\n",
    "    vx2 = (tz2[ :m ] - tx2) / n\n",
    "    vy2 = 1.0 - (tz2[ m: ] - ty2) / m\n",
    "\n",
    "    S11 = np.cov(vx1, vx2, rowvar = False)[ 0, 1 ] * m + np.cov(vy1, vy2, rowvar = False)[ 0, 1 ] * n\n",
    "    S11 /= (m + n)\n",
    "    \n",
    "    auc1, auc2 = roc_auc_score(y_true, y_prob1), roc_auc_score(y_true, y_prob2)\n",
    "    z = (auc1 - auc2) / np.sqrt(max(S11, 1e-8))\n",
    "    p_value = 2 * (1 - st.norm.cdf(abs(z)))\n",
    "    return auc1, auc2, p_value\n",
    "\n",
    "lr_model = LogisticRegression(max_iter = 1000, n_jobs = -1)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_prob_lr : np.ndarray = lr_model.predict_proba(X_test)[ :, 1 ]\n",
    "\n",
    "auc_tabnet, auc_lr, p_val_delong = delong_roc_test(y_true, y_prob_tabnet, y_prob_lr)\n",
    "print( f\"\\n[1] DeLong's Test : TabNet ({auc_tabnet:.4f}) vs LR ({auc_lr:.4f}) | p-value : {p_val_delong:.4e}\" )\n",
    "\n",
    "# 2. Youden's Index\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_true, y_prob_tabnet)\n",
    "youden_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold : float = thresholds_roc[ youden_idx ]\n",
    "\n",
    "y_pred_opt : np.ndarray = (y_prob_tabnet >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred_opt).ravel()\n",
    "\n",
    "sensitivity : float = tp / (tp + fn)\n",
    "specificity : float = tn / (tn + fp)\n",
    "ppv : float = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "npv : float = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "\n",
    "print( f\"\\n[2] Youden's Index Optimal Threshold : {optimal_threshold:.4f}\" )\n",
    "print( f\"    - Sensitivity : {sensitivity:.4f} | Specificity : {specificity:.4f}\" )\n",
    "print( f\"    - PPV         : {ppv:.4f} | NPV         : {npv:.4f}\" )\n",
    "\n",
    "# 3. Bootstrapping\n",
    "n_bootstraps : int = 1000\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def bootstrap_iter(i) :\n",
    "    indices = rng.randint(0, len(y_prob_tabnet), len(y_prob_tabnet))\n",
    "    if len(np.unique(y_true[ indices ])) < 2 :\n",
    "        return None\n",
    "    return roc_auc_score(y_true[ indices ], y_prob_tabnet[ indices ])\n",
    "\n",
    "bootstrapped_scores = Parallel(n_jobs = -1)(delayed(bootstrap_iter)(i) for i in range(n_bootstraps))\n",
    "bootstrapped_scores = [ s for s in bootstrapped_scores if s is not None ]\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "ci_lower : float = sorted_scores[ int(0.025 * len(sorted_scores)) ]\n",
    "ci_upper : float = sorted_scores[ int(0.975 * len(sorted_scores)) ]\n",
    "\n",
    "print( f\"\\n[3] Bootstrapping (1000 iter) 95% CI : {ci_lower:.4f} - {ci_upper:.4f}\" )\n",
    "\n",
    "# 4. Subgroup Analysis\n",
    "def get_mask(feature : str, val : float) :\n",
    "    if feature in valid_features :\n",
    "        idx = valid_features.index(feature)\n",
    "        return X_test[ :, idx ] == val\n",
    "    return np.zeros(len(y_true), dtype = bool)\n",
    "\n",
    "subgroups : dict = {\n",
    "    \"All Patients\" : (np.ones(len(y_true), dtype = bool), \"Total\"),\n",
    "    \"Male\" : (get_mask( \"sex\", 1 ), \"Gender\"),\n",
    "    \"Female\" : (get_mask( \"sex\", 2 ), \"Gender\"),\n",
    "    \"HepaB (+)\" : (get_mask( \"HE_hepaB\", 1 ), \"Liver\"),\n",
    "    \"HepaB (-)\" : (get_mask( \"HE_hepaB\", 0 ), \"Liver\"),\n",
    "    \"Smoker\" : (get_mask( \"sm_presnt\", 1 ), \"Lifestyle\"),\n",
    "    \"Non-Smoker\" : (get_mask( \"sm_presnt\", 0 ), \"Lifestyle\"),\n",
    "}\n",
    "\n",
    "results_sub = []\n",
    "for name, (mask, category) in subgroups.items() :\n",
    "    if np.sum(mask) > 10 and len(np.unique(y_true[ mask ])) > 1 :\n",
    "        score = roc_auc_score(y_true[ mask ], y_prob_tabnet[ mask ])\n",
    "        results_sub.append((name, score, np.sum(mask)))\n",
    "\n",
    "plt.figure(figsize = ( 10, 6 ))\n",
    "names = [ r[ 0 ] for r in results_sub ][ ::-1 ]\n",
    "aucs = [ r[ 1 ] for r in results_sub ][ ::-1 ]\n",
    "counts = [ r[ 2 ] for r in results_sub ][ ::-1 ]\n",
    "\n",
    "plt.scatter(aucs, range(len(names)), color = \"#0047AB\", s = 100, zorder = 3)\n",
    "plt.axvline(x = auc_tabnet, color = \"crimson\", linestyle = \"--\", label = \"Overall AUC\")\n",
    "\n",
    "for i, (auc_val, count) in enumerate(zip(aucs, counts)) :\n",
    "    plt.text(auc_val + 0.01, i, f\"{auc_val:.3f} (N={count})\", va = \"center\", fontsize = 10)\n",
    "\n",
    "plt.yticks(range(len(names)), names, fontsize = 12)\n",
    "plt.xlabel( \"AUROC Score\", fontsize = 12 )\n",
    "plt.title( \"Subgroup AUROC Analysis\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "plt.legend()\n",
    "plt.grid(axis = \"x\", linestyle = \"--\", alpha = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf12b4c",
   "metadata": {},
   "source": [
    "# 22. Comprehensive Model Integrity Analysis : Local Interpretability, Calibration, and Feature Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76446a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as mtext\n",
    "from matplotlib import font_manager, rc\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# 1. í°íŠ¸ ë° ìœ ë‹ˆì½”ë“œ ë§ˆì´ë„ˆìŠ¤ ì„¤ì •\n",
    "font_path : str = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "font_name : str = font_manager.FontProperties(fname = font_path).get_name()\n",
    "rc('font', family = font_name)\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False \n",
    "\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Yì¶• ë ˆì´ë¸” ê°•ì œ ë§µí•‘ ë° ì „ì—­ í…ìŠ¤íŠ¸ ì¹˜í™˜\n",
    "def force_fix_plot_tofu(fig, ax) :\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # 1. Yì¶• ë ˆì´ë¸” ì¢Œí‘œ ê³ ì • ë° ëª…ì‹œì  ì¹˜í™˜\n",
    "    ticks : list = ax.get_yticks()\n",
    "    labels : list = [ item.get_text() for item in ax.get_yticklabels() ]\n",
    "    new_labels : list = [ label.replace('\\u2212', '-').replace('âˆ’', '-').replace('ã…', '-') for label in labels ]\n",
    "    \n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(new_labels)\n",
    "    \n",
    "    # 2. ë‚´ë¶€ í…ìŠ¤íŠ¸(ë§‰ëŒ€ ìˆ˜ì¹˜ ë“±) ì¹˜í™˜\n",
    "    for text_obj in fig.findobj(mtext.Text) :\n",
    "        t : str = text_obj.get_text()\n",
    "        if t :\n",
    "            cleaned : str = t.replace('\\u2212', '-').replace('âˆ’', '-').replace('ã…', '-')\n",
    "            text_obj.set_text(cleaned)\n",
    "\n",
    "# 1. Local XAI Case Study (Waterfall Plot)\n",
    "y_true_arr : np.ndarray = y_test.values if isinstance(y_test, pd.Series) else np.array(y_test)\n",
    "y_prob : np.ndarray = clf.predict_proba(X_test)[ :, 1 ]\n",
    "y_pred : np.ndarray = (y_prob >= 0.3965).astype(int)\n",
    "\n",
    "tp_idx : int = np.where((y_true_arr == 1) & (y_pred == 1))[ 0 ][ 0 ]\n",
    "fn_idx : int = np.where((y_true_arr == 1) & (y_pred == 0))[ 0 ][ 0 ]\n",
    "\n",
    "explainer_local = shap.KernelExplainer(lambda x : clf.predict_proba(x)[ :, 1 ], shap.kmeans(X_train, 50))\n",
    "shap_vals_local : np.ndarray = explainer_local.shap_values(X_test[ [ tp_idx, fn_idx ] ])\n",
    "\n",
    "def plot_local_analysis(shap_values : np.ndarray, base_value : float, data : np.ndarray, title : str) :\n",
    "    explanation = shap.Explanation(\n",
    "        values = shap_values, \n",
    "        base_values = base_value, \n",
    "        data = data, \n",
    "        feature_names = [ clinical_ko_map.get(f, f) for f in valid_features ]\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize = (10, 8))\n",
    "    shap.plots.waterfall(explanation, max_display = 10, show = False)\n",
    "    \n",
    "    plt.title(title, fontsize = 16, fontweight = \"bold\", pad = 25)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    force_fix_plot_tofu(fig, ax)\n",
    "    plt.show()\n",
    "\n",
    "plot_local_analysis(shap_vals_local[ 0 ], explainer_local.expected_value, X_test[ tp_idx ], \"True Positive Case Analysis\")\n",
    "plot_local_analysis(shap_vals_local[ 1 ], explainer_local.expected_value, X_test[ fn_idx ], \"False Negative Case Analysis\")\n",
    "\n",
    "# 2. Probability Calibration with Point Annotations\n",
    "prob_true, prob_pred = calibration_curve(y_true_arr, y_prob, n_bins = 10, strategy = \"quantile\")\n",
    "brier : float = brier_score_loss(y_true_arr, y_prob)\n",
    "\n",
    "fig_cal, ax_cal = plt.subplots(figsize = (9, 9))\n",
    "ax_cal.plot(prob_pred, prob_true, marker = \"o\", lw = 3, color = \"#0047AB\", label = f\"TabNet (Brier : {brier:.4f})\")\n",
    "ax_cal.plot([ 0, 1 ], [ 0, 1 ], ls = \"--\", color = \"black\", alpha = 0.7, label = \"Ideal\")\n",
    "\n",
    "for x, y in zip(prob_pred, prob_true) :\n",
    "    ax_cal.text(x, y + 0.02, f\"P:{x:.3f}\\nO:{y:.3f}\", ha = \"center\", va = \"bottom\", \n",
    "                fontsize = 9, fontweight = \"bold\", color = \"#0047AB\",\n",
    "                bbox = dict(boxstyle = \"round,pad=0.3\", fc = \"white\", ec = \"#0047AB\", alpha = 0.8))\n",
    "\n",
    "ax_cal.set_title( \"ì˜ˆì¸¡ í™•ë¥  êµì • ê³¡ì„  (Calibration Analysis)\", fontsize = 18, fontweight = \"bold\", pad = 25 )\n",
    "ax_cal.set_xlabel( \"í‰ê·  ì˜ˆì¸¡ í™•ë¥  (Mean Predicted Probability)\", fontsize = 12 )\n",
    "ax_cal.set_ylabel( \"ì‹¤ì œ ê´€ì¸¡ ë¹„ìœ¨ (Fraction of Positives)\", fontsize = 12 )\n",
    "ax_cal.legend(loc = \"lower right\")\n",
    "ax_cal.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_cal, ax_cal)\n",
    "plt.show()\n",
    "\n",
    "# 3. Feature Ablation Study\n",
    "baseline_auc : float = roc_auc_score(y_true_arr, y_prob)\n",
    "ablation_results : dict = {}\n",
    "target_features : list = top_10_features[ \"Feature\" ].tolist() if isinstance(top_10_features, pd.DataFrame) else top_10_features\n",
    "\n",
    "for feature in target_features :\n",
    "    if feature in valid_features :\n",
    "        idx : int = valid_features.index(feature)\n",
    "        X_ablated : np.ndarray = X_test.copy()\n",
    "        X_ablated[ :, idx ] = 0 \n",
    "        y_prob_ablated : np.ndarray = clf.predict_proba(X_ablated)[ :, 1 ]\n",
    "        ablation_results[ clinical_ko_map.get(feature, feature) ] = baseline_auc - roc_auc_score(y_true_arr, y_prob_ablated)\n",
    "\n",
    "ab_df : pd.DataFrame = pd.DataFrame(list(ablation_results.items()), columns = [ \"ì§€í‘œ\", \"Drop\" ]).sort_values(by = \"Drop\", ascending = True)\n",
    "colors : list = plt.cm.get_cmap( \"tab10\" )(np.linspace(0, 1, len(ab_df)))\n",
    "\n",
    "fig_ab, ax_ab = plt.subplots(figsize = (11, 7))\n",
    "bars = ax_ab.barh(ab_df[ \"ì§€í‘œ\" ], ab_df[ \"Drop\" ], color = colors, alpha = 0.85)\n",
    "\n",
    "for bar in bars :\n",
    "    width = bar.get_width()\n",
    "    ax_ab.text(width + 0.001, bar.get_y() + bar.get_height() / 2, f\"{width:.4f}\", \n",
    "               va = \"center\", ha = \"left\", fontsize = 10, fontweight = \"bold\")\n",
    "\n",
    "ax_ab.set_title( \"Feature Ablation : ì§€í‘œ ëˆ„ë½ ì‹œ AUROC í•˜ë½í­\", fontsize = 18, fontweight = \"bold\", pad = 25 )\n",
    "ax_ab.set_xlabel( \"AUROC Drop\", fontsize = 12 )\n",
    "ax_ab.set_xlim( 0, max(ab_df[ \"Drop\" ]) * 1.2 )\n",
    "ax_ab.grid(axis = \"x\", ls = \"--\", alpha = 0.4)\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_ab, ax_ab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ca5d0",
   "metadata": {},
   "source": [
    "# 23. False Positive (ìœ„ì–‘ì„±) ì¼€ì´ìŠ¤ ì‹¬ì¸µ ë¶„ì„ : ì¡°ê¸° ê²½ë³´ ê´€ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# ë³€ìˆ˜ ë§¤í•‘ ë° FP ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "X_test_arr : np.ndarray = X_test if isinstance(X_test, np.ndarray) else X_test.values\n",
    "y_true_arr : np.ndarray = y_true if isinstance(y_true, np.ndarray) else y_true.values\n",
    "y_pred : np.ndarray = preds_class\n",
    "\n",
    "fp_indices : np.ndarray = np.where((y_true_arr == 0) & (y_pred == 1))[ 0 ]\n",
    "\n",
    "if len(fp_indices) > 0 :\n",
    "    fp_idx : int = fp_indices[ 0 ]\n",
    "    \n",
    "    # SHAP ë¶„ì„ ìˆ˜í–‰\n",
    "    shap_vals_fp : np.ndarray = explainer_local.shap_values(X_test_arr[ [ fp_idx ] ])\n",
    "    \n",
    "    fig_fp, ax_fp = plt.subplots(figsize = ( 10, 8 ))\n",
    "    explanation_fp = shap.Explanation(\n",
    "        values = shap_vals_fp[ 0 ], \n",
    "        base_values = explainer_local.expected_value, \n",
    "        data = X_test_arr[ fp_idx ], \n",
    "        feature_names = [ clinical_ko_map.get(f, f) for f in valid_features ]\n",
    "    )\n",
    "    \n",
    "    # Waterfall ì‹œê°í™”\n",
    "    shap.plots.waterfall(explanation_fp, max_display = 10, show = False)\n",
    "    ax_fp.set_title(\"False Positive Case Analysis (Early Warning)\", fontsize = 16, fontweight = \"bold\", pad = 25)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else :\n",
    "    print(\"No False Positive cases found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df8f39",
   "metadata": {},
   "source": [
    "# 24. Advanced Algorithmic Integrity : AUPRC & Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d57b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as mtext\n",
    "from matplotlib import font_manager, rc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# í°íŠ¸ ë° ìœ ë‹ˆì½”ë“œ ë§ˆì´ë„ˆìŠ¤ ì„¤ì •\n",
    "font_path : str = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "font_name : str = font_manager.FontProperties(fname = font_path).get_name()\n",
    "rc('font', family = font_name)\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False \n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¹¨ì§ ì¹˜í™˜ í•¨ìˆ˜\n",
    "def force_fix_plot_tofu(fig, ax) :\n",
    "    fig.canvas.draw()\n",
    "    ticks : list = ax.get_yticks()\n",
    "    labels : list = [ item.get_text() for item in ax.get_yticklabels() ]\n",
    "    new_labels : list = [ label.replace('\\u2212', '-').replace('âˆ’', '-').replace('ã…', '-') for label in labels ]\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(new_labels)\n",
    "    for text_obj in fig.findobj(mtext.Text) :\n",
    "        t : str = text_obj.get_text()\n",
    "        if t :\n",
    "            cleaned : str = t.replace('\\u2212', '-').replace('âˆ’', '-').replace('ã…', '-')\n",
    "            text_obj.set_text(cleaned)\n",
    "\n",
    "\n",
    "# 1. AUPRC (Area Under the Precision-Recall Curve)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true_arr, y_prob)\n",
    "auprc_score : float = average_precision_score(y_true_arr, y_prob)\n",
    "baseline_prevalence : float = np.mean(y_true_arr)\n",
    "\n",
    "fig_prc, ax_prc = plt.subplots(figsize = (9, 7))\n",
    "ax_prc.plot(recall, precision, color = \"#0047AB\", lw = 3, label = \"TabNet\")\n",
    "ax_prc.axhline(y = baseline_prevalence, color = \"black\", ls = \"--\", alpha = 0.5, label = \"Baseline\")\n",
    "\n",
    "# [ìˆ˜ì¹˜ í‘œê¸°] AUPRC ë° Baseline ì ìˆ˜ í…ìŠ¤íŠ¸ ì¶”ê°€\n",
    "ax_prc.text(0.6, 0.8, f\"AUPRC : {auprc_score:.4f}\", fontsize = 12, fontweight = \"bold\", color = \"#0047AB\", bbox = dict(facecolor = 'white', alpha = 0.8, edgecolor = '#0047AB'))\n",
    "ax_prc.text(0.6, baseline_prevalence + 0.05, f\"Baseline : {baseline_prevalence:.4f}\", fontsize = 10, color = \"black\", bbox = dict(facecolor = 'white', alpha = 0.6, edgecolor = 'black'))\n",
    "\n",
    "ax_prc.set_title( \"ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„  (AUPRC Analysis)\", fontsize = 18, fontweight = \"bold\", pad = 25 )\n",
    "ax_prc.set_xlabel( \"ì¬í˜„ìœ¨ (Recall / Sensitivity)\", fontsize = 12 )\n",
    "ax_prc.set_ylabel( \"ì •ë°€ë„ (Precision / PPV)\", fontsize = 12 )\n",
    "ax_prc.legend(loc = \"lower left\", fontsize = 12)\n",
    "ax_prc.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_prc, ax_prc)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2. TabNet Learning Curve\n",
    "\n",
    "train_loss : list = clf.history[ \"loss\" ]\n",
    "\n",
    "# ì•ˆì „í•œ Validation Key íƒìƒ‰\n",
    "possible_val_keys : list = [ \"val_0_auc\", \"valid_auc\", \"val_auc\", \"val_0_logloss\", \"valid_logloss\", \"val_0_rmse\" ]\n",
    "val_keys : list = []\n",
    "for key in possible_val_keys :\n",
    "    try :\n",
    "        if len(clf.history[ key ]) > 0 :\n",
    "            val_keys.append(key)\n",
    "    except (KeyError, TypeError, AttributeError) :\n",
    "        pass\n",
    "\n",
    "fig_lc, ax_lc1 = plt.subplots(figsize = (11, 6))\n",
    "epochs : list = list(range(1, len(train_loss) + 1))\n",
    "\n",
    "# Train Loss ë Œë”ë§ ë° ìµœì¢… ìˆ˜ì¹˜ í‘œê¸°\n",
    "ax_lc1.plot(epochs, train_loss, color = \"crimson\", lw = 2, label = \"Training Loss\")\n",
    "ax_lc1.text(epochs[-1], train_loss[-1], f\"{train_loss[-1]:.4f}\", color = \"crimson\", fontweight = \"bold\", ha = 'left', va = 'center')\n",
    "ax_lc1.set_xlabel( \"í•™ìŠµ ì—í¬í¬ (Epochs)\", fontsize = 12 )\n",
    "ax_lc1.set_ylabel( \"Training Loss\", fontsize = 12, color = \"crimson\" )\n",
    "ax_lc1.tick_params(axis = \"y\", labelcolor = \"crimson\")\n",
    "\n",
    "# Validation Metric ë Œë”ë§ ë° ìµœì¢… ìˆ˜ì¹˜ í‘œê¸°\n",
    "if val_keys :\n",
    "    val_key : str = val_keys[ 0 ]\n",
    "    val_metric : list = clf.history[ val_key ]\n",
    "    \n",
    "    ax_lc2 = ax_lc1.twinx()\n",
    "    ax_lc2.plot(epochs, val_metric, color = \"#0047AB\", lw = 2, ls = \"--\", label = f\"Validation ({val_key})\")\n",
    "    ax_lc2.text(epochs[-1], val_metric[-1], f\"{val_metric[-1]:.4f}\", color = \"#0047AB\", fontweight = \"bold\", ha = 'left', va = 'center')\n",
    "    ax_lc2.set_ylabel( f\"Validation Metric ({val_key})\", fontsize = 12, color = \"#0047AB\" )\n",
    "    ax_lc2.tick_params(axis = \"y\", labelcolor = \"#0047AB\")\n",
    "    \n",
    "    lines_1, labels_1 = ax_lc1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax_lc2.get_legend_handles_labels()\n",
    "    ax_lc1.legend(lines_1 + lines_2, labels_1 + labels_2, loc = \"center right\", fontsize = 11)\n",
    "else :\n",
    "    ax_lc1.legend(loc = \"upper right\", fontsize = 11)\n",
    "\n",
    "ax_lc1.set_title( \"TabNet í•™ìŠµ ê³¡ì„ \", fontsize = 18, fontweight = \"bold\", pad = 25 )\n",
    "ax_lc1.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_lc, ax_lc1)\n",
    "if val_keys :\n",
    "    force_fix_plot_tofu(fig_lc, ax_lc2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be333a89",
   "metadata": {},
   "source": [
    "# 25. Analytical Integrity : Global XAI, Subgroups, and Probability Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as mtext\n",
    "from matplotlib import font_manager, rc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# í°íŠ¸ ë° ìœ ë‹ˆì½”ë“œ ë§ˆì´ë„ˆìŠ¤ ì„¤ì •\n",
    "font_path : str = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "font_name : str = font_manager.FontProperties(fname = font_path).get_name()\n",
    "rc('font', family = font_name)\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False \n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¹¨ì§ ê°•ì œ ì¹˜í™˜ í•¨ìˆ˜\n",
    "def force_fix_plot_tofu(fig, ax) :\n",
    "    fig.canvas.draw()\n",
    "    ticks : list = ax.get_yticks()\n",
    "    labels : list = [ item.get_text() for item in ax.get_yticklabels() ]\n",
    "    new_labels : list = [ label.replace('\\u2212', '-').replace('âˆ’', '-').replace('ã…', '-') for label in labels ]\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(new_labels)\n",
    "    for text_obj in fig.findobj(mtext.Text) :\n",
    "        t : str = text_obj.get_text()\n",
    "        if t :\n",
    "            cleaned : str = t.replace('\\u2212', '-').replace('âˆ’', '-').replace('ã…', '-')\n",
    "            text_obj.set_text(cleaned)\n",
    "\n",
    "# 1. Global XAI : Mean Absolute SHAP\n",
    "np.random.seed(42)\n",
    "sample_size : int = min(300, len(X_test_arr))\n",
    "sample_idx : np.ndarray = np.random.choice(len(X_test_arr), sample_size, replace = False)\n",
    "X_test_sampled : np.ndarray = X_test_arr[ sample_idx ]\n",
    "\n",
    "shap_vals_global : np.ndarray = explainer_local.shap_values(X_test_sampled)\n",
    "feature_names_ko : list = [ clinical_ko_map.get(f, f) for f in valid_features ]\n",
    "\n",
    "mean_abs_shap : np.ndarray = np.abs(shap_vals_global).mean(axis = 0)\n",
    "top_10_idx : np.ndarray = np.argsort(mean_abs_shap)[ -10: ]\n",
    "num_bars : int = len(top_10_idx)\n",
    "\n",
    "colors_global : list = plt.cm.get_cmap(\"tab10\")(np.linspace(0, 1, num_bars))\n",
    "\n",
    "fig_global, ax_global = plt.subplots(figsize = (10, 8))\n",
    "bars_global = ax_global.barh(\n",
    "    np.array(feature_names_ko)[ top_10_idx ], \n",
    "    mean_abs_shap[ top_10_idx ], \n",
    "    color = colors_global, alpha = 0.85\n",
    ")\n",
    "\n",
    "for bar in bars_global :\n",
    "    width = bar.get_width()\n",
    "    ax_global.text(\n",
    "        width + 0.002, bar.get_y() + bar.get_height() / 2, \n",
    "        f\"{width:.4f}\", \n",
    "        va = \"center\", ha = \"left\", fontsize = 11, fontweight = \"bold\"\n",
    "    )\n",
    "\n",
    "ax_global.set_title( \"Global XAI : ì „ì—­ì  ë³€ìˆ˜ ì¤‘ìš”ë„\", fontsize = 16, fontweight = \"bold\", pad = 25 )\n",
    "ax_global.set_xlabel( \"í‰ê·  ì ˆëŒ€ SHAP ê¸°ì—¬ë„\", fontsize = 12 )\n",
    "ax_global.set_xlim( 0, max(mean_abs_shap) * 1.15 )\n",
    "ax_global.grid(axis = \"x\", ls = \"--\", alpha = 0.4)\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_global, ax_global)\n",
    "plt.show()\n",
    "\n",
    "# 2. Subgroup Analysis : Forest Plot\n",
    "subgroup_results : list = []\n",
    "\n",
    "total_auc : float = roc_auc_score(y_true_arr, y_prob)\n",
    "subgroup_results.append({ \"Group\" : \"Total Population\", \"AUROC\" : total_auc, \"N\" : len(y_true_arr) })\n",
    "\n",
    "if \"ì„±ë³„\" in feature_names_ko :\n",
    "    gender_idx : int = feature_names_ko.index(\"ì„±ë³„\")\n",
    "    for g_val, g_name in zip([ 1, 2 ], [ \"Male\", \"Female\" ]) :\n",
    "        mask : np.ndarray = (X_test_arr[ :, gender_idx ] == g_val)\n",
    "        if mask.sum() > 10 and len(np.unique(y_true_arr[ mask ])) > 1 :\n",
    "            subgroup_results.append({ \"Group\" : f\"Gender : {g_name}\", \"AUROC\" : roc_auc_score(y_true_arr[ mask ], y_prob[ mask ]), \"N\" : mask.sum() })\n",
    "\n",
    "if \"í—ˆë¦¬ë‘˜ë ˆ\" in feature_names_ko :\n",
    "    waist_idx : int = feature_names_ko.index(\"í—ˆë¦¬ë‘˜ë ˆ\")\n",
    "    for w_cond, w_name in zip([ X_test_arr[ :, waist_idx ] >= 90, X_test_arr[ :, waist_idx ] < 90 ], [ \"Waist >= 90\", \"Waist < 90\" ]) :\n",
    "        if w_cond.sum() > 10 and len(np.unique(y_true_arr[ w_cond ])) > 1 :\n",
    "            subgroup_results.append({ \"Group\" : w_name, \"AUROC\" : roc_auc_score(y_true_arr[ w_cond ], y_prob[ w_cond ]), \"N\" : w_cond.sum() })\n",
    "\n",
    "sg_df : pd.DataFrame = pd.DataFrame(subgroup_results)[ ::-1 ]\n",
    "\n",
    "fig_forest, ax_forest = plt.subplots(figsize = (9, 5))\n",
    "ax_forest.errorbar(sg_df[ \"AUROC\" ], range(len(sg_df)), xerr = 0.005, fmt = 'o', color = '#0047AB', ecolor = 'crimson', elinewidth = 2, capsize = 4, markersize = 8)\n",
    "ax_forest.axvline(x = total_auc, color = 'black', linestyle = '--', alpha = 0.5)\n",
    "\n",
    "ax_forest.set_yticks(range(len(sg_df)))\n",
    "ax_forest.set_yticklabels([ f\"{row['Group']} (N={row['N']})\" for _, row in sg_df.iterrows() ])\n",
    "\n",
    "for i, auc_val in enumerate(sg_df[ \"AUROC\" ]) :\n",
    "    ax_forest.text(auc_val + 0.008, i, f\"AUC : {auc_val:.4f}\", va = 'center', fontweight = 'bold', color = \"crimson\")\n",
    "\n",
    "ax_forest.set_xlabel( \"AUROC Score\", fontsize = 12 )\n",
    "ax_forest.set_title( \"Subgroup Analysis : í•˜ìœ„ ê·¸ë£¹ë³„ ì˜ˆì¸¡ ì„±ëŠ¥\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "ax_forest.set_xlim( min(sg_df[ \"AUROC\" ]) - 0.02, max(sg_df[ \"AUROC\" ]) + 0.03 )\n",
    "ax_forest.grid(axis = 'x', linestyle = '--', alpha = 0.4)\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_forest, ax_forest)\n",
    "plt.show()\n",
    "\n",
    "# 3. Probability Density : KDE Plot\n",
    "fig_kde, ax_kde = plt.subplots(figsize = (10, 6))\n",
    "\n",
    "prob_target_0 : np.ndarray = y_prob[ y_true_arr == 0 ]\n",
    "prob_target_1 : np.ndarray = y_prob[ y_true_arr == 1 ]\n",
    "\n",
    "sns.kdeplot(prob_target_0, fill = True, color = \"#0047AB\", label = \"Target 0 (Normal)\", alpha = 0.4, ax = ax_kde)\n",
    "sns.kdeplot(prob_target_1, fill = True, color = \"crimson\", label = \"Target 1 (Diabetic Risk)\", alpha = 0.4, ax = ax_kde)\n",
    "\n",
    "optimal_threshold : float = 0.3965\n",
    "ax_kde.axvline(x = optimal_threshold, color = \"black\", linestyle = \"--\", linewidth = 2)\n",
    "\n",
    "mean_prob_0 : float = np.mean(prob_target_0)\n",
    "mean_prob_1 : float = np.mean(prob_target_1)\n",
    "y_max : float = ax_kde.get_ylim()[ 1 ]\n",
    "\n",
    "ax_kde.text(mean_prob_0, y_max * 0.5, f\"Mean:\\n{mean_prob_0:.3f}\", ha = \"center\", color = \"#0047AB\", fontweight = \"bold\", bbox = dict(boxstyle = \"round\", fc = \"white\", ec = \"#0047AB\", alpha = 0.8))\n",
    "ax_kde.text(mean_prob_1, y_max * 0.5, f\"Mean:\\n{mean_prob_1:.3f}\", ha = \"center\", color = \"crimson\", fontweight = \"bold\", bbox = dict(boxstyle = \"round\", fc = \"white\", ec = \"crimson\", alpha = 0.8))\n",
    "ax_kde.text(optimal_threshold + 0.02, y_max * 0.8, f\"Optimal Threshold\\n(T = {optimal_threshold:.4f})\", va = \"center\", fontweight = \"bold\", color = \"black\")\n",
    "\n",
    "ax_kde.set_title( \"í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ í™•ë¥  ë°€ë„ ë¶„í¬\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "ax_kde.set_xlabel( \"Predicted Probability of Diabetes\", fontsize = 12 )\n",
    "ax_kde.set_ylabel( \"Density\", fontsize = 12 )\n",
    "ax_kde.set_xlim( -0.1, 1.1 )\n",
    "ax_kde.legend(loc = \"upper center\", fontsize = 11)\n",
    "ax_kde.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_kde, ax_kde)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a33292",
   "metadata": {},
   "source": [
    "# 26. Interaction Analysis & Precise Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01426ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# 1. SHAP Interaction Analysis\n",
    "top_idx_1 : int = top_10_idx[ -1 ]\n",
    "top_idx_2 : int = top_10_idx[ -2 ]\n",
    "\n",
    "shap.dependence_plot(\n",
    "    top_idx_1, \n",
    "    shap_vals_global, \n",
    "    X_test_sampled, \n",
    "    display_features = X_test_sampled,\n",
    "    feature_names = feature_names_ko,\n",
    "    interaction_index = top_idx_2,\n",
    "    show = False\n",
    ")\n",
    "\n",
    "fig_inter = plt.gcf()\n",
    "ax_inter = plt.gca()\n",
    "\n",
    "ax_inter.set_title( f\"Interaction Analysis : {feature_names_ko[top_idx_1]} x {feature_names_ko[top_idx_2]}\", fontsize = 15, fontweight = \"bold\", pad = 20 )\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_inter, ax_inter)\n",
    "plt.show()\n",
    "\n",
    "# 2. Comprehensive Clinical Metrics Table\n",
    "def get_clinical_metrics(y_true, y_prob, threshold : float = 0.3965, n_boot : int = 1000) :\n",
    "    boot_results : list = []\n",
    "    \n",
    "    for _ in range(n_boot) :\n",
    "        idx : np.ndarray = np.random.randint(0, len(y_true), len(y_true))\n",
    "        b_true, b_prob = y_true[ idx ], y_prob[ idx ]\n",
    "        \n",
    "        if len(np.unique(b_true)) < 2 : continue\n",
    "        \n",
    "        b_pred : np.ndarray = (b_prob >= threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(b_true, b_pred).ravel()\n",
    "        \n",
    "        sens : float = tp / (tp + fn)\n",
    "        spec : float = tn / (tn + fp)\n",
    "        ppv : float = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        npv : float = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        f1 : float = f1_score(b_true, b_pred)\n",
    "        \n",
    "        boot_results.append([ sens, spec, ppv, npv, f1 ])\n",
    "    \n",
    "    res_arr : np.ndarray = np.array(boot_results)\n",
    "    means : np.ndarray = np.mean(res_arr, axis = 0)\n",
    "    lowers : np.ndarray = np.percentile(res_arr, 2.5, axis = 0)\n",
    "    uppers : np.ndarray = np.percentile(res_arr, 97.5, axis = 0)\n",
    "    \n",
    "    metrics_df : pd.DataFrame = pd.DataFrame({\n",
    "        \"Metric\" : [ \"Sensitivity\", \"Specificity\", \"PPV (Precision)\", \"NPV\", \"F1-Score\" ],\n",
    "        \"Mean\" : means,\n",
    "        \"95% CI Lower\" : lowers,\n",
    "        \"95% CI Upper\" : uppers\n",
    "    })\n",
    "    return metrics_df\n",
    "\n",
    "final_metrics : pd.DataFrame = get_clinical_metrics(y_true_arr, y_prob)\n",
    "print( \"\\n[ Clinical Performance Metrics at Threshold 0.3965 ]\" )\n",
    "print( final_metrics.to_string(index = False, float_format = lambda x : \"{:.4f}\".format(x)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98787a2",
   "metadata": {},
   "source": [
    "# 27. Clinical Utility and Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_true : np.ndarray = y_true_arr\n",
    "y_prob : np.ndarray = y_prob_tabnet \n",
    "\n",
    "fpr_tmp, tpr_tmp, thresholds_tmp = roc_curve(y_true, y_prob)\n",
    "opt_idx_roc : int = np.argmax(tpr_tmp - fpr_tmp)\n",
    "opt_threshold : float = thresholds_tmp[ opt_idx_roc ]\n",
    "\n",
    "def calculate_net_benefit(y_true : np.ndarray, y_prob : np.ndarray, thresholds : np.ndarray) :\n",
    "    net_benefits : list = []\n",
    "    n : int = len(y_true)\n",
    "    for t in thresholds :\n",
    "        y_pred : np.ndarray = (y_prob >= t).astype(int)\n",
    "        tp : int = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fp : int = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        \n",
    "        if t == 1.0 :\n",
    "            net_benefit : float = 0.0\n",
    "        else :\n",
    "            net_benefit : float = (tp / n) - (fp / n) * (t / (1 - t))\n",
    "        net_benefits.append(net_benefit)\n",
    "    return net_benefits\n",
    "\n",
    "thresholds : np.ndarray = np.linspace(0.01, 0.99, 100)\n",
    "tabnet_nb : list = calculate_net_benefit(y_true, y_prob, thresholds)\n",
    "all_nb : list = calculate_net_benefit(y_true, np.ones(len(y_true)), thresholds)\n",
    "none_nb : list = [ 0.0 ] * len(thresholds)\n",
    "\n",
    "fig_dca, ax_dca = plt.subplots(figsize = ( 10, 8 ))\n",
    "ax_dca.plot(thresholds, tabnet_nb, color = \"#0047AB\", lw = 3, label = \"TabNet\")\n",
    "ax_dca.plot(thresholds, all_nb, color = \"black\", ls = \"--\", alpha = 0.5, label = \"Treat All\")\n",
    "ax_dca.plot(thresholds, none_nb, color = \"gray\", ls = \"-\", alpha = 0.3, label = \"Treat None\")\n",
    "\n",
    "opt_nb : float = tabnet_nb[ np.argmin(np.abs(thresholds - opt_threshold)) ]\n",
    "\n",
    "# ìµœì  ì§€ì  ë§ˆí‚¹ ë° ìˆ˜ì¹˜ í‘œê¸°\n",
    "ax_dca.scatter([ opt_threshold ], [ opt_nb ], color = \"crimson\", s = 100, zorder = 5)\n",
    "ax_dca.text(opt_threshold + 0.02, opt_nb, f\"{opt_nb:.4f}\", color = \"crimson\", fontsize = 13, fontweight = \"bold\", va = \"center\")\n",
    "\n",
    "ax_dca.set_ylim(-0.02, max(tabnet_nb) * 1.2)\n",
    "ax_dca.set_xlim(-0.05, 1.05)\n",
    "\n",
    "ax_dca.set_title(\"ì˜ì‚¬ê²°ì • ê³¡ì„  ë¶„ì„ (Decision Curve Analysis)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_dca.set_xlabel(\"Threshold Probability\", fontsize = 12)\n",
    "ax_dca.set_ylabel(\"Net Benefit\", fontsize = 12)\n",
    "ax_dca.legend(loc = \"upper right\")\n",
    "ax_dca.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "\n",
    "try :\n",
    "    force_fix_plot_tofu(fig_dca, ax_dca)\n",
    "except NameError :\n",
    "    pass\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c69a7",
   "metadata": {},
   "source": [
    "# 28. High-End Engineering : Error Profiling, Uncertainty, and Cost-Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295031f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. Error Profiling : Clustering with Statistics\n",
    "error_mask : np.ndarray = (y_pred != y_true_arr)\n",
    "X_error : np.ndarray = X_test_arr[ error_mask ]\n",
    "y_error_true : np.ndarray = y_true_arr[ error_mask ]\n",
    "\n",
    "pca : PCA = PCA(n_components = 2)\n",
    "X_error_pca : np.ndarray = pca.fit_transform(X_error)\n",
    "\n",
    "kmeans : KMeans = KMeans(n_clusters = 3, random_state = 42)\n",
    "error_clusters : np.ndarray = kmeans.fit_predict(X_error_pca)\n",
    "\n",
    "fig_err, ax_err = plt.subplots(figsize = ( 11, 8 ))\n",
    "scatter = ax_err.scatter(X_error_pca[ :, 0 ], X_error_pca[ :, 1 ], c = error_clusters, cmap = \"viridis\", alpha = 0.5, edgecolor = \"white\", s = 60)\n",
    "\n",
    "centroids : np.ndarray = kmeans.cluster_centers_\n",
    "for i, (cx, cy) in enumerate(centroids) :\n",
    "    n_samples : int = np.sum(error_clusters == i)\n",
    "    ax_err.annotate(\n",
    "        f\"Cluster {i}\\nN = {n_samples}\", \n",
    "        xy = (cx, cy), xytext = (cx + 0.5, cy + 0.5),\n",
    "        bbox = dict(boxstyle = \"round,pad=0.4\", fc = \"white\", ec = \"black\", alpha = 0.9),\n",
    "        arrowprops = dict(arrowstyle = \"->\", connectionstyle = \"arc3,rad=0.2\"),\n",
    "        fontsize = 10, fontweight = \"bold\"\n",
    "    )\n",
    "\n",
    "ax_err.set_title(\"Classification Error Profiling\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "plt.colorbar(scatter, label = \"Cluster ID\")\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_err, ax_err)\n",
    "plt.show()\n",
    "\n",
    "# 2. Uncertainty Quantification : Probabilistic Distribution\n",
    "def quantify_uncertainty(model, X, n_iterations : int = 50) :\n",
    "    preds : list = []\n",
    "    for _ in range(n_iterations) :\n",
    "        idx : np.ndarray = np.random.randint(0, len(X), len(X))\n",
    "        it_prob : np.ndarray = model.predict_proba(X[ idx ])[ :, 1 ]\n",
    "        preds.append(it_prob)\n",
    "    \n",
    "    preds_arr : np.ndarray = np.array(preds)\n",
    "    return np.mean(preds_arr, axis = 0), np.std(preds_arr, axis = 0), \\\n",
    "           np.percentile(preds_arr, 2.5, axis = 0), np.percentile(preds_arr, 97.5, axis = 0)\n",
    "\n",
    "mean_p, std_p, lower_p, upper_p = quantify_uncertainty(clf, X_test_arr[ :20 ])\n",
    "\n",
    "fig_unc, ax_unc = plt.subplots(figsize = ( 12, 6 ))\n",
    "sample_idx : np.ndarray = np.arange(len(mean_p))\n",
    "\n",
    "ax_unc.errorbar(\n",
    "    sample_idx, mean_p, \n",
    "    yerr = [ mean_p - lower_p, upper_p - mean_p ], \n",
    "    fmt = \"o\", color = \"#0047AB\", ecolor = \"crimson\", capsize = 6, elinewidth = 1.5, markersize = 8\n",
    ")\n",
    "\n",
    "for i, val in enumerate(mean_p) :\n",
    "    ax_unc.text(i, val + 0.04, f\"{val:.3f}\", ha = \"center\", va = \"bottom\", fontsize = 9, fontweight = \"bold\", color = \"#0047AB\")\n",
    "\n",
    "ax_unc.set_title(\"Prediction Uncertainty Quantification (95% CI)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_unc.set_xlabel(\"Sample Index\", fontsize = 12)\n",
    "ax_unc.set_ylabel(\"Probability with Uncertainty\", fontsize = 12)\n",
    "ax_unc.set_ylim(-0.05, 1.15)\n",
    "ax_unc.grid(axis = \"y\", alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_unc, ax_unc)\n",
    "plt.show()\n",
    "\n",
    "# 3. Clinical Cost-Sensitive Optimization\n",
    "def get_cost_curve(y_true, y_prob, penalty : int = 5) :\n",
    "    thresholds : np.ndarray = np.linspace(0.01, 0.99, 100)\n",
    "    costs : list = []\n",
    "    for t in thresholds :\n",
    "        y_p : np.ndarray = (y_prob >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_p).ravel()\n",
    "        costs.append((fn * penalty) + fp)\n",
    "    return thresholds, np.array(costs)\n",
    "\n",
    "fig_cost, ax_cost = plt.subplots(figsize = ( 11, 7 ))\n",
    "penalties : list = [ 1, 3, 5 ]\n",
    "colors : list = [ \"gray\", \"#0047AB\", \"crimson\" ]\n",
    "\n",
    "for p, c in zip(penalties, colors) :\n",
    "    ts, cs = get_cost_curve(y_true_arr, y_prob, penalty = p)\n",
    "    opt_t : float = ts[ np.argmin(cs) ]\n",
    "    min_c : float = np.min(cs)\n",
    "    \n",
    "    ax_cost.plot(ts, cs, color = c, lw = 2.5, label = f\"FN Penalty : {p}x\")\n",
    "    ax_cost.scatter(opt_t, min_c, color = c, s = 100, edgecolors = \"black\", zorder = 5)\n",
    "    \n",
    "    ax_cost.annotate(\n",
    "        f\"T = {opt_t:.3f}\\nCost = {min_c:.0f}\", \n",
    "        xy = (opt_t, min_c), xytext = (opt_t + 0.05, min_c + (np.max(cs) * 0.05)),\n",
    "        arrowprops = dict(arrowstyle = \"->\", color = c, lw = 1.5),\n",
    "        fontsize = 10, fontweight = \"bold\", color = c\n",
    "    )\n",
    "\n",
    "ax_cost.set_title(\"Clinical Cost-Sensitive Threshold Optimization\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_cost.set_xlabel(\"Decision Threshold\", fontsize = 12)\n",
    "ax_cost.set_ylabel(\"Weighted Clinical Cost\", fontsize = 12)\n",
    "ax_cost.legend(fontsize = 11)\n",
    "ax_unc.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_cost, ax_cost)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a532d",
   "metadata": {},
   "source": [
    "# 29. Blind-spot Signature : ì˜¤ë¶„ë¥˜ êµ°ì§‘ë³„ ì„ìƒ íŠ¹ì§• ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f547a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Blind-spot Signature Extraction\n",
    "X_test_df : pd.DataFrame = pd.DataFrame(X_test_arr, columns = feature_names_ko)\n",
    "X_error_df : pd.DataFrame = X_test_df.iloc[ error_mask ].copy()\n",
    "X_error_df[ \"Cluster\" ] = error_clusters\n",
    "\n",
    "# ì „ì²´ ì¸êµ¬ í‰ê·  ëŒ€ë¹„ êµ°ì§‘ë³„ í¸ì°¨(Z-score ê¸°ë°˜) ì‚°ì¶œ\n",
    "total_mean : pd.Series = X_test_df.mean()\n",
    "total_std : pd.Series = X_test_df.std()\n",
    "cluster_profiles : pd.DataFrame = X_error_df.groupby( \"Cluster\" ).mean().drop( columns = [] )\n",
    "relative_deviation : pd.DataFrame = (cluster_profiles - total_mean) / total_std\n",
    "\n",
    "# ìƒìœ„ ê¸°ì—¬ í”¼ì²˜ 5ê°œ ì„ ì • ë° ì‹œê°í™”\n",
    "top_diff_features : list = relative_deviation.abs().mean().sort_values( ascending = False ).head( 5 ).index.tolist()\n",
    "plot_df : pd.DataFrame = relative_deviation[ top_diff_features ].T\n",
    "\n",
    "fig_sig, ax_sig = plt.subplots(figsize = ( 11, 7 ))\n",
    "plot_df.plot(kind = \"bar\", ax = ax_sig, color = [ \"gray\", \"#0047AB\", \"crimson\" ], alpha = 0.8)\n",
    "\n",
    "# ê·¸ë˜í”„ ë‚´ ìˆ˜ì¹˜ê°’(Deviation) ì§ì ‘ í‘œê¸°\n",
    "for container in ax_sig.containers :\n",
    "    labels : list = [ f\"{v.get_height():.2f}\" for v in container ]\n",
    "    ax_sig.bar_label(container, labels = labels, label_type = \"edge\", fontsize = 9, fontweight = \"bold\", padding = 3)\n",
    "\n",
    "ax_sig.set_title( \"Blind-spot Signature : ì—ëŸ¬ êµ°ì§‘ë³„ í”¼ì²˜ í¸ì°¨ ë¶„ì„\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "ax_sig.set_ylabel( \"í‘œì¤€ í¸ì°¨ ê¸°ì¤€ ì°¨ì´ (Z-score)\", fontsize = 12 )\n",
    "ax_sig.axhline( 0, color = \"black\", lw = 1 )\n",
    "ax_sig.legend( title = \"Error Cluster\", loc = \"upper right\" )\n",
    "ax_sig.grid( axis = \"y\", alpha = 0.2 )\n",
    "plt.xticks( rotation = 45 )\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_sig, ax_sig)\n",
    "plt.show()\n",
    "\n",
    "# 2. Risk Stratification Tiering\n",
    "risk_tiers : list = [ \"Low\", \"Moderate\", \"High\", \"Urgent\" ]\n",
    "bins : list = [ 0, 0.2, 0.4, 0.8, 1.0 ]\n",
    "y_prob_tiers : pd.Series = pd.cut(y_prob, bins = bins, labels = risk_tiers)\n",
    "tier_counts : pd.Series = y_prob_tiers.value_counts().reindex(risk_tiers)\n",
    "\n",
    "fig_tier, ax_tier = plt.subplots(figsize = ( 10, 6 ))\n",
    "bars = ax_tier.bar(risk_tiers, tier_counts, color = sns.color_palette( \"coolwarm\", 4 ), edgecolor = \"black\", alpha = 0.7)\n",
    "\n",
    "# ê° ë“±ê¸‰ë³„ í™˜ì ìˆ˜ ë° ë¹„ìœ¨ í‘œê¸°\n",
    "total_n : int = len(y_prob)\n",
    "for bar in bars :\n",
    "    height = bar.get_height()\n",
    "    ax_tier.text(\n",
    "        bar.get_x() + bar.get_width()/2, height + 50,\n",
    "        f\"N = {int(height)}\\n({height/total_n:.1%})\",\n",
    "        ha = \"center\", va = \"bottom\", fontsize = 11, fontweight = \"bold\"\n",
    "    )\n",
    "\n",
    "ax_tier.set_title( \"Clinical Risk Stratification : ìœ„í—˜ ë“±ê¸‰ë³„ í™˜ì ë¶„í¬\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "ax_tier.set_xlabel( \"Risk Tier\", fontsize = 12 )\n",
    "ax_tier.set_ylabel( \"Patient Count\", fontsize = 12 )\n",
    "ax_tier.set_ylim( 0, max(tier_counts) * 1.2 )\n",
    "ax_tier.grid( axis = \"y\", alpha = 0.2 )\n",
    "plt.tight_layout()\n",
    "\n",
    "force_fix_plot_tofu(fig_tier, ax_tier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca1fd0",
   "metadata": {},
   "source": [
    "# 30. Engineering Completion : Robustness, Calibration, and Tier-specific SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378dcf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import brier_score_loss, confusion_matrix\n",
    "\n",
    "# GPU ë””ë°”ì´ìŠ¤ ì„¤ì • ë° ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. GPU-Accelerated Feature Sensitivity Stress Test\n",
    "def conduct_stress_test_gpu(model, X_arr, noise_level : float = 0.05, n_iterations : int = 30) :\n",
    "    # ë°ì´í„°ë¥¼ GPU í…ì„œë¡œ ë³€í™˜\n",
    "    X_tensor : torch.Tensor = torch.tensor(X_arr, dtype = torch.float32).to(device)\n",
    "    \n",
    "    # ì›ë³¸ í™•ë¥  ì¶”ì¶œ (TabNet ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)\n",
    "    original_probs : np.ndarray = model.predict_proba(X_arr)[ :, 1 ]\n",
    "    volatilities : list = []\n",
    "    \n",
    "    for _ in range(n_iterations) :\n",
    "        # GPU ë‚´ì—ì„œ ì§ì ‘ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±í•˜ì—¬ ì—°ì‚° ì†ë„ ê·¹ëŒ€í™”\n",
    "        noise : torch.Tensor = torch.randn_like(X_tensor) * noise_level\n",
    "        X_noisy_tensor : torch.Tensor = X_tensor + (X_tensor * noise)\n",
    "        \n",
    "        # ëª¨ë¸ ì¶”ë¡ \n",
    "        X_noisy_np : np.ndarray = X_noisy_tensor.cpu().numpy()\n",
    "        noisy_probs : np.ndarray = model.predict_proba(X_noisy_np)[ :, 1 ]\n",
    "        \n",
    "        diff : np.ndarray = np.abs(noisy_probs - original_probs)\n",
    "        volatilities.append(np.mean(diff))\n",
    "        \n",
    "    return volatilities\n",
    "\n",
    "noise_levels : list = [ 0.05, 0.10, 0.15 ]\n",
    "stress_results : dict = {}\n",
    "\n",
    "for nl in noise_levels :\n",
    "    stress_results[ f\"{int(nl*100)}%\" ] = conduct_stress_test_gpu(clf, X_test_arr, noise_level = nl)\n",
    "\n",
    "fig_stress, ax_stress = plt.subplots(figsize = ( 10, 6 ))\n",
    "sns.boxplot(data = pd.DataFrame(stress_results), ax = ax_stress, palette = \"pastel\")\n",
    "\n",
    "for i, nl in enumerate(noise_levels) :\n",
    "    mean_vol : float = np.mean(stress_results[ f\"{int(nl*100)}%\" ])\n",
    "    ax_stress.text(i, mean_vol + 0.002, f\"Mean Volatility : {mean_vol:.4f}\", \n",
    "                  ha = \"center\", fontweight = \"bold\", color = \"black\")\n",
    "\n",
    "ax_stress.set_title( \"Feature Sensitivity Stress Test : GPU ê°€ì† ë°ì´í„° ê²¬ê³ ì„±\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "ax_stress.set_xlabel( \"Noise Level\", fontsize = 12 )\n",
    "ax_stress.set_ylabel( \"Mean Probability Volatility\", fontsize = 12 )\n",
    "ax_stress.grid( axis = \"y\", alpha = 0.2 )\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_stress, ax_stress)\n",
    "plt.show()\n",
    "\n",
    "# 2. GPU-Based Expected Calibration Error (ECE)\n",
    "def calculate_ece_gpu(y_true, y_prob, n_bins : int = 10) :\n",
    "    y_true_t : torch.Tensor = torch.tensor(y_true, dtype = torch.float32).to(device)\n",
    "    y_prob_t : torch.Tensor = torch.tensor(y_prob, dtype = torch.float32).to(device)\n",
    "    \n",
    "    bin_boundaries : torch.Tensor = torch.linspace(0, 1, n_bins + 1).to(device)\n",
    "    ece : torch.Tensor = torch.tensor(0.0).to(device)\n",
    "    \n",
    "    for i in range(n_bins) :\n",
    "        lower : torch.Tensor = bin_boundaries[ i ]\n",
    "        upper : torch.Tensor = bin_boundaries[ i + 1 ]\n",
    "        \n",
    "        # ë§ˆìŠ¤í‚¹ ì—°ì‚°ì„ í†µí•œ ë¹ˆ(Bin) í• ë‹¹\n",
    "        in_bin : torch.Tensor = (y_prob_t > lower) & (y_prob_t <= upper)\n",
    "        prop_in_bin : torch.Tensor = torch.mean(in_bin.float())\n",
    "        \n",
    "        if prop_in_bin > 0 :\n",
    "            acc_in_bin : torch.Tensor = torch.mean(y_true_t[ in_bin ])\n",
    "            conf_in_bin : torch.Tensor = torch.mean(y_prob_t[ in_bin ])\n",
    "            ece += torch.abs(conf_in_bin - acc_in_bin) * prop_in_bin\n",
    "            \n",
    "    return ece.item()\n",
    "\n",
    "ece_val : float = calculate_ece_gpu(y_true_arr, y_prob)\n",
    "brier_val : float = brier_score_loss(y_true_arr, y_prob)\n",
    "\n",
    "print( f\"\\n[ Quantitative Calibration Assessment - GPU Accelerated ]\" )\n",
    "print( f\"Expected Calibration Error (ECE) : {ece_val:.4f}\" )\n",
    "print( f\"Brier Score : {brier_val:.4f}\" )\n",
    "\n",
    "# 3. Urgent-Tier SHAP Analysis : Multi-colored Visualization\n",
    "urgent_mask : np.ndarray = (y_prob >= 0.8)\n",
    "X_urgent : np.ndarray = X_test_arr[ urgent_mask ]\n",
    "if len(X_urgent) > 200 :\n",
    "    X_urgent = X_urgent[ np.random.choice(len(X_urgent), 200, replace = False) ]\n",
    "\n",
    "shap_vals_urgent : np.ndarray = explainer_local.shap_values(X_urgent)\n",
    "\n",
    "fig_urgent, ax_urgent = plt.subplots(figsize = ( 10, 8 ))\n",
    "mean_abs_shap_urgent : np.ndarray = np.abs(shap_vals_urgent).mean(axis = 0)\n",
    "top_5_idx_urgent : np.ndarray = np.argsort(mean_abs_shap_urgent)[ -5: ]\n",
    "num_bars : int = len(top_5_idx_urgent)\n",
    "\n",
    "colors_urgent : list = plt.cm.get_cmap(\"tab10\")(np.linspace(0, 1, num_bars))\n",
    "\n",
    "bars_urgent = ax_urgent.barh(\n",
    "    np.array(feature_names_ko)[ top_5_idx_urgent ], \n",
    "    mean_abs_shap_urgent[ top_5_idx_urgent ], \n",
    "    color = colors_urgent, alpha = 0.8\n",
    ")\n",
    "\n",
    "for bar in bars_urgent :\n",
    "    width = bar.get_width()\n",
    "    ax_urgent.text(width + 0.002, bar.get_y() + bar.get_height()/2, f\"{width:.4f}\", va = \"center\", fontweight = \"bold\")\n",
    "\n",
    "ax_urgent.set_title( \"Urgent Tier Feature Contribution : ì´ˆê³ ìœ„í—˜êµ° í•µì‹¬ ì¸ì\", fontsize = 16, fontweight = \"bold\", pad = 20 )\n",
    "ax_urgent.set_xlabel( \"Mean Absolute SHAP Value\", fontsize = 12 )\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_urgent, ax_urgent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94432493",
   "metadata": {},
   "source": [
    "# 31. Prescriptive AI Simulation & Deployment Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06994124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clf.network.to(device)\n",
    "clf.network.float()\n",
    "\n",
    "# 1. Counterfactual What-If Simulation\n",
    "urgent_indices : np.ndarray = np.where(y_prob >= 0.8)[ 0 ]\n",
    "if len(urgent_indices) > 0 :\n",
    "    target_idx : int = urgent_indices[ 0 ]\n",
    "    X_base : np.ndarray = X_test_arr[ target_idx ].copy()\n",
    "    base_prob : float = y_prob[ target_idx ]\n",
    "\n",
    "    target_feat_1 : int = top_5_idx_urgent[ -1 ]\n",
    "    target_feat_2 : int = top_5_idx_urgent[ -2 ]\n",
    "\n",
    "    X_sim_1 : np.ndarray = X_base.copy()\n",
    "    X_sim_1[ target_feat_1 ] = np.median(X_test_arr[ :, target_feat_1 ])\n",
    "    prob_sim_1 : float = clf.predict_proba(X_sim_1.reshape(1, -1))[ 0, 1 ]\n",
    "\n",
    "    X_sim_2 : np.ndarray = X_sim_1.copy()\n",
    "    X_sim_2[ target_feat_2 ] = np.median(X_test_arr[ :, target_feat_2 ])\n",
    "    prob_sim_2 : float = clf.predict_proba(X_sim_2.reshape(1, -1))[ 0, 1 ]\n",
    "\n",
    "    scenarios : list = [ \"Baseline\", f\"Control: {feature_names_ko[target_feat_1]}\", \"Control: Both Features\" ]\n",
    "    probs : list = [ base_prob, prob_sim_1, prob_sim_2 ]\n",
    "\n",
    "    fig_cf, ax_cf = plt.subplots(figsize = ( 10, 6 ))\n",
    "    colors_cf : list = plt.cm.get_cmap(\"tab10\")(np.linspace(0, 1, len(probs)))\n",
    "\n",
    "    bars_cf = ax_cf.bar(scenarios, probs, color = colors_cf, alpha = 0.85, edgecolor = \"black\")\n",
    "\n",
    "    for bar in bars_cf :\n",
    "        height = bar.get_height()\n",
    "        ax_cf.text(bar.get_x() + bar.get_width() / 2, height + 0.02, f\"{height:.4f}\", ha = \"center\", va = \"bottom\", fontweight = \"bold\", fontsize = 12)\n",
    "\n",
    "    ax_cf.set_title(\"Counterfactual What-If Simulation\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "    ax_cf.set_ylabel(\"Predicted Risk Probability\", fontsize = 12)\n",
    "    ax_cf.set_ylim(0, 1.1)\n",
    "    ax_cf.grid(axis = \"y\", ls = \"--\", alpha = 0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    try :\n",
    "        force_fix_plot_tofu(fig_cf, ax_cf)\n",
    "    except NameError :\n",
    "        pass\n",
    "    plt.show()\n",
    "\n",
    "# 2. Partial Dependence Plot (PDP)\n",
    "feat_idx : int = top_5_idx_urgent[ -2 ]\n",
    "feat_name : str = feature_names_ko[ feat_idx ]\n",
    "\n",
    "unique_vals : np.ndarray = np.unique(X_test_arr[ :, feat_idx ])\n",
    "\n",
    "if len(unique_vals) <= 2 :\n",
    "    grid_vals : np.ndarray = unique_vals\n",
    "else :\n",
    "    p05 : float = np.percentile(X_test_arr[ :, feat_idx ], 5)\n",
    "    p95 : float = np.percentile(X_test_arr[ :, feat_idx ], 95)\n",
    "    grid_vals : np.ndarray = np.linspace(p05, p95, 50) if p05 != p95 else unique_vals\n",
    "\n",
    "pdp_probs : list = []\n",
    "X_temp : np.ndarray = X_test_arr.copy()\n",
    "\n",
    "for val in grid_vals :\n",
    "    X_temp[ :, feat_idx ] = val\n",
    "    mean_prob : float = clf.predict_proba(X_temp)[ :, 1 ].mean()\n",
    "    pdp_probs.append(mean_prob)\n",
    "\n",
    "fig_pdp, ax_pdp = plt.subplots(figsize = ( 10, 6 ))\n",
    "\n",
    "if len(grid_vals) <= 2 :\n",
    "    ax_pdp.plot(grid_vals, pdp_probs, color = \"crimson\", lw = 3, marker = \"o\", markersize = 10)\n",
    "    ax_pdp.set_xticks(grid_vals)\n",
    "else :\n",
    "    ax_pdp.plot(grid_vals, pdp_probs, color = \"crimson\", lw = 3)\n",
    "\n",
    "min_idx : int = np.argmin(pdp_probs)\n",
    "max_idx : int = np.argmax(pdp_probs)\n",
    "\n",
    "ax_pdp.scatter([ grid_vals[ min_idx ], grid_vals[ max_idx ] ], [ pdp_probs[ min_idx ], pdp_probs[ max_idx ] ], color = \"black\", s = 100, zorder = 5)\n",
    "\n",
    "# ìˆ˜ì¹˜ ìœ„ì¹˜ êµì •\n",
    "ax_pdp.annotate(f\"Min: {pdp_probs[min_idx]:.4f}\", xy = (grid_vals[ min_idx ], pdp_probs[ min_idx ]), xytext = (0, -20), textcoords = \"offset points\", ha = \"center\", fontweight = \"bold\")\n",
    "ax_pdp.annotate(f\"Max: {pdp_probs[max_idx]:.4f}\", xy = (grid_vals[ max_idx ], pdp_probs[ max_idx ]), xytext = (0, 15), textcoords = \"offset points\", ha = \"center\", fontweight = \"bold\")\n",
    "\n",
    "ax_pdp.set_title(f\"Partial Dependence Plot : {feat_name}\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_pdp.set_xlabel(f\"Feature Value\", fontsize = 12)\n",
    "ax_pdp.set_ylabel(\"Mean Predicted Probability\", fontsize = 12)\n",
    "\n",
    "# yì¶• ìŠ¤ì¼€ì¼ ë™ì  ì—¬ë°± í™•ë³´\n",
    "margin : float = (max(pdp_probs) - min(pdp_probs)) * 0.2 if max(pdp_probs) != min(pdp_probs) else 0.05\n",
    "ax_pdp.set_ylim(min(pdp_probs) - margin, max(pdp_probs) + margin)\n",
    "\n",
    "ax_pdp.grid(alpha = 0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "try :\n",
    "    force_fix_plot_tofu(fig_pdp, ax_pdp)\n",
    "except NameError :\n",
    "    pass\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b7d6d",
   "metadata": {},
   "source": [
    "# 32. Clinical Research Validation : Ablation Study & Algorithmic Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05319d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. Minimal Feature Set Ablation (ìµœì†Œ ë³€ìˆ˜ í†µì œ ê²€ì¦)\n",
    "top_5_features_idx : np.ndarray = top_10_idx[ -5: ]\n",
    "X_test_ablated : np.ndarray = X_test_arr.copy()\n",
    "\n",
    "num_features : int = X_test_arr.shape[ 1 ]\n",
    "for i in range(num_features) :\n",
    "    if i not in top_5_features_idx :\n",
    "        X_test_ablated[ :, i ] = np.median(X_test_arr[ :, i ])\n",
    "\n",
    "# í†µì œëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "y_prob_minimal : np.ndarray = clf.predict_proba(X_test_ablated)[ :, 1 ]\n",
    "\n",
    "auc_min : float = roc_auc_score(y_true_arr, y_prob_minimal)\n",
    "prc_min : float = average_precision_score(y_true_arr, y_prob_minimal)\n",
    "\n",
    "auc_full : float = roc_auc_score(y_true_arr, y_prob)\n",
    "prc_full : float = average_precision_score(y_true_arr, y_prob)\n",
    "\n",
    "fig_abl, ax_abl = plt.subplots(figsize = ( 10, 6 ))\n",
    "metrics_labels : list = [ \"Full AUC (All Feat)\", \"Minimal AUC (Top 5 Only)\", \"Full PRC (All Feat)\", \"Minimal PRC (Top 5 Only)\" ]\n",
    "metrics_vals : list = [ auc_full, auc_min, prc_full, prc_min ]\n",
    "colors_abl : list = plt.cm.get_cmap(\"tab10\")(np.linspace(0, 1, len(metrics_labels)))\n",
    "\n",
    "bars_abl = ax_abl.bar(metrics_labels, metrics_vals, color = colors_abl, alpha = 0.85, edgecolor = \"black\")\n",
    "\n",
    "for bar in bars_abl :\n",
    "    height = bar.get_height()\n",
    "    ax_abl.text(bar.get_x() + bar.get_width() / 2, height + 0.02, f\"{height:.4f}\", ha = \"center\", va = \"bottom\", fontweight = \"bold\", fontsize = 12)\n",
    "\n",
    "ax_abl.set_title(\"Minimal Feature Set Ablation Study\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_abl.set_ylabel(\"Score\", fontsize = 12)\n",
    "ax_abl.set_ylim(0, 1.15)\n",
    "ax_abl.grid(axis = \"y\", ls = \"--\", alpha = 0.3)\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_abl, ax_abl)\n",
    "plt.show()\n",
    "\n",
    "# 2. Algorithmic Fairness Assessment (ì¸êµ¬í†µê³„í•™ì  ê³µì •ì„± ê²€ì¦)\n",
    "gender_idx : int = feature_names_ko.index(\"ì„±ë³„\")\n",
    "gender_vals : np.ndarray = X_test_arr[ :, gender_idx ]\n",
    "unique_genders : np.ndarray = np.unique(gender_vals)\n",
    "\n",
    "fairness_records : list = []\n",
    "threshold : float = 0.3965\n",
    "\n",
    "for g in unique_genders :\n",
    "    mask : np.ndarray = (gender_vals == g)\n",
    "    y_t : np.ndarray = y_true_arr[ mask ]\n",
    "    y_p : np.ndarray = y_prob[ mask ]\n",
    "    \n",
    "    y_pred_bin : np.ndarray = (y_p >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_t, y_pred_bin).ravel()\n",
    "    \n",
    "    tpr : float = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr : float = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    gender_label : str = \"Male\" if g == 1.0 else \"Female\"\n",
    "    fairness_records.append({ \"Demographic_Group\" : f\"Gender : {gender_label}\", \"TPR (Recall)\" : tpr, \"FPR (False Alarm)\" : fpr })\n",
    "\n",
    "df_fairness : pd.DataFrame = pd.DataFrame(fairness_records).set_index(\"Demographic_Group\")\n",
    "\n",
    "fig_fair, ax_fair = plt.subplots(figsize = ( 10, 6 ))\n",
    "df_fairness.plot(kind = \"bar\", ax = ax_fair, color = [ \"#0047AB\", \"crimson\" ], alpha = 0.85, edgecolor = \"black\")\n",
    "\n",
    "for container in ax_fair.containers :\n",
    "    labels : list = [ f\"{v.get_height():.4f}\" for v in container ]\n",
    "    ax_fair.bar_label(container, labels = labels, label_type = \"edge\", fontsize = 11, fontweight = \"bold\", padding = 3)\n",
    "\n",
    "ax_fair.set_title(\"Algorithmic Fairness Assessment (Equal Opportunity)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_fair.set_xlabel(\"Demographic Subgroup\", fontsize = 12)\n",
    "ax_fair.set_ylabel(\"Metric Rate\", fontsize = 12)\n",
    "ax_fair.set_ylim(0, 1.2)\n",
    "ax_fair.legend(loc = \"upper right\")\n",
    "ax_fair.grid(axis = \"y\", ls = \"--\", alpha = 0.3)\n",
    "plt.xticks(rotation = 0)\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_fair, ax_fair)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e688142a",
   "metadata": {},
   "source": [
    "# 33. Reliability & Out-of-Distribution Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe2091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 1. Reliability Diagram (Calibration Curve ì‹œê°í™”)\n",
    "prob_true, prob_pred = calibration_curve(y_true_arr, y_prob, n_bins = 10)\n",
    "\n",
    "fig_cal, ax_cal = plt.subplots(figsize = ( 8, 8 ))\n",
    "ax_cal.plot(prob_pred, prob_true, marker = \"s\", ls = \"-\", color = \"#0047AB\", label = \"TabNet Calibration\")\n",
    "ax_cal.plot([ 0, 1 ], [ 0, 1 ], ls = \"--\", color = \"gray\", label = \"Perfectly Calibrated\")\n",
    "\n",
    "# ì •ëŸ‰ì  ì§€í‘œ(ECE) ì‚½ì…\n",
    "ax_cal.text(0.05, 0.95, f\"ECE : {ece_val:.4f}\", fontsize = 12, fontweight = \"bold\", \n",
    "            bbox = dict(boxstyle = \"round\", facecolor = \"white\", alpha = 0.8))\n",
    "\n",
    "for p, t in zip(prob_pred, prob_true) :\n",
    "    ax_cal.annotate(\n",
    "        f\"{t:.3f}\", \n",
    "        (p, t), textcoords = \"offset points\", xytext = ( 0, 10 ), \n",
    "        ha = \"center\", fontsize = 10, fontweight = \"bold\", color = \"#0047AB\"\n",
    "    )\n",
    "\n",
    "ax_cal.set_title(\"Reliability Diagram (Calibration Curve)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_cal.set_xlabel(\"Mean Predicted Probability\", fontsize = 12)\n",
    "ax_cal.set_ylabel(\"Fraction of Positives\", fontsize = 12)\n",
    "ax_cal.legend(loc = \"lower right\")\n",
    "ax_cal.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Out-of-Distribution (OOD) Detection\n",
    "iso_forest : IsolationForest = IsolationForest(n_estimators = 200, contamination = 0.05, random_state = 42)\n",
    "iso_forest.fit(X_test_arr)\n",
    "\n",
    "anomaly_scores : np.ndarray = iso_forest.decision_function(X_test_arr)\n",
    "is_inlier : np.ndarray = iso_forest.predict(X_test_arr) \n",
    "ood_count : int = np.sum(is_inlier == -1)\n",
    "\n",
    "fig_ood, ax_ood = plt.subplots(figsize = ( 12, 7 ))\n",
    "counts, bins, patches = ax_ood.hist(anomaly_scores, bins = 50, color = \"#4A90E2\", edgecolor = \"white\", alpha = 0.8)\n",
    "sns.kdeplot(anomaly_scores, ax = ax_ood, color = \"#002255\", lw = 2)\n",
    "\n",
    "# íˆìŠ¤í† ê·¸ë¨ ë§‰ëŒ€ë³„ ë¹ˆë„ìˆ˜ í…ìŠ¤íŠ¸ ë§¤í•‘\n",
    "for count, patch in zip(counts, patches) :\n",
    "    if count > 0 :\n",
    "        ax_ood.text(\n",
    "            patch.get_x() + patch.get_width() / 2, count + (np.max(counts) * 0.015), \n",
    "            f\"{int(count)}\", \n",
    "            ha = \"center\", va = \"bottom\", fontsize = 9, fontweight = \"bold\", color = \"#333333\"\n",
    "        )\n",
    "\n",
    "# OOD íŒì • ì„ê³„ì¹˜(í•˜ìœ„ 5%) ì‹œê°í™”\n",
    "threshold : float = np.percentile(anomaly_scores, 5)\n",
    "ax_ood.axvline(threshold, color = \"crimson\", ls = \"--\", lw = 2.5)\n",
    "\n",
    "ax_ood.annotate(\n",
    "    f\"OOD Threshold\\n(5% : {threshold:.4f})\", \n",
    "    xy = (threshold, np.max(counts) * 0.8), xytext = (threshold - 0.03, np.max(counts) * 0.9),\n",
    "    arrowprops = dict(arrowstyle = \"->\", color = \"crimson\", lw = 1.5),\n",
    "    fontsize = 11, fontweight = \"bold\", color = \"crimson\", ha = \"right\"\n",
    ")\n",
    "\n",
    "summary_text : str = f\"[ OOD Summary ]\\nTotal : {len(anomaly_scores)}\\nNormal : {len(anomaly_scores) - ood_count}\\nOOD : {ood_count} (5%)\"\n",
    "ax_ood.text(\n",
    "    0.02, 0.95, summary_text, transform = ax_ood.transAxes, \n",
    "    fontsize = 11, fontweight = \"bold\", va = \"top\", \n",
    "    bbox = dict(boxstyle = \"round,pad=0.5\", facecolor = \"white\", edgecolor = \"black\", alpha = 0.9)\n",
    ")\n",
    "\n",
    "ax_ood.set_title(\"OOD Detection : Anomaly Score Distribution\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_ood.set_xlabel(\"Anomaly Score (Higher is more normal)\", fontsize = 12)\n",
    "ax_ood.set_ylabel(\"Frequency\", fontsize = 12)\n",
    "ax_ood.set_ylim(0, np.max(counts) * 1.15)\n",
    "ax_ood.grid(axis = \"y\", alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fb0ef",
   "metadata": {},
   "source": [
    "# 34. Multi-level Model Interpretability : Internal Attention, Feature Interaction, and Clinical Rule Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# 1. TabNet Local Attention Mask Visualization\n",
    "explain_matrix, masks = clf.explain(X_test_arr)\n",
    "\n",
    "fig_mask, axes = plt.subplots(1, 3, figsize = ( 16, 6 ))\n",
    "sample_limit : int = 15\n",
    "\n",
    "for i in range(3) :\n",
    "    ax = axes[ i ]\n",
    "    mask_data : np.ndarray = masks[ i ][ :sample_limit, : ]\n",
    "    cax = ax.imshow(mask_data, cmap = \"viridis\", aspect = \"auto\")\n",
    "    \n",
    "    ax.set_title(f\"Attention Mask : Step {i+1}\", fontsize = 14, fontweight = \"bold\", pad = 10)\n",
    "    ax.set_xlabel(\"Feature Index\", fontsize = 11)\n",
    "    ax.set_ylabel(\"Sample Index\", fontsize = 11)\n",
    "    \n",
    "    for row in range(sample_limit) :\n",
    "        top_feat_idx : int = np.argmax(mask_data[ row ])\n",
    "        top_val : float = mask_data[ row, top_feat_idx ]\n",
    "        ax.text(\n",
    "            top_feat_idx, row, f\"{top_val:.2f}\", \n",
    "            ha = \"center\", va = \"center\", color = \"white\", fontsize = 8, fontweight = \"bold\"\n",
    "        )\n",
    "\n",
    "plt.colorbar(cax, ax = axes, fraction = 0.02, pad = 0.04, label = \"Attention Weight\")\n",
    "plt.suptitle(\"TabNet Local Attention Weights per Step (Top features annotated)\", fontsize = 18, fontweight = \"bold\")\n",
    "force_fix_plot_tofu(fig_mask, axes[ 0 ])\n",
    "plt.show()\n",
    "\n",
    "# 2. SHAP Interaction Value Analysis\n",
    "np.random.seed(42)\n",
    "sample_idx : np.ndarray = np.random.choice(len(X_test_arr), 200, replace = False)\n",
    "X_interact : np.ndarray = X_test_arr[ sample_idx ]\n",
    "shap_vals_interact : np.ndarray = explainer_local.shap_values(X_interact)\n",
    "\n",
    "feat_1_idx : int = top_5_idx_urgent[ -1 ]\n",
    "feat_2_idx : int = top_5_idx_urgent[ -2 ]\n",
    "\n",
    "fig_shap, ax_shap = plt.subplots(figsize = ( 11, 7 ))\n",
    "scatter = ax_shap.scatter(\n",
    "    X_interact[ :, feat_1_idx ], shap_vals_interact[ :, feat_1_idx ],\n",
    "    c = X_interact[ :, feat_2_idx ], cmap = \"coolwarm\", alpha = 0.8, edgecolors = \"black\", s = 60\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax = ax_shap)\n",
    "cbar.set_label(f\"Interaction : {feature_names_ko[feat_2_idx]}\", fontsize = 11, fontweight = \"bold\")\n",
    "\n",
    "max_idx : int = np.argmax(shap_vals_interact[ :, feat_1_idx ])\n",
    "min_idx : int = np.argmin(shap_vals_interact[ :, feat_1_idx ])\n",
    "\n",
    "ax_shap.annotate(\n",
    "    f\"Max SHAP : {shap_vals_interact[max_idx, feat_1_idx]:.3f}\\n(Val : {X_interact[max_idx, feat_1_idx]:.1f})\", \n",
    "    (X_interact[ max_idx, feat_1_idx ], shap_vals_interact[ max_idx, feat_1_idx ]),\n",
    "    xytext = (10, -15), textcoords = \"offset points\", fontweight = \"bold\", color = \"crimson\",\n",
    "    arrowprops = dict(arrowstyle = \"->\", color = \"crimson\")\n",
    ")\n",
    "\n",
    "ax_shap.annotate(\n",
    "    f\"Min SHAP : {shap_vals_interact[min_idx, feat_1_idx]:.3f}\\n(Val : {X_interact[min_idx, feat_1_idx]:.1f})\", \n",
    "    (X_interact[ min_idx, feat_1_idx ], shap_vals_interact[ min_idx, feat_1_idx ]),\n",
    "    xytext = (-20, 20), textcoords = \"offset points\", fontweight = \"bold\", color = \"#0047AB\",\n",
    "    arrowprops = dict(arrowstyle = \"->\", color = \"#0047AB\")\n",
    ")\n",
    "\n",
    "ax_shap.set_title(f\"SHAP Interaction : {feature_names_ko[feat_1_idx]}\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_shap.set_xlabel(f\"Feature Value : {feature_names_ko[feat_1_idx]}\", fontsize = 12)\n",
    "ax_shap.set_ylabel(\"SHAP Value (Impact on Model Output)\", fontsize = 12)\n",
    "ax_shap.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_shap, ax_shap)\n",
    "plt.show()\n",
    "\n",
    "# 3. Global Surrogate Decision Tree\n",
    "surrogate_tree : DecisionTreeClassifier = DecisionTreeClassifier(max_depth = 3, random_state = 42)\n",
    "y_pred_surrogate : np.ndarray = (y_prob >= 0.3965).astype(int)\n",
    "surrogate_tree.fit(X_test_arr, y_pred_surrogate)\n",
    "\n",
    "fig_tree, ax_tree = plt.subplots(figsize = ( 20, 10 ))\n",
    "plot_tree(\n",
    "    surrogate_tree, \n",
    "    feature_names = feature_names_ko, \n",
    "    class_names = [ \"Low_Risk\", \"High_Risk\" ], \n",
    "    filled = True, rounded = True, ax = ax_tree, \n",
    "    fontsize = 12, impurity = False, proportion = True\n",
    ")\n",
    "\n",
    "ax_tree.set_title(\"Global Surrogate Decision Tree (Clinical Rule Extraction)\", fontsize = 20, fontweight = \"bold\", pad = 20)\n",
    "plt.tight_layout()\n",
    "force_fix_plot_tofu(fig_tree, ax_tree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a2d8a",
   "metadata": {},
   "source": [
    "# 35. Statistical Rigor and Clinical Error Profiling : 95% CI Bootstrapping & Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# 1. Bootstrapping 95% Confidence Intervals\n",
    "n_iterations : int = 1000\n",
    "n_size : int = len(y_true_arr)\n",
    "auc_scores : list = []\n",
    "prc_scores : list = []\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(n_iterations) :\n",
    "    idx : np.ndarray = np.random.randint(0, n_size, n_size)\n",
    "    y_true_boot : np.ndarray = y_true_arr[ idx ]\n",
    "    y_prob_boot : np.ndarray = y_prob[ idx ]\n",
    "    \n",
    "    if len(np.unique(y_true_boot)) < 2 :\n",
    "        continue\n",
    "        \n",
    "    auc_scores.append(roc_auc_score(y_true_boot, y_prob_boot))\n",
    "    prc_scores.append(average_precision_score(y_true_boot, y_prob_boot))\n",
    "\n",
    "auc_lower : float = np.percentile(auc_scores, 2.5)\n",
    "auc_upper : float = np.percentile(auc_scores, 97.5)\n",
    "prc_lower : float = np.percentile(prc_scores, 2.5)\n",
    "prc_upper : float = np.percentile(prc_scores, 97.5)\n",
    "\n",
    "fig_ci, axes_ci = plt.subplots(1, 2, figsize = ( 14, 6 ))\n",
    "\n",
    "sns.histplot(auc_scores, bins = 30, kde = True, ax = axes_ci[ 0 ], color = \"#0047AB\", alpha = 0.6)\n",
    "axes_ci[ 0 ].axvline(auc_lower, color = \"crimson\", ls = \"--\", lw = 2)\n",
    "axes_ci[ 0 ].axvline(auc_upper, color = \"crimson\", ls = \"--\", lw = 2)\n",
    "axes_ci[ 0 ].set_title(f\"AUROC 95% CI : {np.mean(auc_scores):.4f}\\n[{auc_lower:.4f} - {auc_upper:.4f}]\", fontsize = 13, fontweight = \"bold\")\n",
    "\n",
    "sns.histplot(prc_scores, bins = 30, kde = True, ax = axes_ci[ 1 ], color = \"#E24A33\", alpha = 0.6)\n",
    "axes_ci[ 1 ].axvline(prc_lower, color = \"black\", ls = \"--\", lw = 2)\n",
    "axes_ci[ 1 ].axvline(prc_upper, color = \"black\", ls = \"--\", lw = 2)\n",
    "axes_ci[ 1 ].set_title(f\"AUPRC 95% CI : {np.mean(prc_scores):.4f}\\n[{prc_lower:.4f} - {prc_upper:.4f}]\", fontsize = 13, fontweight = \"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "if 'force_fix_plot_tofu' in locals():\n",
    "    force_fix_plot_tofu(fig_ci, axes_ci[ 0 ])\n",
    "plt.show()\n",
    "\n",
    "# 2. Clinical Misclassification Profiling (Visualization Fix)\n",
    "threshold : float = 0.3965\n",
    "mask_tp : np.ndarray = (y_true_arr == 1) & (y_prob >= threshold)\n",
    "mask_fn : np.ndarray = (y_true_arr == 1) & (y_prob < threshold)\n",
    "\n",
    "tp_data : np.ndarray = X_test_arr[ mask_tp ]\n",
    "fn_data : np.ndarray = X_test_arr[ mask_fn ]\n",
    "\n",
    "profiling_records : list = []\n",
    "for idx in top_5_idx_urgent :\n",
    "    feat_name : str = feature_names_ko[ idx ]\n",
    "    tp_vals : np.ndarray = tp_data[ :, idx ]\n",
    "    fn_vals : np.ndarray = fn_data[ :, idx ]\n",
    "    t_stat, p_val = ttest_ind(tp_vals, fn_vals, equal_var = False)\n",
    "    profiling_records.append({\n",
    "        \"Feature\" : feat_name,\n",
    "        \"TP_Mean\" : np.mean(tp_vals),\n",
    "        \"FN_Mean\" : np.mean(fn_vals),\n",
    "        \"P_Value\" : p_val\n",
    "    })\n",
    "\n",
    "df_profile : pd.DataFrame = pd.DataFrame(profiling_records)\n",
    "\n",
    "fig_prof, ax_prof = plt.subplots(figsize = ( 14, 7 ))\n",
    "x_pos : np.ndarray = np.arange(len(top_5_idx_urgent))\n",
    "width : float = 0.35\n",
    "\n",
    "bar_tp = ax_prof.bar(x_pos - width/2, df_profile[ \"TP_Mean\" ], width, label = \"True Positive (Detected)\", color = \"#0047AB\", alpha = 0.85, edgecolor = \"black\")\n",
    "bar_fn = ax_prof.bar(x_pos + width/2, df_profile[ \"FN_Mean\" ], width, label = \"False Negative (Missed)\", color = \"crimson\", alpha = 0.85, edgecolor = \"black\")\n",
    "\n",
    "def label_bars(bars) :\n",
    "    for bar in bars :\n",
    "        height = bar.get_height()\n",
    "        va = \"bottom\" if height >= 0 else \"top\"\n",
    "        xytext = ( 0, 5 ) if height >= 0 else ( 0, -5 )\n",
    "        ax_prof.annotate(f\"{height:.3f}\",\n",
    "                         xy = (bar.get_x() + bar.get_width() / 2, height),\n",
    "                         xytext = xytext, textcoords = \"offset points\",\n",
    "                         ha = \"center\", va = va, fontsize = 10, fontweight = \"bold\")\n",
    "\n",
    "label_bars(bar_tp)\n",
    "label_bars(bar_fn)\n",
    "\n",
    "for i in range(len(top_5_idx_urgent)) :\n",
    "    pval : float = df_profile.loc[ i, \"P_Value\" ]\n",
    "    significance : str = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"*\" if pval < 0.05 else \"ns\"\n",
    "    max_h : float = max(df_profile.loc[ i, \"TP_Mean\" ], df_profile.loc[ i, \"FN_Mean\" ])\n",
    "    ax_prof.text(x_pos[ i ], (max_h + 0.1) if max_h > 0 else 0.1, significance, ha = \"center\", va = \"bottom\", fontsize = 14, fontweight = \"bold\", color = \"black\")\n",
    "\n",
    "# yì¶• ìƒí•œì„  í™•ëŒ€ ë° ë²”ë¡€ ìœ„ì¹˜ ë³€ê²½ìœ¼ë¡œ ìˆ˜ì¹˜ ê°€ë¦¼ í˜„ìƒ í•´ê²°\n",
    "y_min : float = df_profile[ [ \"TP_Mean\", \"FN_Mean\" ] ].values.min()\n",
    "y_max : float = df_profile[ [ \"TP_Mean\", \"FN_Mean\" ] ].values.max()\n",
    "ax_prof.set_ylim(y_min - 0.5, y_max + 0.8)  # yì¶• ë²”ìœ„ë¥¼ ëŠ˜ë ¤ ìƒë‹¨ ë°ì´í„° ë¼ë²¨ ê°€ë¦¼ ë°©ì§€\n",
    "ax_prof.legend(ncol = 2, loc = 'upper center', bbox_to_anchor = ( 0.5, -0.1 )) # ë²”ë¡€ë¥¼ ê·¸ë˜í”„ ë°– í•˜ë‹¨ìœ¼ë¡œ ì´ë™\n",
    "\n",
    "# í†µê³„ì  ìœ ì˜ì„± ë²”ë¡€ ìœ„ì¹˜ëŠ” í•˜ë‹¨ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ì—¬ ë§‰ëŒ€ì™€ ê²¹ì¹¨ ë°©ì§€\n",
    "p_val_legend : str = (\n",
    "    \"[ Statistical Significance ]\\n\"\n",
    "    \"*** : $p < 0.001$ (Very Strong Evidence)\\n\"\n",
    "    \"** : $p < 0.01$ (Strong Evidence)\\n\"\n",
    "    \"* : $p < 0.05$ (Significant)\\n\"\n",
    "    \"ns  : $p \\geq 0.05$ (Not Significant)\"\n",
    ")\n",
    "ax_prof.text(0.02, 0.05, p_val_legend, transform = ax_prof.transAxes, fontsize = 10, \n",
    "             fontweight = \"bold\", va = \"bottom\", bbox = dict(boxstyle = \"round\", facecolor = \"white\", alpha = 0.9))\n",
    "\n",
    "ax_prof.axhline(0, color = \"black\", lw = 1)\n",
    "ax_prof.set_xticks(x_pos)\n",
    "ax_prof.set_xticklabels(df_profile[ \"Feature\" ], rotation = 15, fontsize = 11, fontweight = \"bold\")\n",
    "ax_prof.set_title(\"Misclassification Profiling : Feature Mean Comparison (Top 5)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_prof.set_ylabel(\"Mean Feature Value (Z-score)\", fontsize = 12)\n",
    "ax_prof.grid(axis = \"y\", ls = \"--\", alpha = 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if 'force_fix_plot_tofu' in locals():\n",
    "    force_fix_plot_tofu(fig_prof, ax_prof)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723db3da",
   "metadata": {},
   "source": [
    "# 36. Model Deployment : ONNX Conversion & Latency Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. ê²½ë¡œ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "base_dir : str = r\"C:\\Users\\sck32\\Desktop\\Clinical Diabetes TabNet\"\n",
    "export_path : Path = Path(base_dir) / \"metabolic_ai_models\"\n",
    "export_path.mkdir(parents = True, exist_ok = True)\n",
    "onnx_file_path : str = str(export_path / \"tabnet_high_risk_model.onnx\")\n",
    "\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clf.network.to(device)\n",
    "clf.network.eval()\n",
    "\n",
    "# 2. ONNX ë³€í™˜ì„ ìœ„í•œ Dummy Input ìƒì„±\n",
    "dummy_input : torch.Tensor = torch.tensor(X_test_arr[ :1 ], dtype = torch.float32).to(device)\n",
    "\n",
    "# 3. ONNX Export\n",
    "try :\n",
    "    torch.onnx.export(\n",
    "        clf.network, \n",
    "        dummy_input, \n",
    "        onnx_file_path,\n",
    "        export_params = True,\n",
    "        opset_version = 13,\n",
    "        do_constant_folding = True,\n",
    "        input_names = [ \"input\" ],\n",
    "        output_names = [ \"output\" ],\n",
    "        dynamic_axes = { \"input\" : { 0 : \"batch_size\" }, \"output\" : { 0 : \"batch_size\" } }\n",
    "    )\n",
    "    \n",
    "    # ONNX ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦\n",
    "    onnx_model = onnx.load(onnx_file_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(f\"âœ… ONNX ë³€í™˜ ë° ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ : {onnx_file_path}\")\n",
    "\n",
    "except Exception as e :\n",
    "    print(f\"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨ : {e}\")\n",
    "\n",
    "# 4. Latency Benchmark (PyTorch vs ONNX)\n",
    "n_iters : int = 1000\n",
    "test_tensor : torch.Tensor = torch.tensor(X_test_arr[ :128 ], dtype = torch.float32).to(device)\n",
    "test_numpy : np.ndarray = test_tensor.cpu().numpy()\n",
    "\n",
    "# PyTorch Latency\n",
    "start_time = time.perf_counter()\n",
    "with torch.no_grad() :\n",
    "    for _ in range(n_iters) :\n",
    "        _ = clf.network(test_tensor)\n",
    "pytorch_latency : float = (time.perf_counter() - start_time) / n_iters * 1000\n",
    "\n",
    "# ONNX Runtime Latency (CPU Provider for safe universal deployment)\n",
    "ort_session = ort.InferenceSession(onnx_file_path, providers = [ \"CPUExecutionProvider\" ])\n",
    "start_time = time.perf_counter()\n",
    "for _ in range(n_iters) :\n",
    "    _ = ort_session.run(None, { \"input\" : test_numpy })\n",
    "onnx_latency : float = (time.perf_counter() - start_time) / n_iters * 1000\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"âš¡ Inference Latency Benchmark (Batch Size = 128)\")\n",
    "print(f\" - PyTorch ({device}) : {pytorch_latency:.3f} ms / batch\")\n",
    "print(f\" - ONNX (CPU)   : {onnx_latency:.3f} ms / batch\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c10bf8",
   "metadata": {},
   "source": [
    "# 37. Clinical Fairness Validation : Subgroup Disparity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "age_idx : int = valid_features.index(\"age\")\n",
    "sex_idx : int = valid_features.index(\"sex\")\n",
    "\n",
    "age_median : float = np.median(X_test_arr[ :, age_idx ])\n",
    "\n",
    "groups : dict = {\n",
    "    \"Male\" : X_test_arr[ :, sex_idx ] == 1,\n",
    "    \"Female\" : X_test_arr[ :, sex_idx ] == 2,\n",
    "    \"Age <= Median\" : X_test_arr[ :, age_idx ] <= age_median,\n",
    "    \"Age > Median\" : X_test_arr[ :, age_idx ] > age_median\n",
    "}\n",
    "\n",
    "opt_threshold : float = 0.0817 \n",
    "y_pred_opt : np.ndarray = (y_prob >= opt_threshold).astype(int)\n",
    "\n",
    "fairness_records : list = []\n",
    "\n",
    "for group_name, mask in groups.items() :\n",
    "    if np.sum(mask) == 0 :\n",
    "        continue\n",
    "        \n",
    "    y_true_g : np.ndarray = y_true_arr[ mask ]\n",
    "    y_pred_g : np.ndarray = y_pred_opt[ mask ]\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_g, y_pred_g, labels = [ 0, 1 ]).ravel()\n",
    "    \n",
    "    tpr : float = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    fpr : float = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    \n",
    "    fairness_records.append({\n",
    "        \"Subgroup\" : group_name,\n",
    "        \"TPR (Sensitivity)\" : tpr,\n",
    "        \"FPR (False Alarm)\" : fpr,\n",
    "        \"N\" : np.sum(mask)\n",
    "    })\n",
    "\n",
    "df_fairness : pd.DataFrame = pd.DataFrame(fairness_records)\n",
    "\n",
    "fig_fair, ax_fair = plt.subplots(figsize = ( 12, 6 ))\n",
    "x_pos : np.ndarray = np.arange(len(df_fairness))\n",
    "width : float = 0.35\n",
    "\n",
    "bar_tpr = ax_fair.bar(x_pos - width/2, df_fairness[ \"TPR (Sensitivity)\" ], width, label = \"TPR (True Positive Rate)\", color = \"#0047AB\", edgecolor = \"black\")\n",
    "bar_fpr = ax_fair.bar(x_pos + width/2, df_fairness[ \"FPR (False Alarm)\" ], width, label = \"FPR (False Positive Rate)\", color = \"crimson\", edgecolor = \"black\")\n",
    "\n",
    "def annotate_bars(bars) :\n",
    "    for bar in bars :\n",
    "        height = bar.get_height()\n",
    "        ax_fair.annotate(f\"{height:.3f}\",\n",
    "                         xy = (bar.get_x() + bar.get_width() / 2, height),\n",
    "                         xytext = (0, 3), textcoords = \"offset points\",\n",
    "                         ha = \"center\", va = \"bottom\", fontsize = 10, fontweight = \"bold\")\n",
    "\n",
    "annotate_bars(bar_tpr)\n",
    "annotate_bars(bar_fpr)\n",
    "\n",
    "ax_fair.set_xticks(x_pos)\n",
    "ax_fair.set_xticklabels([ f\"{row['Subgroup']}\\n(N={row['N']})\" for _, row in df_fairness.iterrows() ], fontsize = 11, fontweight = \"bold\")\n",
    "ax_fair.set_title(\"Model Fairness Validation (Equal Opportunity & Predictive Parity)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_fair.set_ylabel(\"Rate\", fontsize = 12)\n",
    "ax_fair.set_ylim(0, 1.15)\n",
    "ax_fair.legend(loc = \"upper right\")\n",
    "ax_fair.grid(axis = \"y\", ls = \"--\", alpha = 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if 'force_fix_plot_tofu' in locals() :\n",
    "    force_fix_plot_tofu(fig_fair, ax_fair)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308d5dd",
   "metadata": {},
   "source": [
    "# 38. Clinical Utility Validation : Non-Invasive Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# íƒ€ê²Ÿ ë° í•™ìŠµ ë°ì´í„° ë§¤í•‘\n",
    "X_train_arr : np.ndarray = X_train if isinstance(X_train, np.ndarray) else X_train.values\n",
    "X_test_arr : np.ndarray = X_test if isinstance(X_test, np.ndarray) else X_test.values\n",
    "y_train_arr : np.ndarray = y_train if isinstance(y_train, np.ndarray) else y_train.values\n",
    "y_true_arr : np.ndarray = y_test if isinstance(y_test, np.ndarray) else y_test.values\n",
    "\n",
    "# ë°ì´í„° ëˆ„ìˆ˜ ì§€í‘œ ì œê±° ë° í”¼ì²˜ ì¸ë±ì‹±\n",
    "leakage_features : list = [ \"HE_glu\", \"HE_HbA1c\" ] \n",
    "reduced_features : list = [ f for f in valid_features if f not in leakage_features ]\n",
    "\n",
    "reduced_idx : list = [ valid_features.index(f) for f in reduced_features ]\n",
    "X_train_red : np.ndarray = X_train_arr[ :, reduced_idx ]\n",
    "X_test_red : np.ndarray = X_test_arr[ :, reduced_idx ]\n",
    "\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clf_ni = TabNetClassifier(\n",
    "    n_d = 16, \n",
    "    n_a = 16,\n",
    "    optimizer_fn = torch.optim.Adam,\n",
    "    optimizer_params = { \"lr\" : 2e-2 },\n",
    "    scheduler_params = { \"step_size\" : 10, \"gamma\" : 0.9 },\n",
    "    scheduler_fn = torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type = \"entmax\",\n",
    "    device_name = device\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"ğŸš€ ë¹„ì¹¨ìŠµì  ì„ìƒ ì§€í‘œ ê¸°ë°˜ TabNet ì¬í•™ìŠµ ì‹œì‘ (Data Leakage ë°°ì œ)\")\n",
    "\n",
    "clf_ni.fit(\n",
    "    X_train = X_train_red, y_train = y_train_arr,\n",
    "    eval_set = [ (X_train_red, y_train_arr), (X_test_red, y_true_arr) ],\n",
    "    eval_name = [ \"train\", \"valid\" ],\n",
    "    eval_metric = [ \"auc\" ],\n",
    "    max_epochs = 200,\n",
    "    patience = 20,\n",
    "    batch_size = 1024,\n",
    "    virtual_batch_size = 128\n",
    ")\n",
    "\n",
    "preds_prob_ni : np.ndarray = clf_ni.predict_proba(X_test_red)[ :, 1 ]\n",
    "auc_ni : float = roc_auc_score(y_true_arr, preds_prob_ni)\n",
    "\n",
    "print(f\"âœ… Non-invasive Model Test AUC : {auc_ni:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9aabec",
   "metadata": {},
   "source": [
    "# 39. Cost Sensitive Optimization : Asymmetric Cost Matrix Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ë¹„ìš© ê°€ì¤‘ì¹˜ ì„¤ì • (FN : 5.0, FP : 1.0)\n",
    "cost_FN : float = 5.0 \n",
    "cost_FP : float = 1.0  \n",
    "\n",
    "thresholds : np.ndarray = np.linspace(0.01, 0.99, 100)\n",
    "costs : list = []\n",
    "\n",
    "for t in thresholds :\n",
    "    preds_t : np.ndarray = (preds_prob_ni >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_arr, preds_t).ravel()\n",
    "    total_cost : float = (fn * cost_FN) + (fp * cost_FP)\n",
    "    expected_cost : float = total_cost / len(y_true_arr)\n",
    "    costs.append(expected_cost)\n",
    "\n",
    "opt_idx : int = np.argmin(costs)\n",
    "opt_thresh_cost : float = thresholds[ opt_idx ]\n",
    "min_cost : float = costs[ opt_idx ]\n",
    "\n",
    "fig_cost, ax_cost = plt.subplots(figsize = ( 10, 6 ))\n",
    "\n",
    "ax_cost.plot(thresholds, costs, color = \"#0047AB\", lw = 3, label = \"Expected Cost Curve\")\n",
    "ax_cost.scatter([ opt_thresh_cost ], [ min_cost ], color = \"crimson\", s = 100, zorder = 5)\n",
    "ax_cost.annotate(\n",
    "    f\"Optimal Threshold : {opt_thresh_cost:.4f}\\nMin Expected Cost : {min_cost:.4f}\",\n",
    "    xy = (opt_thresh_cost, min_cost),\n",
    "    xytext = (opt_thresh_cost + 0.05, min_cost + 0.05),\n",
    "    arrowprops = dict(facecolor = \"black\", shrink = 0.05, width = 1, headwidth = 8),\n",
    "    fontsize = 11, fontweight = \"bold\",\n",
    "    bbox = dict(boxstyle = \"round,pad=0.5\", fc = \"white\", ec = \"crimson\", alpha = 0.9)\n",
    ")\n",
    "\n",
    "ax_cost.set_title(\"Cost-Sensitive Threshold Optimization (FN : 5, FP : 1)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_cost.set_xlabel(\"Decision Threshold\", fontsize = 12)\n",
    "ax_cost.set_ylabel(\"Expected Cost per Patient\", fontsize = 12)\n",
    "ax_cost.grid(alpha = 0.3, ls = \"--\")\n",
    "ax_cost.legend(loc = \"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "if 'force_fix_plot_tofu' in locals() :\n",
    "    force_fix_plot_tofu(fig_cost, ax_cost)\n",
    "plt.show()\n",
    "\n",
    "preds_final_opt : np.ndarray = (preds_prob_ni >= opt_thresh_cost).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true_arr, preds_final_opt).ravel()\n",
    "sensitivity : float = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "specificity : float = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "print(f\"ğŸ“Š Cost-Optimized Metrics (Threshold : {opt_thresh_cost:.4f})\")\n",
    "print(f\" - Sensitivity (Recall) : {sensitivity:.4f}\")\n",
    "print(f\" - Specificity : {specificity:.4f}\")\n",
    "print(f\" - False Negatives (Missed) : {fn}\")\n",
    "print(f\" - False Positives (Over-diagnosed) : {fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef221ff9",
   "metadata": {},
   "source": [
    "# 40. Non-Invasive Feature Interpretation (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f089938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "X_sample : np.ndarray = X_test_red[ :200 ]\n",
    "explainer_ni = shap.KernelExplainer(\n",
    "    lambda x : clf_ni.predict_proba(x.astype(np.float32))[ :, 1 ], \n",
    "    shap.sample(X_train_red, 50)\n",
    ")\n",
    "shap_values_ni : np.ndarray = explainer_ni.shap_values(X_sample)\n",
    "\n",
    "mean_abs_shap : np.ndarray = np.abs(shap_values_ni).mean(axis = 0)\n",
    "df_shap : pd.DataFrame = pd.DataFrame({\n",
    "    \"Feature\" : reduced_features,\n",
    "    \"Importance\" : mean_abs_shap\n",
    "}).sort_values(by = \"Importance\", ascending = True)\n",
    "\n",
    "# ê¸°ì—¬ë„ í¬ê¸°ì— ë¹„ë¡€í•˜ëŠ” ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ ë§µ(Colormap) ì ìš©\n",
    "cmap = cm.get_cmap(\"viridis\")\n",
    "norm = mcolors.Normalize(vmin = df_shap[ \"Importance\" ].min(), vmax = df_shap[ \"Importance\" ].max())\n",
    "colors = cmap(norm(df_shap[ \"Importance\" ].values))\n",
    "\n",
    "fig_shap_bar, ax_shap_bar = plt.subplots(figsize = ( 12, 10 ))\n",
    "bars = ax_shap_bar.barh(df_shap[ \"Feature\" ], df_shap[ \"Importance\" ], color = colors, edgecolor = \"black\", alpha = 0.85)\n",
    "\n",
    "for bar in bars :\n",
    "    width : float = bar.get_width()\n",
    "    ax_shap_bar.text(\n",
    "        width + (max(mean_abs_shap) * 0.01), \n",
    "        bar.get_y() + bar.get_height() / 2, \n",
    "        f\"{width:.4f}\", \n",
    "        va = \"center\", \n",
    "        fontsize = 10, \n",
    "        fontweight = \"bold\"\n",
    "    )\n",
    "\n",
    "ax_shap_bar.set_title(\"Non-Invasive Model : Feature Importance (Mean |SHAP|)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_shap_bar.set_xlabel(\"mean(|SHAP value|)\", fontsize = 12)\n",
    "ax_shap_bar.grid(axis = \"x\", ls = \"--\", alpha = 0.3)\n",
    "ax_shap_bar.set_xlim(0, max(mean_abs_shap) * 1.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "if 'force_fix_plot_tofu' in locals() :\n",
    "    force_fix_plot_tofu(fig_shap_bar, ax_shap_bar)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = ( 12, 8 ))\n",
    "shap.summary_plot(\n",
    "    shap_values_ni, \n",
    "    X_sample, \n",
    "    feature_names = reduced_features, \n",
    "    show = False\n",
    ")\n",
    "plt.title(\"Feature Impact Directionality (Non-Invasive)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "plt.tight_layout()\n",
    "\n",
    "if 'force_fix_plot_tofu' in locals() :\n",
    "    force_fix_plot_tofu(plt.gcf(), plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b86fd9",
   "metadata": {},
   "source": [
    "# 41. Prediction Dataset Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ì•ì„œ ì„¤ì •í•œ ë¹„ìš© ìµœì í™” ì„ê³„ê°’ ì ìš©\n",
    "opt_thresh_cost : float = 0.3268\n",
    "\n",
    "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„ êµ¬ì„±\n",
    "final_results : pd.DataFrame = pd.DataFrame({\n",
    "    \"True_Metabolic_Risk\" : y_true_arr,\n",
    "    \"Non_Invasive_Prob\" : preds_prob_ni,\n",
    "    \"AI_Diagnosis_Result\" : (preds_prob_ni >= opt_thresh_cost).astype(int)\n",
    "})\n",
    "\n",
    "# ì§„ë‹¨ ê²°ê³¼ ë¼ë²¨ë§ (1: ê³ ìœ„í—˜êµ°, 0: ì •ìƒ)\n",
    "final_results[ \"AI_Diagnosis_Label\" ] = final_results[ \"AI_Diagnosis_Result\" ].map({ 1 : \"High-Risk\", 0 : \"Safe\" })\n",
    "\n",
    "# ë°”íƒ•í™”ë©´ ì§€ì • ê²½ë¡œë¡œ CSV ìµìŠ¤í¬íŠ¸\n",
    "base_dir : str = r\"C:\\Users\\sck32\\Desktop\\Clinical Diabetes TabNet\"\n",
    "file_path : str = f\"{base_dir}\\\\final_non_invasive_predictions.csv\"\n",
    "\n",
    "try :\n",
    "    final_results.to_csv(file_path, index = False, encoding = \"utf-8-sig\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"âœ… ìµœì¢… ì˜ˆì¸¡ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ\")\n",
    "    print(f\"ğŸ“‚ ê²½ë¡œ : {file_path}\")\n",
    "    print(f\"ğŸ“Š ì´ ì¶”ì¶œëœ í™˜ì ë°ì´í„° ìˆ˜ : {len(final_results):,}ëª…\")\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e :\n",
    "    print(f\"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd29bf9",
   "metadata": {},
   "source": [
    "# 42. DeepSurv : Deep Neural Networks for Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a250f",
   "metadata": {},
   "source": [
    "## 42-1. ìƒì¡´ ë¶„ì„ìš© ë°ì´í„°(Time, Event) í…ì„œí™” ë° ë°ì´í„°ë¡œë” êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ì‹¤ì œ ì¶”ì  ê´€ì°° ë°ì´í„°ê°€ ë¶€ì¬í•˜ë¯€ë¡œ, ì‹œì—°ìš© ì¶”ì  ì‹œê°„(Time) ê°€ìƒ ìƒì„± (ë‹¨ìœ„ : Month)\n",
    "np.random.seed(42)\n",
    "time_train : np.ndarray = np.where(y_train_arr == 1, np.random.exponential(scale = 24, size = len(y_train_arr)), np.random.exponential(scale = 120, size = len(y_train_arr)))\n",
    "time_test : np.ndarray = np.where(y_true_arr == 1, np.random.exponential(scale = 24, size = len(y_true_arr)), np.random.exponential(scale = 120, size = len(y_true_arr)))\n",
    "\n",
    "time_train_t : torch.Tensor = torch.tensor(time_train, dtype = torch.float32)\n",
    "event_train_t : torch.Tensor = torch.tensor(y_train_arr, dtype = torch.float32)\n",
    "x_train_t : torch.Tensor = torch.tensor(X_train_red, dtype = torch.float32)\n",
    "\n",
    "time_test_t : torch.Tensor = torch.tensor(time_test, dtype = torch.float32)\n",
    "event_test_t : torch.Tensor = torch.tensor(y_true_arr, dtype = torch.float32)\n",
    "x_test_t : torch.Tensor = torch.tensor(X_test_red, dtype = torch.float32)\n",
    "\n",
    "# VRAM 12GB í™˜ê²½ì„ ê³ ë ¤í•œ Batch Size ì„¤ì •\n",
    "batch_size : int = 2048\n",
    "\n",
    "train_dataset = TensorDataset(x_train_t, time_train_t, event_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9ade0",
   "metadata": {},
   "source": [
    "## 42-2. DeepSurv ëª¨ë¸ ì •ì˜ ë° Negative Partial Log-Likelihood ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09be96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSurv(nn.Module) :\n",
    "    def __init__(self, in_features : int) :\n",
    "        super(DeepSurv, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor :\n",
    "        return self.net(x)\n",
    "\n",
    "def cox_ph_loss(risk_pred : torch.Tensor, time : torch.Tensor, event : torch.Tensor) -> torch.Tensor :\n",
    "    # Risk sorting (ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)\n",
    "    sorted_idx : torch.Tensor = torch.argsort(time, descending = True)\n",
    "    risk_pred_sorted : torch.Tensor = risk_pred[ sorted_idx ]\n",
    "    event_sorted : torch.Tensor = event[ sorted_idx ]\n",
    "    \n",
    "    # ëˆ„ì  ìœ„í—˜ë„ (Risk Set ê³„ì‚°)\n",
    "    risk_exp : torch.Tensor = torch.exp(risk_pred_sorted)\n",
    "    risk_sum : torch.Tensor = torch.cumsum(risk_exp, dim = 0)\n",
    "    \n",
    "    # Log ë¶€ë¶„ ìš°ë„ ê³„ì‚°\n",
    "    log_risk : torch.Tensor = torch.log(risk_sum + 1e-7)\n",
    "    uncensored_likelihood : torch.Tensor = risk_pred_sorted - log_risk\n",
    "    loss : torch.Tensor = -torch.sum(uncensored_likelihood * event_sorted) / (torch.sum(event_sorted) + 1e-7)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6d5e5",
   "metadata": {},
   "source": [
    "# 43. Longitudinal Survival Analysis : DeepSurv Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "\n",
    "dim_in : int = X_train_red.shape[ 1 ]\n",
    "model = DeepSurv(in_features = dim_in).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs : int = 150\n",
    "print(\"-\" * 50)\n",
    "print(\"ğŸš€ DeepSurv ìƒì¡´ ë¶„ì„ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (FP16 AMP)\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs) :\n",
    "    total_loss : float = 0.0\n",
    "    \n",
    "    for batch_x, batch_time, batch_event in train_loader :\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_time = batch_time.to(device)\n",
    "        batch_event = batch_event.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast() :\n",
    "            risk_pred : torch.Tensor = model(batch_x)\n",
    "            loss : torch.Tensor = cox_ph_loss(risk_pred, batch_time, batch_event)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    if (epoch + 1) % 30 == 0 or epoch == 0 :\n",
    "        avg_loss : float = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1:03d}/{epochs} | Cox Loss : {avg_loss:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    x_test_cuda : torch.Tensor = x_test_t.to(device)\n",
    "    risk_pred_test : np.ndarray = model(x_test_cuda).cpu().numpy().flatten()\n",
    "\n",
    "c_index : float = concordance_index(time_test, -risk_pred_test, y_true_arr)\n",
    "\n",
    "print(f\"âœ… DeepSurv Test C-Index : {c_index:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1e399",
   "metadata": {},
   "source": [
    "# 44. Survival Curve Visualization : Kaplan Meier Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "import numpy as np\n",
    "\n",
    "median_risk : float = np.median(risk_pred_test)\n",
    "high_risk_mask : np.ndarray = risk_pred_test >= median_risk\n",
    "low_risk_mask : np.ndarray = risk_pred_test < median_risk\n",
    "\n",
    "kmf_high = KaplanMeierFitter()\n",
    "kmf_low = KaplanMeierFitter()\n",
    "\n",
    "fig_km, ax_km = plt.subplots(figsize = ( 12, 7 ))\n",
    "\n",
    "kmf_high.fit(time_test[ high_risk_mask ], event_observed = y_true_arr[ high_risk_mask ], label = \"High Risk Group\")\n",
    "kmf_high.plot_survival_function(ax = ax_km, color = \"crimson\", lw = 2.5)\n",
    "\n",
    "kmf_low.fit(time_test[ low_risk_mask ], event_observed = y_true_arr[ low_risk_mask ], label = \"Low Risk Group\")\n",
    "kmf_low.plot_survival_function(ax = ax_km, color = \"#0047AB\", lw = 2.5)\n",
    "\n",
    "# ì£¼ìš” ì‹œì (Time Points)ë³„ ìƒì¡´ í™•ë¥  ìˆ˜ì¹˜\n",
    "test_times : list = [ 0, 200, 400, 600, 800 ]\n",
    "for t in test_times :\n",
    "    # High Risk Group ìˆ˜ì¹˜ í‘œì‹œ\n",
    "    surv_prob_h : float = kmf_high.predict(t)\n",
    "    ax_km.text(t, surv_prob_h - 0.03, f\"{surv_prob_h:.3f}\", color = \"crimson\", fontsize = 10, fontweight = \"bold\", ha = \"center\")\n",
    "    \n",
    "    # Low Risk Group ìˆ˜ì¹˜ í‘œì‹œ\n",
    "    surv_prob_l : float = kmf_low.predict(t)\n",
    "    ax_km.text(t, surv_prob_l + 0.01, f\"{surv_prob_l:.3f}\", color = \"#0047AB\", fontsize = 10, fontweight = \"bold\", ha = \"center\")\n",
    "\n",
    "ax_km.set_title(\"Kaplan-Meier Survival Curve with Probability Labels\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_km.set_xlabel(\"Time (Months)\", fontsize = 12)\n",
    "ax_km.set_ylabel(\"Survival Probability\", fontsize = 12)\n",
    "ax_km.set_ylim(0.65, 1.05)\n",
    "ax_km.grid(alpha = 0.3, ls = \"--\")\n",
    "ax_km.legend(loc = \"lower left\", fontsize = 11)\n",
    "\n",
    "plt.tight_layout()\n",
    "if 'force_fix_plot_tofu' in locals() :\n",
    "    force_fix_plot_tofu(fig_km, ax_km)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f33c15",
   "metadata": {},
   "source": [
    "# 45. Individual Diagnostic Support : SHAP Waterfall Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows : Malgun Gothic)\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "# í”¼ì²˜ ì˜ë¬¸ -êµ­ë¬¸ ë§¤í•‘ ì‚¬ì „\n",
    "feature_mapping : dict = {\n",
    "    \"sm_presnt\" : \"í˜„ì¬ í¡ì—° ì—¬ë¶€\",\n",
    "    \"HE_Upro\" : \"ìš”ë‹¨ë°±\",\n",
    "    \"HE_hepaB\" : \"Bí˜•ê°„ì—¼ ì–‘ì„±\",\n",
    "    \"age\" : \"ì—°ë ¹\",\n",
    "    \"HE_chol\" : \"ì´ ì½œë ˆìŠ¤í…Œë¡¤\",\n",
    "    \"HE_wc\" : \"í—ˆë¦¬ë‘˜ë ˆ\",\n",
    "    \"HE_BUN\" : \"í˜ˆì¤‘ìš”ì†Œì§ˆì†Œ\",\n",
    "    \"HE_crea\" : \"í˜ˆì¤‘ í¬ë ˆì•„í‹°ë‹Œ\",\n",
    "    \"HE_TG\" : \"ì¤‘ì„±ì§€ë°©\",\n",
    "    \"HE_alt\" : \"ALT (ê°„ìˆ˜ì¹˜)\",\n",
    "    \"N_K\" : \"ì¹¼ë¥¨ ì„­ì·¨ëŸ‰\",\n",
    "    \"HE_hepaC\" : \"Cí˜•ê°„ì—¼ ì–‘ì„±\",\n",
    "    \"N_FAT\" : \"ì§€ë°© ì„­ì·¨ëŸ‰\",\n",
    "    \"HE_WBC\" : \"ë°±í˜ˆêµ¬ ìˆ˜\",\n",
    "    \"HE_ast\" : \"AST (ê°„ìˆ˜ì¹˜)\",\n",
    "    \"N_PROT\" : \"ë‹¨ë°±ì§ˆ ì„­ì·¨ëŸ‰\",\n",
    "    \"sex\" : \"ì„±ë³„\",\n",
    "    \"HE_BMI\" : \"ì²´ì§ˆëŸ‰ì§€ìˆ˜ (BMI)\",\n",
    "    \"HE_dbp\" : \"ì´ì™„ê¸° í˜ˆì••\",\n",
    "    \"N_CHO\" : \"íƒ„ìˆ˜í™”ë¬¼ ì„­ì·¨ëŸ‰\",\n",
    "    \"N_NA\" : \"ë‚˜íŠ¸ë¥¨ ì„­ì·¨ëŸ‰\",\n",
    "    \"HE_sbp\" : \"ìˆ˜ì¶•ê¸° í˜ˆì••\",\n",
    "    \"N_EN\" : \"ì—ë„ˆì§€ ì„­ì·¨ëŸ‰\",\n",
    "    \"HE_HDL_st2\" : \"HDL ì½œë ˆìŠ¤í…Œë¡¤\"\n",
    "}\n",
    "\n",
    "# í˜„ì¬ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í”¼ì²˜ ìˆœì„œì— ë§ì¶° í•œê¸€ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "ko_features : list = [ feature_mapping.get(f, f) for f in reduced_features ]\n",
    "\n",
    "patient_idx : int = 0\n",
    "sample_patient : np.ndarray = X_test_red[ patient_idx : patient_idx + 1 ]\n",
    "\n",
    "exp_value : np.ndarray = explainer_ni.shap_values(sample_patient)\n",
    "base_value : float = explainer_ni.expected_value\n",
    "\n",
    "plt.figure(figsize = ( 12, 8 ))\n",
    "shap.plots._waterfall.waterfall_legacy(\n",
    "    base_value,\n",
    "    exp_value[ 0 ],\n",
    "    feature_names = ko_features,\n",
    "    max_display = 10,\n",
    "    show = False\n",
    ")\n",
    "\n",
    "plt.title(f\"í™˜ì ë²ˆí˜¸ {patient_idx} : AI ì§„ë‹¨ ê·¼ê±° ìƒì„¸ ë¶„ì„\", fontsize = 16, fontweight = \"bold\", pad = 25)\n",
    "plt.xlabel(\"SHAP Value (ìœ„í—˜ë„ ê¸°ì—¬ë„)\", fontsize = 12)\n",
    "plt.tight_layout()\n",
    "\n",
    "if 'force_fix_plot_tofu' in locals() :\n",
    "    force_fix_plot_tofu(plt.gcf(), plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a074cd3",
   "metadata": {},
   "source": [
    "# 46. Integrated Diagnostics : Unified Clinical Summary Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492011eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import shap\n",
    "import torch\n",
    "\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "def generate_robust_clinical_report(patient_idx, x_data, model_clf, model_surv, explainer, features, threshold) :\n",
    "    if patient_idx >= len(x_data) :\n",
    "        print(f\"Error : ì…ë ¥í•œ í™˜ì ë²ˆí˜¸({patient_idx})ê°€ ì „ì²´ ë°ì´í„° í¬ê¸°({len(x_data)})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    sample : np.ndarray = x_data[ patient_idx : patient_idx + 1 ]\n",
    "    \n",
    "    prob : float = model_clf.predict_proba(sample.astype(np.float32))[ 0, 1 ]\n",
    "    diag : str = \"ê³ ìœ„í—˜êµ°\" if prob >= threshold else \"ì •ìƒ\"\n",
    "    \n",
    "    local_device = next(model_surv.parameters()).device\n",
    "    model_surv.eval()\n",
    "    with torch.no_grad() :\n",
    "        risk_score : float = model_surv(torch.tensor(sample, dtype = torch.float32).to(local_device)).cpu().item()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸ©º [ í†µí•© ì„ìƒ ì§„ë‹¨ ë¦¬í¬íŠ¸ ] í™˜ì ë²ˆí˜¸ : {patient_idx}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # [1/3] ì§„ë‹¨ í™•ë¥  ì°¨íŠ¸\n",
    "    # ---------------------------------------------------------\n",
    "    fig1, ax1 = plt.subplots(figsize = ( 12, 3 ))\n",
    "    color : str = \"crimson\" if diag == \"ê³ ìœ„í—˜êµ°\" else \"#0047AB\"\n",
    "    \n",
    "    bar_val : float = prob if prob > 0 else 0.001 \n",
    "    ax1.barh([ \"ëŒ€ì‚¬ ì§ˆí™˜ ìœ„í—˜ë„\" ], [ bar_val ], color = color, height = 0.4, alpha = 0.8)\n",
    "    ax1.axvline(threshold, color = \"black\", ls = \"--\", lw = 2, label = f\"ì„ê³„ê°’ ({threshold})\")\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_title(f\"[1/3] ìµœì¢… ì§„ë‹¨ ê²°ê³¼ : {diag}\", fontsize = 16, fontweight = \"bold\", pad = 15)\n",
    "    ax1.legend(loc = \"center right\", fontsize = 12)\n",
    "    ax1.grid(axis = \"x\", alpha = 0.3, ls = \"--\")\n",
    "    \n",
    "    ax1.text(prob + 0.015, 0, f\"{prob:.4f}\", va = \"center\", ha = \"left\", fontsize = 14, fontweight = \"bold\", color = color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if 'force_fix_plot_tofu' in locals() :\n",
    "        force_fix_plot_tofu(fig1, ax1)\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # [2/3] SHAP Waterfall ì°¨íŠ¸\n",
    "    # ---------------------------------------------------------\n",
    "    shap_vals : np.ndarray = explainer.shap_values(sample)\n",
    "    ko_features : list = [ feature_mapping.get(f, f) for f in features ]\n",
    "    base_val : float = explainer.expected_value[ 0 ] if isinstance(explainer.expected_value, np.ndarray) else explainer.expected_value\n",
    "    \n",
    "    fig2 = plt.figure(figsize = ( 12, 6 ))\n",
    "    shap.plots._waterfall.waterfall_legacy(\n",
    "        explainer.expected_value,\n",
    "        shap_vals[ 0 ],\n",
    "        feature_names = ko_features,\n",
    "        max_display = 10,\n",
    "        show = False\n",
    "    )\n",
    "    plt.title(\"[2/3] ì¸ê³µì§€ëŠ¥ ì§„ë‹¨ ê·¼ê±° ìƒì„¸ (SHAP)\", fontsize = 16, fontweight = \"bold\", pad = 25)\n",
    "    \n",
    "    guide_text : str = f\"â€» í•´ì„ ê°€ì´ë“œ\\nì¶œë°œ ê¸°ì¤€ì  : ì „ì²´ í™˜ì í‰ê·  ìœ„í—˜ë„ ({base_val:.3f})\\në§‰ëŒ€ ìˆ˜ì¹˜ : ê¸°ì¤€ì  ëŒ€ë¹„ í•´ë‹¹ ì§€í‘œì˜ 'ìœ„í—˜ë„ ì¦ê°í­'\"\n",
    "    plt.figtext(0.01, 0.85, guide_text, fontsize = 10, color = \"#4B4B4B\", bbox = dict(facecolor = \"whitesmoke\", edgecolor = \"lightgrey\", boxstyle = \"round,pad=0.5\"))\n",
    "\n",
    "    for text_obj in plt.gcf().findobj(mpl.text.Text) :\n",
    "        text_val : str = text_obj.get_text()\n",
    "        if \"\\u2212\" in text_val or \"âˆ’\" in text_val :\n",
    "            text_obj.set_text(text_val.replace(\"\\u2212\", \"-\").replace(\"âˆ’\", \"-\"))\n",
    "        \n",
    "    plt.tight_layout(rect = [ 0, 0, 1, 0.95 ])\n",
    "    if 'force_fix_plot_tofu' in locals() :\n",
    "        force_fix_plot_tofu(plt.gcf(), plt.gca())\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # [3/3] ì •ë°€ ìƒì¡´ ê³¡ì„  \n",
    "    # ---------------------------------------------------------\n",
    "    fig3, ax3 = plt.subplots(figsize = ( 12, 6 ))\n",
    "    times : np.ndarray = np.sort(time_test)\n",
    "    base_surv : np.ndarray = np.array([ kmf_low.predict(t) for t in times ])\n",
    "    high_surv : np.ndarray = np.array([ kmf_high.predict(t) for t in times ])\n",
    "    \n",
    "    patient_surv : np.ndarray = (1.0 - prob) * base_surv + prob * high_surv\n",
    "    \n",
    "    ax3.plot(times, base_surv, color = \"#0047AB\", ls = \"--\", alpha = 0.4, label = \"ì €ìœ„í—˜êµ° í‰ê· \")\n",
    "    ax3.plot(times, high_surv, color = \"crimson\", ls = \"--\", alpha = 0.4, label = \"ê³ ìœ„í—˜êµ° í‰ê· \")\n",
    "    ax3.plot(times, patient_surv, color = \"black\", lw = 3.5, label = \"í•´ë‹¹ í™˜ì ì˜ˆìƒ ê²½ë¡œ\")\n",
    "    \n",
    "    final_time : float = times[ -1 ]\n",
    "    p_surv_final : float = patient_surv[ -1 ]\n",
    "    surv_h_final : float = high_surv[ -1 ]\n",
    "    surv_l_final : float = base_surv[ -1 ]\n",
    "\n",
    "    offset_p : float = 0.0\n",
    "    offset_l : float = 0.0\n",
    "    offset_h : float = 0.0\n",
    "    \n",
    "    if abs(p_surv_final - surv_l_final) < 0.02 :\n",
    "        offset_p = 0.01 if p_surv_final >= surv_l_final else -0.01\n",
    "        offset_l = -0.01 if p_surv_final >= surv_l_final else 0.01\n",
    "    elif abs(p_surv_final - surv_h_final) < 0.02 :\n",
    "        offset_p = 0.01 if p_surv_final >= surv_h_final else -0.01\n",
    "        offset_h = -0.01 if p_surv_final >= surv_h_final else 0.01\n",
    "            \n",
    "    ax3.text(final_time, p_surv_final + offset_p, f\"  {p_surv_final:.3f}\", color = \"black\", fontsize = 12, fontweight = \"bold\", va = \"center\")\n",
    "    ax3.text(final_time, surv_h_final + offset_h, f\"  {surv_h_final:.3f}\", color = \"crimson\", fontsize = 10, va = \"center\")\n",
    "    ax3.text(final_time, surv_l_final + offset_l, f\"  {surv_l_final:.3f}\", color = \"#0047AB\", fontsize = 10, va = \"center\")\n",
    "\n",
    "    ax3.set_title(\"[3/3] ì˜ˆìƒ ë¬´ë³‘ ìƒì¡´ ê²½ë¡œ (Cox PH)\", fontsize = 16, fontweight = \"bold\", pad = 15)\n",
    "    ax3.set_xlabel(\"ì‹œê°„ (ê°œì›”)\", fontsize = 12)\n",
    "    ax3.set_ylabel(\"ë°œë³‘ ì•ˆ í•  í™•ë¥ \", fontsize = 12)\n",
    "    \n",
    "    min_surv : float = min(p_surv_final, surv_h_final, surv_l_final)\n",
    "    y_lower : float = max(0.0, min_surv - 0.10) \n",
    "    \n",
    "    ax3.set_xlim(left = 0, right = final_time * 1.08) \n",
    "    ax3.set_ylim(y_lower, 1.05)\n",
    "    ax3.legend(loc = \"lower left\", fontsize = 12)\n",
    "    ax3.grid(alpha = 0.3, ls = \"--\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if 'force_fix_plot_tofu' in locals() :\n",
    "        force_fix_plot_tofu(fig3, ax3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f44f9",
   "metadata": {},
   "source": [
    "## 46-1 Highest Risk Patient Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ìµœê³  ìœ„í—˜êµ° (High Risk) í™˜ì ìë™ íƒìƒ‰\n",
    "high_risk_idx : int = int(np.argmax(all_probs))\n",
    "\n",
    "print(f\"ğŸ’¡ í…ŒìŠ¤íŠ¸ ì…‹ ë‚´ ìµœê³  ìœ„í—˜êµ° í™˜ì ì¸ë±ìŠ¤ : {high_risk_idx} (ì˜ˆì¸¡ í™•ë¥  : {all_probs[high_risk_idx]:.4f})\")\n",
    "\n",
    "# ë¦¬í¬íŠ¸ ìë™ ìƒì„±\n",
    "generate_robust_clinical_report(\n",
    "    patient_idx = high_risk_idx,\n",
    "    x_data = X_test_red,\n",
    "    model_clf = clf_ni,\n",
    "    model_surv = model,\n",
    "    explainer = explainer_ni,\n",
    "    features = reduced_features,\n",
    "    threshold = 0.3268\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec8a35",
   "metadata": {},
   "source": [
    "## 46-2 Median Risk Patient Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa473df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ì¤‘ê°„ ìœ„í—˜êµ° (Medium Risk / Borderline) í™˜ì íƒìƒ‰\n",
    "target_prob : float = 0.50\n",
    "medium_risk_idx : int = int(np.argmin(np.abs(all_probs - target_prob)))\n",
    "\n",
    "print(f\"ğŸ’¡ í…ŒìŠ¤íŠ¸ ì…‹ ë‚´ ì¤‘ê°„ ìœ„í—˜êµ° í™˜ì ì¸ë±ìŠ¤ : {medium_risk_idx} (ì˜ˆì¸¡ í™•ë¥  : {all_probs[medium_risk_idx]:.4f})\")\n",
    "\n",
    "# ë¦¬í¬íŠ¸ ìë™ ìƒì„±\n",
    "generate_robust_clinical_report(\n",
    "    patient_idx = medium_risk_idx,\n",
    "    x_data = X_test_red,\n",
    "    model_clf = clf_ni,\n",
    "    model_surv = model,\n",
    "    explainer = explainer_ni,\n",
    "    features = reduced_features,\n",
    "    threshold = 0.3268\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f494714",
   "metadata": {},
   "source": [
    "## 46-3 Lowest Risk Patient Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a601677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ìµœì € ìœ„í—˜êµ° (Lowest Risk) í™˜ì ìë™ íƒìƒ‰\n",
    "low_risk_idx : int = int(np.argmin(all_probs))\n",
    "\n",
    "print(f\"ğŸ’¡ í…ŒìŠ¤íŠ¸ ì…‹ ë‚´ ìµœì € ìœ„í—˜êµ° í™˜ì ì¸ë±ìŠ¤ : {low_risk_idx} (ì˜ˆì¸¡ í™•ë¥  : {all_probs[low_risk_idx]:.4f})\")\n",
    "\n",
    "# 2. ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ (ì§§ì€ ì½”ë“œ)\n",
    "generate_robust_clinical_report(\n",
    "    patient_idx = low_risk_idx,\n",
    "    x_data = X_test_red,\n",
    "    model_clf = clf_ni,\n",
    "    model_surv = model,\n",
    "    explainer = explainer_ni,\n",
    "    features = reduced_features,\n",
    "    threshold = 0.3268\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9040d30",
   "metadata": {},
   "source": [
    "# 47. Time Dependent ROC & AUC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "# í‰ê°€ ì‹œì  ì„¤ì • : 24ê°œì›”(2ë…„), 60ê°œì›”(5ë…„), 120ê°œì›”(10ë…„)\n",
    "eval_times : list = [ 24, 60, 120 ] \n",
    "\n",
    "fig_roc, ax_roc = plt.subplots(figsize = ( 8, 8 ))\n",
    "colors : list = [ \"#0047AB\", \"crimson\", \"forestgreen\" ]\n",
    "\n",
    "for idx, t in enumerate(eval_times) :\n",
    "    binary_labels : np.ndarray = ((time_test <= t) & (y_true_arr == 1)).astype(int)\n",
    "    \n",
    "    if np.sum(binary_labels) > 0 and np.sum(binary_labels) < len(binary_labels) :\n",
    "        fpr, tpr, _ = roc_curve(binary_labels, risk_pred_test)\n",
    "        roc_auc : float = auc(fpr, tpr)\n",
    "        \n",
    "        ax_roc.plot(fpr, tpr, color = colors[ idx ], lw = 2.5, label = f\"{t}ê°œì›” ì˜ˆì¸¡ (AUC : {roc_auc:.4f})\")\n",
    "\n",
    "ax_roc.plot([ 0, 1 ], [ 0, 1 ], color = \"black\", ls = \"--\", lw = 1.5, alpha = 0.5, label = \"ë¬´ì‘ìœ„ ì˜ˆì¸¡ (AUC : 0.5000)\")\n",
    "\n",
    "ax_roc.set_title(\"ì‹œê°„ ì˜ì¡´ì  ROC ê³¡ì„  (Time-Dependent ROC)\", fontsize = 16, fontweight = \"bold\", pad = 20)\n",
    "ax_roc.set_xlabel(\"ìœ„ì–‘ì„±ë¥  (False Positive Rate)\", fontsize = 12)\n",
    "ax_roc.set_ylabel(\"ë¯¼ê°ë„ (True Positive Rate)\", fontsize = 12)\n",
    "ax_roc.legend(loc = \"lower right\", fontsize = 12)\n",
    "ax_roc.grid(alpha = 0.3, ls = \"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "if 'force_fix_plot_tofu' in locals() :\n",
    "    force_fix_plot_tofu(fig_roc, ax_roc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ecc48",
   "metadata": {},
   "source": [
    "# 48. Project Finalization : Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697241fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "save_dir : str = \"./final_models\"\n",
    "os.makedirs(save_dir, exist_ok = True)\n",
    "\n",
    "# 1. TabNet ëª¨ë¸ ì €ì¥ (zip ì••ì¶• í˜•íƒœë¡œ ì €ì¥ë¨)\n",
    "tabnet_save_path : str = os.path.join(save_dir, \"tabnet_metabolic_model\")\n",
    "saved_filepath : str = clf_ni.save_model(tabnet_save_path)\n",
    "print(f\"âœ… TabNet ëª¨ë¸ ì €ì¥ ì™„ë£Œ : {saved_filepath}\")\n",
    "\n",
    "# 2. DeepSurv ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ (PyTorch State Dict)\n",
    "deepsurv_save_path : str = os.path.join(save_dir, \"deepsurv_weights.pth\")\n",
    "torch.save(model.state_dict(), deepsurv_save_path)\n",
    "print(f\"âœ… DeepSurv ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ : {deepsurv_save_path}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"ğŸš€ ëŒ€ì‚¬ ì§ˆí™˜ ì˜ˆì¸¡ ë° ìƒì¡´ ë¶„ì„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dc1bb",
   "metadata": {},
   "source": [
    "# 49. Export All Visualizations (Save & Zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import shap\n",
    "import torch\n",
    "from sklearn.metrics import roc_curve, auc, brier_score_loss\n",
    "\n",
    "# í°íŠ¸ ë° ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ì„¤ì •\n",
    "plt.rcParams[ \"font.family\" ] = \"Malgun Gothic\"\n",
    "plt.rcParams[ \"axes.unicode_minus\" ] = False\n",
    "\n",
    "# ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "save_dir : str = \"./results\"\n",
    "if os.path.exists(save_dir) :\n",
    "    shutil.rmtree(save_dir)\n",
    "os.makedirs(save_dir, exist_ok = True)\n",
    "\n",
    "print(\"ğŸš€ ìµœì‹  ë²„ì „ì˜ ê³ í•´ìƒë„ ê·¸ë˜í”„ ì¼ê´„ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. ê³ ë„í™”ëœ ì˜ˆì¸¡ í™•ë¥  êµì • ê³¡ì„  (Calibration Analysis) ì €ì¥\n",
    "# ---------------------------------------------------------\n",
    "brier_val : float = brier_score_loss(y_true_arr, clf_ni.predict_proba(X_test_red.astype(np.float32))[ :, 1 ])\n",
    "\n",
    "fig_cal, ax_cal = plt.subplots(figsize = ( 10, 10 ))\n",
    "ax_cal.plot(prob_pred, prob_true, marker = \"o\", ls = \"-\", color = \"#0047AB\", lw = 3, label = f\"TabNet (Brier : {brier_val:.4f})\")\n",
    "ax_cal.plot([ 0, 1 ], [ 0, 1 ], ls = \"--\", color = \"#4B4B4B\", label = \"Ideal\")\n",
    "\n",
    "for p, o in zip(prob_pred, prob_true) :\n",
    "    ax_cal.annotate(\n",
    "        f\"P : {p:.3f}\\nO : {o:.3f}\", \n",
    "        xy = (p, o), \n",
    "        xytext = (0, 15), \n",
    "        textcoords = \"offset points\", \n",
    "        ha = \"center\", va = \"bottom\",\n",
    "        fontsize = 10, fontweight = \"bold\", color = \"#0047AB\",\n",
    "        bbox = dict(boxstyle = \"round,pad=0.3\", edgecolor = \"#0047AB\", facecolor = \"white\", alpha = 0.9)\n",
    "    )\n",
    "\n",
    "ax_cal.set_title(\"ì˜ˆì¸¡ í™•ë¥  êµì • ê³¡ì„  (Calibration Analysis)\", fontsize = 18, fontweight = \"bold\", pad = 20)\n",
    "ax_cal.set_xlabel(\"í‰ê·  ì˜ˆì¸¡ í™•ë¥  (Mean Predicted Probability)\", fontsize = 12)\n",
    "ax_cal.set_ylabel(\"ì‹¤ì œ ê´€ì¸¡ ë¹„ìœ¨ (Fraction of Positives)\", fontsize = 12)\n",
    "ax_cal.set_ylim(-0.1, 1.1)\n",
    "ax_cal.legend(loc = \"lower right\", fontsize = 12)\n",
    "ax_cal.grid(alpha = 0.2)\n",
    "plt.tight_layout()\n",
    "fig_cal.savefig(os.path.join(save_dir, \"01_calibration_curve.png\"), dpi = 300)\n",
    "plt.close(fig_cal)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. OOD Detection Histogram ì €ì¥\n",
    "# ---------------------------------------------------------\n",
    "fig_ood, ax_ood = plt.subplots(figsize = ( 12, 7 ))\n",
    "counts, bins, patches = ax_ood.hist(anomaly_scores, bins = 50, color = \"#4A90E2\", edgecolor = \"white\", alpha = 0.7)\n",
    "\n",
    "# í•˜ìœ„ 5% ì„ê³„ê°’ ì„¤ì •\n",
    "threshold_val : float = np.percentile(anomaly_scores, 5)\n",
    "ax_ood.axvline(threshold_val, color = \"crimson\", ls = \"--\", lw = 3, label = f\"ì„ê³„ê°’ (í•˜ìœ„ 5% : {threshold_val:.4f})\")\n",
    "\n",
    "# ì˜ì—­ë³„ í…ìŠ¤íŠ¸ ì„¤ëª… ì¶”ê°€\n",
    "ax_ood.text(threshold_val - 0.01, max(counts) * 0.8, \"ì´ìƒì¹˜ ì˜ì—­ (OOD) \", color = \"crimson\", ha = \"right\", fontweight = \"bold\", fontsize = 12)\n",
    "ax_ood.text(threshold_val + 0.01, max(counts) * 0.8, \" ì •ìƒ ë°ì´í„° ì˜ì—­\", color = \"#0047AB\", ha = \"left\", fontweight = \"bold\", fontsize = 12)\n",
    "\n",
    "ax_ood.set_title(\"OOD Detection : Anomaly Score Distribution\", fontsize = 18, fontweight = \"bold\", pad = 20)\n",
    "ax_ood.set_xlabel(\"Anomaly Score (Higher is more normal)\", fontsize = 12)\n",
    "ax_ood.set_ylabel(\"Frequency\", fontsize = 12)\n",
    "ax_ood.legend(loc = \"upper left\", fontsize = 12)\n",
    "ax_ood.grid(axis = \"y\", alpha = 0.2, ls = \"--\")\n",
    "plt.tight_layout()\n",
    "fig_ood.savefig(os.path.join(save_dir, \"02_ood_detection.png\"), dpi = 300)\n",
    "plt.close(fig_ood)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Time-Dependent ROC Curve ì €ì¥\n",
    "# ---------------------------------------------------------\n",
    "fig_roc, ax_roc = plt.subplots(figsize = ( 10, 8 ))\n",
    "colors : list = [ \"#0047AB\", \"crimson\", \"forestgreen\" ]\n",
    "for idx, t in enumerate(eval_times) :\n",
    "    binary_labels : np.ndarray = ((time_test <= t) & (y_true_arr == 1)).astype(int)\n",
    "    if 0 < np.sum(binary_labels) < len(binary_labels) :\n",
    "        fpr, tpr, _ = roc_curve(binary_labels, risk_pred_test)\n",
    "        roc_auc : float = auc(fpr, tpr)\n",
    "        ax_roc.plot(fpr, tpr, color = colors[ idx ], lw = 3, label = f\"{t}ê°œì›” ì˜ˆì¸¡ (AUC : {roc_auc:.4f})\")\n",
    "\n",
    "ax_roc.plot([ 0, 1 ], [ 0, 1 ], color = \"black\", ls = \"--\", lw = 1.5, alpha = 0.5, label = \"Random\")\n",
    "ax_roc.set_title(\"ì‹œê°„ ì˜ì¡´ì  ROC ê³¡ì„  (Time-Dependent ROC)\", fontsize = 18, fontweight = \"bold\", pad = 20)\n",
    "ax_roc.set_xlabel(\"FPR (False Positive Rate)\", fontsize = 12)\n",
    "ax_roc.set_ylabel(\"TPR (True Positive Rate)\", fontsize = 12)\n",
    "ax_roc.legend(loc = \"lower right\", fontsize = 12)\n",
    "ax_roc.grid(alpha = 0.3, ls = \"--\")\n",
    "plt.tight_layout()\n",
    "fig_roc.savefig(os.path.join(save_dir, \"03_time_dependent_roc.png\"), dpi = 300)\n",
    "plt.close(fig_roc)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. ê°œë³„ í™˜ì ë¦¬í¬íŠ¸ ì¼ê´„ ì €ì¥ í•¨ìˆ˜\n",
    "# ---------------------------------------------------------\n",
    "def save_final_patient_report(patient_idx, case_name, threshold) :\n",
    "    sample : np.ndarray = X_test_red[ patient_idx : patient_idx + 1 ]\n",
    "    prob : float = clf_ni.predict_proba(sample.astype(np.float32))[ 0, 1 ]\n",
    "    diag : str = \"ê³ ìœ„í—˜êµ°\" if prob >= threshold else \"ì •ìƒ\"\n",
    "    \n",
    "    local_device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    with torch.no_grad() :\n",
    "        risk_score : float = model(torch.tensor(sample, dtype = torch.float32).to(local_device)).cpu().item()\n",
    "    \n",
    "    # 4-1. Diagnosis Bar\n",
    "    fig1, ax1 = plt.subplots(figsize = ( 12, 3 ))\n",
    "    color : str = \"crimson\" if diag == \"ê³ ìœ„í—˜êµ°\" else \"#0047AB\"\n",
    "    bar_val : float = prob if prob > 0 else 0.001 \n",
    "    ax1.barh([ \"ëŒ€ì‚¬ ì§ˆí™˜ ìœ„í—˜ë„\" ], [ bar_val ], color = color, height = 0.4, alpha = 0.8)\n",
    "    ax1.axvline(threshold, color = \"black\", ls = \"--\", lw = 2, label = f\"ì„ê³„ê°’ ({threshold})\")\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_title(f\"[1/3] {case_name} ì§„ë‹¨ ê²°ê³¼ : {diag}\", fontsize = 16, fontweight = \"bold\", pad = 15)\n",
    "    ax1.legend(loc = \"center right\", fontsize = 12)\n",
    "    ax1.text(prob + 0.015, 0, f\"{prob:.4f}\", va = \"center\", ha = \"left\", fontsize = 14, fontweight = \"bold\", color = color)\n",
    "    plt.tight_layout()\n",
    "    fig1.savefig(os.path.join(save_dir, f\"04_{case_name}_1_Diagnosis.png\"), dpi = 300)\n",
    "    plt.close(fig1)\n",
    "\n",
    "    # 4-2. SHAP Waterfall (UX ê°€ì´ë“œ í¬í•¨)\n",
    "    shap_vals : np.ndarray = explainer_ni.shap_values(sample)\n",
    "    ko_features : list = [ feature_mapping.get(f, f) for f in reduced_features ]\n",
    "    base_val : float = explainer_ni.expected_value[ 0 ] if isinstance(explainer_ni.expected_value, np.ndarray) else explainer_ni.expected_value\n",
    "    \n",
    "    fig2 = plt.figure(figsize = ( 12, 6 ))\n",
    "    shap.plots._waterfall.waterfall_legacy(explainer_ni.expected_value, shap_vals[ 0 ], feature_names = ko_features, max_display = 10, show = False)\n",
    "    plt.title(f\"[2/3] {case_name} ì¸ê³µì§€ëŠ¥ ì§„ë‹¨ ê·¼ê±° ìƒì„¸\", fontsize = 16, fontweight = \"bold\", pad = 25)\n",
    "    \n",
    "    guide_text : str = f\"â€» í•´ì„ ê°€ì´ë“œ\\nì¶œë°œ ê¸°ì¤€ì  : ì „ì²´ í™˜ì í‰ê·  ìœ„í—˜ë„ ({base_val:.3f})\\në§‰ëŒ€ ìˆ˜ì¹˜ : ê¸°ì¤€ì  ëŒ€ë¹„ í•´ë‹¹ ì§€í‘œì˜ 'ìœ„í—˜ë„ ì¦ê°í­'\"\n",
    "    plt.figtext(0.01, 0.85, guide_text, fontsize = 10, color = \"#4B4B4B\", bbox = dict(facecolor = \"whitesmoke\", edgecolor = \"lightgrey\", boxstyle = \"round,pad=0.5\"))\n",
    "\n",
    "    for text_obj in plt.gcf().findobj(mpl.text.Text) :\n",
    "        text_val : str = text_obj.get_text()\n",
    "        if \"\\u2212\" in text_val or \"âˆ’\" in text_val :\n",
    "            text_obj.set_text(text_val.replace(\"\\u2212\", \"-\").replace(\"âˆ’\", \"-\"))\n",
    "            \n",
    "    plt.tight_layout(rect = [ 0, 0, 1, 0.95 ])\n",
    "    fig2.savefig(os.path.join(save_dir, f\"04_{case_name}_2_SHAP.png\"), dpi = 300)\n",
    "    plt.close(fig2)\n",
    "\n",
    "    # 4-3. Survival Curve (ì•™ìƒë¸” ë³´ê°„ ì ìš©)\n",
    "    fig3, ax3 = plt.subplots(figsize = ( 12, 6 ))\n",
    "    times : np.ndarray = np.sort(time_test)\n",
    "    base_surv : np.ndarray = np.array([ kmf_low.predict(t) for t in times ])\n",
    "    high_surv : np.ndarray = np.array([ kmf_high.predict(t) for t in times ])\n",
    "    \n",
    "    patient_surv : np.ndarray = (1.0 - prob) * base_surv + prob * high_surv\n",
    "    \n",
    "    ax3.plot(times, base_surv, color = \"#0047AB\", ls = \"--\", alpha = 0.4, label = \"ì €ìœ„í—˜êµ° í‰ê· \")\n",
    "    ax3.plot(times, high_surv, color = \"crimson\", ls = \"--\", alpha = 0.4, label = \"ê³ ìœ„í—˜êµ° í‰ê· \")\n",
    "    ax3.plot(times, patient_surv, color = \"black\", lw = 3.5, label = \"í•´ë‹¹ í™˜ì ì˜ˆìƒ ê²½ë¡œ\")\n",
    "    \n",
    "    final_time : float = times[ -1 ]\n",
    "    p_surv_final : float = patient_surv[ -1 ]\n",
    "    surv_h_final : float = high_surv[ -1 ]\n",
    "    surv_l_final : float = base_surv[ -1 ]\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ê²¹ì¹¨ ë°©ì§€ ì˜¤í”„ì…‹\n",
    "    offset_p, offset_l, offset_h = 0.0, 0.0, 0.0\n",
    "    if abs(p_surv_final - surv_l_final) < 0.02 :\n",
    "        offset_p = 0.01 if p_surv_final >= surv_l_final else -0.01\n",
    "        offset_l = -0.01 if p_surv_final >= surv_l_final else 0.01\n",
    "    elif abs(p_surv_final - surv_h_final) < 0.02 :\n",
    "        offset_p = 0.01 if p_surv_final >= surv_h_final else -0.01\n",
    "        offset_h = -0.01 if p_surv_final >= surv_h_final else 0.01\n",
    "            \n",
    "    ax3.text(final_time, p_surv_final + offset_p, f\"  {p_surv_final:.3f}\", color = \"black\", fontsize = 12, fontweight = \"bold\", va = \"center\")\n",
    "    ax3.text(final_time, surv_h_final + offset_h, f\"  {surv_h_final:.3f}\", color = \"crimson\", fontsize = 10, va = \"center\")\n",
    "    ax3.text(final_time, surv_l_final + offset_l, f\"  {surv_l_final:.3f}\", color = \"#0047AB\", fontsize = 10, va = \"center\")\n",
    "\n",
    "    ax3.set_title(f\"[3/3] {case_name} ì˜ˆìƒ ë¬´ë³‘ ìƒì¡´ ê²½ë¡œ\", fontsize = 16, fontweight = \"bold\", pad = 15)\n",
    "    ax3.set_xlabel(\"ì‹œê°„ (ê°œì›”)\", fontsize = 12)\n",
    "    ax3.set_ylabel(\"ë°œë³‘ ì•ˆ í•  í™•ë¥ \", fontsize = 12)\n",
    "    \n",
    "    y_lower : float = max(0.0, min(p_surv_final, surv_h_final, surv_l_final) - 0.10) \n",
    "    ax3.set_xlim(0, final_time * 1.08) \n",
    "    ax3.set_ylim(y_lower, 1.05)\n",
    "    ax3.legend(loc = \"lower left\", fontsize = 12)\n",
    "    ax3.grid(alpha = 0.3, ls = \"--\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig3.savefig(os.path.join(save_dir, f\"04_{case_name}_3_Survival.png\"), dpi = 300)\n",
    "    plt.close(fig3)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. í•µì‹¬ 3ëŒ€ ìŠ¤í™íŠ¸ëŸ¼ í™˜ì ë¦¬í¬íŠ¸ ì¼ê´„ ì €ì¥\n",
    "# ---------------------------------------------------------\n",
    "all_probs_test = clf_ni.predict_proba(X_test_red.astype(np.float32))[ :, 1 ]\n",
    "target_thresh = 0.3268\n",
    "\n",
    "idx_high : int = int(np.argmax(all_probs_test))\n",
    "idx_mid : int = int(np.argmin(np.abs(all_probs_test - 0.50)))\n",
    "idx_low : int = int(np.argmin(all_probs_test))\n",
    "\n",
    "save_final_patient_report(idx_high, \"HighRisk\", target_thresh)\n",
    "save_final_patient_report(idx_mid, \"Borderline\", target_thresh)\n",
    "save_final_patient_report(idx_low, \"LowRisk\", target_thresh)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. í´ë” ì••ì¶• (ZIP)\n",
    "# ---------------------------------------------------------\n",
    "zip_filename : str = \"Metabolic_CDSS_Final_Assets\"\n",
    "shutil.make_archive(zip_filename, 'zip', save_dir)\n",
    "print(f\"âœ¨ ëª¨ë“  ê·¸ë˜í”„ê°€ ì €ì¥ ë° ì••ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤ : {zip_filename}.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
